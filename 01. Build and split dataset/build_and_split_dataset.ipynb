{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546c37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9447106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = ['book', 'Amazon-KG-5core-Books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "896963d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def leave_k_out_split(interaction_df: pd.DataFrame, val_k: int, test_k: int, k_core: int, user_col, item_col, time_col):\n",
    "    \"\"\"\n",
    "    Split dataset using Chronological split with Iterative K-Core Filtering.\n",
    "    Cho phép số lượng item tập Val khác số lượng item tập Test.\n",
    "    \n",
    "    Args:\n",
    "        interaction_df: Dataframe of user-item interactions\n",
    "        val_k (int): Số lượng item rút ra cho Validation (Ví dụ: 1)\n",
    "        test_k (int): Số lượng item rút ra cho Test (Ví dụ: 2)\n",
    "        k_core (int): Ngưỡng lọc K-core. (Cần lớn hơn tổng val_k + test_k)\n",
    "    \"\"\"\n",
    "    # 1. Cập nhật điều kiện kiểm tra an toàn\n",
    "    if k_core <= (val_k + test_k):\n",
    "        raise ValueError(f\"k_core ({k_core}) phải lớn hơn tổng val_k + test_k ({val_k + test_k}) để đảm bảo tập Train có dữ liệu!\")\n",
    "\n",
    "    if val_k >= test_k:\n",
    "        print(f\"⚠️ Lưu ý: Bạn đang set val_k ({val_k}) >= test_k ({test_k}). Thường thì tập Val nên nhỏ hơn tập Test.\")\n",
    "\n",
    "    df_filtered = interaction_df.copy()\n",
    "    \n",
    "    #########################################################\n",
    "    #### 1. ITERATIVE K-CORE FILTERING (Bộ lọc sinh tử)\n",
    "    #########################################################\n",
    "    print(f\"Bắt đầu lọc {k_core}-core...\")\n",
    "    while True:\n",
    "        start_len = len(df_filtered)\n",
    "        \n",
    "        # Lọc User\n",
    "        user_counts = df_filtered[user_col].value_counts()\n",
    "        valid_users = user_counts[user_counts >= k_core].index\n",
    "        df_filtered = df_filtered[df_filtered[user_col].isin(valid_users)]\n",
    "        \n",
    "        # Lọc Item\n",
    "        item_counts = df_filtered[item_col].value_counts()\n",
    "        valid_items = item_counts[item_counts >= k_core].index\n",
    "        df_filtered = df_filtered[df_filtered[item_col].isin(valid_items)]\n",
    "        \n",
    "        # Nếu số lượng dòng không giảm nữa -> Đồ thị đã ổn định\n",
    "        if len(df_filtered) == start_len:\n",
    "            break\n",
    "            \n",
    "    print(f\"Sau khi lọc: Còn lại {df_filtered[user_col].nunique()} Users và {df_filtered[item_col].nunique()} Items.\")\n",
    "    \n",
    "    #########################################################\n",
    "    #### 2. Sort dataframe by timestamp\n",
    "    #########################################################\n",
    "    df_filtered[time_col] = pd.to_datetime(df_filtered[time_col])        \n",
    "    df_filtered = df_filtered.sort_values(by=[user_col, time_col]).reset_index(drop=True)\n",
    "\n",
    "    #########################################################\n",
    "    #### 3. Split train, validation, test dataset\n",
    "    #########################################################\n",
    "    \n",
    "    # Rút test_k tương tác MỚI NHẤT làm tập Test\n",
    "    test_indices = df_filtered.groupby(user_col).tail(test_k).index\n",
    "    test_interaction_df = df_filtered.loc[test_indices]\n",
    "    \n",
    "    remaining_after_test = df_filtered.drop(test_indices)\n",
    "    \n",
    "    # Rút val_k tương tác KẾ TIẾP (áp chót) làm tập Validation\n",
    "    val_indices = remaining_after_test.groupby(user_col).tail(val_k).index\n",
    "    val_interaction_df = df_filtered.loc[val_indices]\n",
    "    \n",
    "    # Toàn bộ phần còn lại (cũ nhất) đưa vào tập Train\n",
    "    train_indices = remaining_after_test.drop(val_indices).index\n",
    "    train_interaction_df = df_filtered.loc[train_indices]\n",
    "    \n",
    "    return train_interaction_df, val_interaction_df, test_interaction_df, df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c115ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_and_interactions(interactions: pd.DataFrame, link: pd.DataFrame, static_graph: pd.DataFrame):\n",
    "    #########################################################\n",
    "    #### 1. Build static knowledge graph\n",
    "    #########################################################\n",
    "    user_tokens = interactions['user_id:token'].unique()\n",
    "    num_users = len(user_tokens)\n",
    "\n",
    "    entity_tokens = link['entity_id:token'].unique()\n",
    "\n",
    "    head_tokens = static_graph['head_id:token'].unique()\n",
    "    tail_tokens = static_graph['tail_id:token'].unique()\n",
    "\n",
    "    all_entity_tokens = pd.unique(np.concatenate([entity_tokens, head_tokens, tail_tokens]))\n",
    "    entity2id = {entity: (num_users + idx + 1) for idx, entity in enumerate(all_entity_tokens)}\n",
    "\n",
    "    static_graph['head_id'] = static_graph['head_id:token'].map(entity2id)\n",
    "    static_graph['tail_id'] = static_graph['tail_id:token'].map(entity2id)\n",
    "\n",
    "    static_graph['relation_id'] = static_graph['relation_id:token'].astype('category').cat.codes  \n",
    "    static_graph['relation_id'] = static_graph['relation_id'] + 1   \n",
    "\n",
    "    static_graph = static_graph.dropna()\n",
    "    static_graph = static_graph.astype({'head_id': 'long', 'relation_id': 'long', 'tail_id': 'long'})\n",
    "    static_graph = static_graph[['head_id', 'relation_id', 'tail_id',\n",
    "             'head_id:token', 'relation_id:token', 'tail_id:token']]\n",
    "\n",
    "    static_graph = static_graph.sort_values(by=['head_id', 'tail_id'])\n",
    "\n",
    "    #########################################################\n",
    "    #### 2. Build interactions\n",
    "    #########################################################\n",
    "    user2id = {user: (idx + 1) for idx, user in enumerate(user_tokens)}\n",
    "\n",
    "    item2entity = dict(zip(link['item_id:token'], link['entity_id:token']))\n",
    "    item2entity_id = {item: entity2id[entity] for item, entity in item2entity.items()}\n",
    "\n",
    "    interactions['entity_id:token'] = interactions['item_id:token'].map(item2entity)\n",
    "    interactions['user_id'] = interactions['user_id:token'].map(user2id)\n",
    "    interactions['entity_id'] = interactions['item_id:token'].map(item2entity_id)\n",
    "\n",
    "    interactions = interactions.dropna()\n",
    "    interactions = interactions.astype({'user_id': 'long', 'entity_id': 'long'})\n",
    "    interactions = interactions[['user_id', 'entity_id', 'timestamp','user_id:token',\n",
    "                                 'entity_id:token', 'item_id:token']]\n",
    "\n",
    "    interactions = interactions.sort_values(by=['user_id', 'entity_id'])\n",
    "\n",
    "    return static_graph, interactions, entity2id, user2id, item2entity, item2entity_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd154d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu lọc 8-core...\n",
      "Sau khi lọc: Còn lại 2890 Users và 2068 Items.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    name0 = NAMES[0]\n",
    "    name1 = NAMES[1]\n",
    "\n",
    "    interactions = pd.read_csv(f'./data/{name0}/{name0}_interaction.csv', sep= ',')\n",
    "    link = pd.read_csv(f'./data/{name0}/{name1}.link', sep=\"\\t\")\n",
    "    static_graph = pd.read_csv(f'./data/{name0}/{name1}.kg', sep=\"\\t\")\n",
    "    \n",
    "    train_df, val_df, test_df, filtered_interactions = leave_k_out_split(interactions, val_k= 1, test_k = 2, k_core= 8,\n",
    "                                                                            user_col= 'user_id:token', \n",
    "                                                                            item_col= 'item_id:token', \n",
    "                                                                            time_col='timestamp')\n",
    "\n",
    "\n",
    "    static_graph, interactions, entity2id, user2id, item2entity, item2entity_id = build_graph_and_interactions(filtered_interactions, link, static_graph)\n",
    "    \n",
    "    static_graph.to_csv(f'./data/{name0}/{name0}_processed_static_graph.csv', index=False)\n",
    "    interactions.to_csv(f'./data/{name0}/{name0}_processed_interactions.csv', index= False)\n",
    "\n",
    "    \n",
    "\n",
    "    train_df['entity_id:token'] = train_df['item_id:token'].map(item2entity)\n",
    "    train_df['user_id'] = train_df['user_id:token'].map(user2id)\n",
    "    train_df['entity_id'] = train_df['item_id:token'].map(item2entity_id)\n",
    "    train_df = train_df[['user_id', 'entity_id', 'timestamp','user_id:token',\n",
    "                                'entity_id:token', 'item_id:token']]\n",
    "    train_df = train_df.sort_values(by=['user_id', 'entity_id'])\n",
    "    \n",
    "\n",
    "    val_df['entity_id:token'] = val_df['item_id:token'].map(item2entity)\n",
    "    val_df['user_id'] = val_df['user_id:token'].map(user2id)\n",
    "    val_df['entity_id'] = val_df['item_id:token'].map(item2entity_id)\n",
    "    val_df = val_df[['user_id', 'entity_id', 'timestamp','user_id:token',\n",
    "                                'entity_id:token', 'item_id:token']]\n",
    "    val_df = val_df.sort_values(by=['user_id', 'entity_id'])\n",
    "\n",
    "    test_df['entity_id:token'] = test_df['item_id:token'].map(item2entity)\n",
    "    test_df['user_id'] = test_df['user_id:token'].map(user2id)\n",
    "    test_df['entity_id'] = test_df['item_id:token'].map(item2entity_id)\n",
    "    test_df = test_df[['user_id', 'entity_id', 'timestamp','user_id:token',\n",
    "                                'entity_id:token', 'item_id:token']]\n",
    "    test_df = test_df.sort_values(by=['user_id', 'entity_id'])\n",
    "\n",
    "    train_df.to_csv(f'./data/{name0}/{name0}_train_interactions.csv', index= False)\n",
    "    val_df.to_csv(f'./data/{name0}/{name0}_val_interactions.csv', index= False)\n",
    "    test_df.to_csv(f'./data/{name0}/{name0}_test_interactions.csv', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
