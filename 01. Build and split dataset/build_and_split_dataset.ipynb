{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "546c37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9447106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = ['movie', 'Amazon-KG-5core-Movies_and_TV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c115ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_and_interactions(name = NAMES):\n",
    "    name0 = NAMES[0]\n",
    "    name1 = NAMES[1]\n",
    "\n",
    "    #########################################################\n",
    "    #### 1. Build static knowledge graph\n",
    "    #########################################################\n",
    "    interactions = pd.read_csv(f'./data/{name0}/{name0}_interaction.csv', sep= ',')\n",
    "    user_tokens = interactions['user_id:token'].unique()\n",
    "    num_users = len(user_tokens)\n",
    "\n",
    "    link = pd.read_csv(f'./data/{name0}/{name1}.link', sep=\"\\t\")\n",
    "    entity_tokens = link['entity_id:token'].unique()\n",
    "\n",
    "    static_graph = pd.read_csv(f'./data/{name0}/{name1}.kg', sep=\"\\t\")\n",
    "    head_tokens = static_graph['head_id:token'].unique()\n",
    "    tail_tokens = static_graph['tail_id:token'].unique()\n",
    "\n",
    "    all_entity_tokens = np.unique(np.concatenate([entity_tokens, head_tokens, tail_tokens]))\n",
    "    entity2id = {entity: (num_users + idx + 1) for idx, entity in enumerate(all_entity_tokens)}\n",
    "    # id2entity = {idx: entity for entity, idx in entity2id.items()}\n",
    "\n",
    "    item2entity = dict(zip(link['item_id:token'], link['entity_id:token']))\n",
    "    item2entity_id = {item: entity2id[entity] for item, entity in item2entity.items()}\n",
    "\n",
    "    static_graph['head_id'] = static_graph['head_id:token'].map(entity2id)\n",
    "    static_graph['tail_id'] = static_graph['tail_id:token'].map(entity2id)\n",
    "\n",
    "    static_graph['relation_id'] = static_graph['relation_id:token'].astype('category').cat.codes  \n",
    "    static_graph['relation_id'] = static_graph['relation_id'] + 1   \n",
    "\n",
    "    static_graph = static_graph.dropna()\n",
    "    static_graph = static_graph.astype({'head_id': 'long', 'relation_id': 'long', 'tail_id': 'long'})\n",
    "    static_graph = static_graph[['head_id', 'relation_id', 'tail_id',\n",
    "             'head_id:token', 'relation_id:token', 'tail_id:token']]\n",
    "\n",
    "    #########################################################\n",
    "    #### . Build interactions\n",
    "    #########################################################\n",
    "    user2id = {user: (idx + 1) for idx, user in enumerate(user_tokens)}\n",
    "\n",
    "    interactions['entity_id:token'] = interactions['item_id:token'].map(item2entity)\n",
    "    interactions['user_id'] = interactions['user_id:token'].map(user2id)\n",
    "    interactions['entity_id'] = interactions['item_id:token'].map(item2entity_id)\n",
    "\n",
    "    interactions = interactions.dropna()\n",
    "    interactions = interactions.astype({'user_id': 'long', 'entity_id': 'long'})\n",
    "    interactions = interactions[['user_id', 'entity_id', 'timestamp','user_id:token',\n",
    "                                 'entity_id:token', 'item_id:token']]\n",
    "\n",
    "    return static_graph, interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cea2361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_k_out_split(interaction_df: pd.DataFrame, k:int, user_col='user_id', time_col='timestamp'):\n",
    "    \"\"\"\n",
    "    Split dataset using Leave-K-Out strategy (Chronological).\n",
    "    \n",
    "    Args:\n",
    "        interaction_df (pd.DataFrame): Dataframe of user-item interactions\n",
    "        k (int): k-items for validation dataset & k-items for test dataset\n",
    "        \n",
    "    Returns:\n",
    "        train_df, val_df, test_df\n",
    "    \"\"\"\n",
    "    \n",
    "    #########################################################\n",
    "    #### 1. Sort dataframe by timestamp\n",
    "    #########################################################\n",
    "    interaction_df[time_col] = pd.to_datetime(interaction_df[time_col])        \n",
    "    interaction_df = interaction_df.sort_values(by=[user_col, time_col]).reset_index(drop=True)\n",
    "    \n",
    "    #########################################################\n",
    "    #### 2. Filter data\n",
    "    #### 2.1 To split Train (> k item) + Val (k items) + Test (k items)\n",
    "    #### 2.2 Each user must have >= 3*k interactions\n",
    "    #########################################################\n",
    "    min_interactions = 3*k\n",
    "    \n",
    "    user_counts = interaction_df[user_col].value_counts()\n",
    "    valid_users = user_counts[user_counts >= min_interactions].index\n",
    "    \n",
    "    # Chỉ giữ lại các user đủ điều kiện\n",
    "    df_filtered = interaction_df[interaction_df[user_col].isin(valid_users)].copy()\n",
    "        \n",
    "    if len(df_filtered) == 0:\n",
    "        raise ValueError(\"K value is too high, please reduce K value.\".format(k))\n",
    "\n",
    "    #########################################################\n",
    "    #### 3. Split train, validation, test dataset\n",
    "    #########################################################\n",
    "\n",
    "    test_indices = df_filtered.groupby(user_col).tail(k).index\n",
    "    test_interaction_df = df_filtered.loc[test_indices]\n",
    "    \n",
    "    remaining_after_test = df_filtered.drop(test_indices)\n",
    "    \n",
    "    val_indices = remaining_after_test.groupby(user_col).tail(k).index\n",
    "    val_interaction_df = df_filtered.loc[val_indices]\n",
    "    \n",
    "    train_indices = remaining_after_test.drop(val_indices).index\n",
    "    train_interaction_df = df_filtered.loc[train_indices]\n",
    "    \n",
    "    return train_interaction_df, val_interaction_df, test_interaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bd154d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    name0 = NAMES[0]\n",
    "    name1 = NAMES[1]\n",
    "\n",
    "    static_graph, interaction_df = build_graph_and_interactions()\n",
    "    \n",
    "    static_graph = static_graph.sort_values(by=['tail_id'])\n",
    "    interaction_df = interaction_df.sort_values(by=['user_id'])\n",
    "\n",
    "    static_graph.to_csv(f'./data/{name0}/{name0}_processed_static_graph.csv', index=False)\n",
    "    interaction_df.to_csv(f'./data/{name0}/{name0}_processed_interactions.csv', index= False)\n",
    "        \n",
    "    train_interaction_df, val_interaction_df, test_interaction_df = leave_k_out_split(interaction_df, k = 10)\n",
    "    \n",
    "    train_interaction_df.to_csv(f'./data/{name0}/{name0}_train_interactions.csv', index= False)\n",
    "    val_interaction_df.to_csv(f'./data/{name0}/{name0}_val_interactions.csv', index= False)\n",
    "    test_interaction_df.to_csv(f'./data/{name0}/{name0}_test_interactions.csv', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
