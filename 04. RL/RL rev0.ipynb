{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36101a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Set environment variables for reproducibility and safety\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 1. Configuration & Seeding\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'book' \n",
    "INTERACTION_PATH = f'./data/{NAME}_processed_interactions.csv'\n",
    "GRAPH_PATH = f'./data/{NAME}_processed_graph.csv'\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "LR = 0.001\n",
    "MAX_STEPS = 3\n",
    "GAMMA = 0.99 # Discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48479536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCKG:\n",
    "    def __init__(self, tckg_csv_path):\n",
    "        self.adj_list = defaultdict(list)\n",
    "        \n",
    "        print(f\"Loading TCKG from {tckg_csv_path}...\")\n",
    "        df_tckg = pd.read_csv(tckg_csv_path, usecols=['head_id', 'relation_id', 'tail_id'])\n",
    "        \n",
    "        offset = df_tckg['relation_id'].max() + 1\n",
    "            \n",
    "        self.num_relations = offset * 2 # Total relation_id (bidirection)\n",
    "        print(f\"Inverse Relation Offset: {offset}. Total Relation Space: {self.num_relations}\")\n",
    "\n",
    "        data = df_tckg[['head_id', 'relation_id', 'tail_id']].to_numpy() # Using numpy to speedup\n",
    "        \n",
    "        for h, r, t in data:\n",
    "            h, r, t = int(h), int(r), int(t)\n",
    "\n",
    "            self.adj_list[h].append((r, t))\n",
    "            self.adj_list[t].append((r + offset, h)) # Ex: r=5 (watched) -> r_inv=105 (watched_by)\n",
    "\n",
    "        print(f\"TCKG Loaded successfully. Graph construction complete.\")\n",
    "\n",
    "    def get_neighbors(self, node_id):\n",
    "        return self.adj_list[node_id]\n",
    "\n",
    "    def get_all_nodes(self):\n",
    "        return list(self.adj_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAwareRewardFunction(nn.Module):\n",
    "    def __init__(self, k, entity_embs, relation_embs, interaction_cluster_ids, bias_embs=None, temperature= None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_embs (nn.Embedding): Embedding của User (e_u)\n",
    "            entity_embs (nn.Embedding): Embedding của Item/Entity (e_v)\n",
    "            relation_embs (nn.Embedding): Embedding của Relation (dùng để lấy V_U)\n",
    "            interaction_cluster_ids (list or tensor): Danh sách các Relation ID đại diện cho Time Clusters.\n",
    "                                                      Ví dụ: [20, 21, 22] ứng với interacted_0, interacted_1...\n",
    "                                                      Đây chính là tập {V_U^1, ..., V_U^L}\n",
    "            bias_embs (nn.Embedding, optional): Bias của entity (b_v). Nếu None sẽ tự khởi tạo.\n",
    "        \"\"\"\n",
    "        super(TimeAwareRewardFunction, self).__init__()\n",
    "        \n",
    "        self.user_embs = user_embs\n",
    "        self.entity_embs = entity_embs\n",
    "        self.relation_embs = relation_embs\n",
    "        \n",
    "        # Danh sách ID của các cluster tương tác theo thời gian (V_U)\n",
    "        # Chuyển thành tensor để tính toán song song\n",
    "        self.register_buffer('cluster_ids', torch.tensor(interaction_cluster_ids, dtype=torch.long))\n",
    "        \n",
    "        # Entity Bias (b_v) - Eq (11)\n",
    "        if bias_embs is None:\n",
    "            num_entities = entity_embs.num_embeddings\n",
    "            self.bias_embs = nn.Embedding(num_entities, 1)\n",
    "            nn.init.zeros_(self.bias_embs.weight) # Khởi tạo bias bằng 0\n",
    "        else:\n",
    "            self.bias_embs = bias_embs\n",
    "\n",
    "        if temperature is None:\n",
    "            self.temperature = self.entity_embs.embedding_dim ** 0.5\n",
    "        else:\n",
    "            self.temperature = temperature\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_weights(self, history_relation_ids):\n",
    "        \"\"\"\n",
    "        Thực hiện Eq (13): Tính trọng số w_h dựa trên tần suất xuất hiện trong lịch sử.\n",
    "        \n",
    "        Args:\n",
    "            history_relation_ids: (Batch, Max_History_Len) - Chứa relation ID trong quá khứ của user.\n",
    "            \n",
    "        Returns:\n",
    "            weights: (Batch, Num_Clusters) - Vector W_hu\n",
    "        \"\"\"\n",
    "        # 1. So khớp History với Cluster IDs\n",
    "        # history: (B, H, 1)\n",
    "        # clusters: (1, 1, L)\n",
    "        # matches: (B, H, L) -> True nếu relation tại history khớp với cluster ID\n",
    "        hist_expanded = history_relation_ids.unsqueeze(-1)\n",
    "        clusters_expanded = self.cluster_ids.view(1, 1, -1)\n",
    "        \n",
    "        matches = (hist_expanded == clusters_expanded).float()\n",
    "        \n",
    "        # 2. Đếm số lần xuất hiện (Tử số Eq 13)\n",
    "        # Sum theo chiều History (dim=1) -> (B, L)\n",
    "        counts = matches.sum(dim=1) \n",
    "        \n",
    "        # 3. Tính độ dài q thực tế (Mẫu số Eq 13)\n",
    "        # q = tổng số lần xuất hiện của bất kỳ cluster nào trong history (tránh tính padding 0)\n",
    "        q = counts.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # 4. Normalize để ra trọng số (tránh chia cho 0)\n",
    "        weights = counts / (q + 1e-9)\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    def forward(self, user_ids, item_ids, history_relation_ids):\n",
    "        \"\"\"\n",
    "        Tính Reward Score g_R(v | u)\n",
    "        \n",
    "        Args:\n",
    "            user_ids: (Batch,)\n",
    "            item_ids: (Batch,) - Item đích (v_hat) mà Agent dự đoán/dừng lại\n",
    "            history_relation_ids: (Batch, History_Len) - Lịch sử relation của user\n",
    "            \n",
    "        Returns:\n",
    "            scores: (Batch,) - Điểm reward\n",
    "        \"\"\"\n",
    "        # --- BƯỚC 1: Lấy Embeddings cơ bản ---\n",
    "        u_e = self.user_embs(user_ids)       # (B, Dim) -> e_u\n",
    "        v_e = self.entity_embs(item_ids)     # (B, Dim) -> e_v\n",
    "        v_b = self.bias_embs(item_ids).squeeze(-1) # (B,) -> b_v\n",
    "        \n",
    "        # --- BƯỚC 2: Tính Personalized Interaction Relation (Eq 12) ---\n",
    "        # r_vu^T = W_hu * V_U\n",
    "        \n",
    "        # a. Tính weights (B, L)\n",
    "        weights = self.calculate_weights(history_relation_ids)\n",
    "        \n",
    "        # b. Lấy embedding của các cluster V_U^1...L\n",
    "        # cluster_embs shape: (L, Dim)\n",
    "        cluster_embs = self.relation_embs(self.cluster_ids)\n",
    "        \n",
    "        # c. Tính tổng có trọng số\n",
    "        # (B, L) x (L, Dim) -> (B, Dim)\n",
    "        r_interaction = torch.matmul(weights, cluster_embs)\n",
    "        \n",
    "        # --- BƯỚC 3: Tính Score (Eq 11 & Final Eq) ---\n",
    "        # g = (e_u + r_interaction) . e_v + b_v\n",
    "        \n",
    "        # Dot product: (e_u + r) * e_v\n",
    "        query_vector = u_e + r_interaction # (B, Dim)\n",
    "        dot_product = torch.sum(query_vector * v_e, dim=1) # (B,)\n",
    "        \n",
    "        scores = dot_product + v_b # Cộng bias\n",
    "        \n",
    "        # 1. Scale Score (Chia cho nhiệt độ)\n",
    "        scaled_score = scores / self.temperature\n",
    "        \n",
    "        # 2. Áp dụng Sigmoid\n",
    "        rewards = torch.sigmoid(scaled_score)\n",
    "        \n",
    "        return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911afabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPRecEnvironment(nn.Module):\n",
    "    def __init__(self, tckg, entity_embeddings, relation_embeddings, max_path_len=3, history_len=3):\n",
    "        \"\"\"\n",
    "        tckg: TCKG object containing adj_list.\n",
    "        entity_embeddings: nn.Embedding (ll entities)\n",
    "        relation_embeddings: nn.Embedding (all relations)\n",
    "        max_path_len: \n",
    "        history_len: (k' in paper)\n",
    "        \"\"\"\n",
    "        super(TPRecEnvironment, self).__init__()\n",
    "        self.tckg = tckg\n",
    "        self.entity_embs = entity_embeddings\n",
    "        self.relation_embs = relation_embeddings\n",
    "        \n",
    "        self.max_path_len = max_path_len\n",
    "        self.history_len = history_len\n",
    "        \n",
    "        # State tracking\n",
    "        self.current_entities = None # e_k\n",
    "        self.current_users = None    # u\n",
    "        self.histories = None        # h_k (lưu chuỗi relation và entity)\n",
    "        self.step_counter = 0\n",
    "\n",
    "    def reset(self, user_ids):\n",
    "        \"\"\"\n",
    "        Initiate status s_0 = (u, u, ∅)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = user_ids.size(0)\n",
    "\n",
    "        self.current_users = user_ids       # Multiple values because of batch_size\n",
    "        self.current_entities = user_ids    # Initiate status s_0 = (u, u, ∅), the 2nd value is user_ids as well\n",
    "        \n",
    "        # History h_k: store (relation, entity)\n",
    "        # Using torch.zeros for empty ∅\n",
    "        self.path_history = torch.zeros((batch_size, self.history_len * 2), dtype=torch.long)\n",
    "\n",
    "        self.step_counter = 0        \n",
    "\n",
    "        \n",
    "        return self._get_state_embedding()\n",
    "\n",
    "    def _get_state_embedding(self):\n",
    "        \"\"\"\n",
    "        State = [User_Emb, Flattened_History_Emb, Current_Entity_Emb]\n",
    "        Input cho Policy Network sẽ là vector nối dài của 3 thành phần này.\n",
    "        \"\"\"\n",
    "        # 1. User Embedding & Current Entity Embedding\n",
    "        # Shape: (Batch, Dim)\n",
    "        u_emb = self.entity_embs(self.current_users)\n",
    "        e_emb = self.entity_embs(self.current_entities)\n",
    "        \n",
    "        # 2. XỬ LÝ HISTORY (Lấy cả Relation và Entity)\n",
    "        # self.path_history shape: (Batch, history_len * 2) \n",
    "        # Cấu trúc: [r1, e1, r2, e2, r3, e3]\n",
    "        \n",
    "        # Bước 2a: Tách index của Relation và Entity bằng kỹ thuật Slicing\n",
    "        # Lấy các cột ở vị trí chẵn (0, 2, 4...) -> Relation Indices\n",
    "        r_indices = self.path_history[:, 0::2] # Shape: (Batch, history_len)\n",
    "        \n",
    "        # Lấy các cột ở vị trí lẻ (1, 3, 5...) -> Entity Indices\n",
    "        e_indices = self.path_history[:, 1::2] # Shape: (Batch, history_len)\n",
    "        \n",
    "        # Bước 2b: Lookup Embedding\n",
    "        # Shape: (Batch, history_len, Dim)\n",
    "        r_vecs = self.relation_embs(r_indices) \n",
    "        e_vecs = self.entity_embs(e_indices)\n",
    "        \n",
    "        # Bước 2c: Kết hợp Relation và Entity tại mỗi bước\n",
    "        # Cách tốt nhất: Nối (Concat) vector r và e lại với nhau\n",
    "        # Shape: (Batch, history_len, Rel_Dim + Ent_Dim)\n",
    "        step_vecs = torch.cat([r_vecs, e_vecs], dim=2)\n",
    "        \n",
    "        # Bước 2d: Làm phẳng (Flatten) toàn bộ lịch sử thành 1 vector dài\n",
    "        # Vì ta muốn giữ thứ tự: bước 1 khác bước 3\n",
    "        # Shape: (Batch, history_len * (Rel_Dim + Ent_Dim))\n",
    "        batch_size = step_vecs.size(0)\n",
    "        h_emb_flat = step_vecs.view(batch_size, -1)\n",
    "\n",
    "        # 3. KẾT HỢP TẤT CẢ (Concatenate)\n",
    "        # State = [User (Dim) + History (Len*2*Dim) + Current Entity (Dim)]\n",
    "        state_vector = torch.cat([u_emb, h_emb_flat, e_emb], dim=1) #[r1, e1, r2, e2, r3, e3]\n",
    "        \n",
    "        return state_vector\n",
    "\n",
    "    def get_pruned_actions(self, epsilon=10):\n",
    "        \"\"\"\n",
    "        Thực hiện Eq (8): Pruning function g_k((r, e_{k+1}) | u)\n",
    "        Cắt tỉa không gian hành động, chỉ giữ lại top-epsilon neighbors tốt nhất.\n",
    "        \"\"\"\n",
    "        batch_size = self.current_users.size(0)\n",
    "        valid_actions = []\n",
    "        \n",
    "        # Lấy embedding cần thiết cho công thức Eq(8)\n",
    "        # g_k = (e_u + sum(r_k)) * e_{k+1} + b_{k+1}\n",
    "        # Lưu ý: sum(r_k) là tổng các relation trong lịch sử (path composition)\n",
    "        \n",
    "        u_emb = self.entity_embs(self.current_users) # (B, dim)\n",
    "        \n",
    "        # Giả sử ta đã tính được tổng relation embedding của path hiện tại\n",
    "        # path_rel_sum: (B, dim) \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            u_id = self.current_users[i].item()\n",
    "            curr_node = self.current_entities[i].item()\n",
    "            \n",
    "            # Lấy tất cả neighbors của node hiện tại\n",
    "            neighbors = self.kg.get_neighbors(curr_node) # List các tuple (relation, next_node)\n",
    "            \n",
    "            if not neighbors:\n",
    "                valid_actions.append([]) # Dead end\n",
    "                continue\n",
    "                \n",
    "            # Tách relation và entity node ra\n",
    "            rels = torch.tensor([n[0] for n in neighbors])\n",
    "            next_nodes = torch.tensor([n[1] for n in neighbors])\n",
    "            \n",
    "            # Tính Score theo Eq (8)\n",
    "            # Query vector = User + Path_History_Relation\n",
    "            # Ở đây minh họa đơn giản là User + Relation hiện tại (thực tế cần cộng dồn history)\n",
    "            r_emb = self.relation_embs(rels)\n",
    "            next_node_emb = self.entity_embs(next_nodes)\n",
    "            \n",
    "            # (e_u + r) * e_{next} (Dot product)\n",
    "            # score shape: (num_neighbors,)\n",
    "            query = u_emb[i] + r_emb \n",
    "            scores = torch.sum(query * next_node_emb, dim=1) \n",
    "            \n",
    "            # Chọn top epsilon\n",
    "            k = min(epsilon, len(scores))\n",
    "            top_scores, top_indices = torch.topk(scores, k)\n",
    "            \n",
    "            # Lưu lại danh sách hành động hợp lệ cho user i\n",
    "            actions_i = []\n",
    "            for idx in top_indices:\n",
    "                actions_i.append((rels[idx], next_nodes[idx]))\n",
    "            valid_actions.append(actions_i)\n",
    "            \n",
    "        return valid_actions\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Transition function (Eq 9): Chuyển trạng thái sang bước k+1\n",
    "        \n",
    "        Args:\n",
    "            actions: List các tuple (relation_id, next_node_id) có độ dài bằng batch_size.\n",
    "                     Đây là hành động a_k mà Agent vừa chọn.\n",
    "        \n",
    "        Returns:\n",
    "            next_state_emb: Vector trạng thái s_{k+1}\n",
    "            done: Boolean (True nếu đã đi hết số bước quy định)\n",
    "        \"\"\"\n",
    "        device = self.current_entities.device\n",
    "        batch_size = len(actions)\n",
    "        \n",
    "        # 1. Tách Relation và Entity từ Actions ra thành 2 Tensors riêng biệt\n",
    "        # actions là list [(r1, e1), (r2, e2)...] -> cần zip lại\n",
    "        # next_rels: (Batch,)\n",
    "        # next_ents: (Batch,)\n",
    "        rels_list = [a[0] for a in actions]\n",
    "        ents_list = [a[1] for a in actions]\n",
    "        \n",
    "        next_relations = torch.tensor(rels_list, dtype=torch.long, device=device)\n",
    "        next_entities = torch.tensor(ents_list, dtype=torch.long, device=device)\n",
    "        \n",
    "        # 2. CẬP NHẬT LỊCH SỬ (History Update - Sliding Window)\n",
    "        # h_k hiện tại có shape (Batch, history_len * 2)\n",
    "        # Cấu trúc h_k: [r_{old}, e_{old}, ..., r_{k-1}, e_{k-1}]\n",
    "        \n",
    "        # Bước 2a: Tạo cặp (r_k, e_k) mới để nối vào đuôi\n",
    "        # Shape: (Batch, 1)\n",
    "        new_r = next_relations.unsqueeze(1)\n",
    "        new_e = next_entities.unsqueeze(1)\n",
    "        # Shape: (Batch, 2) -> Mỗi dòng là [r_k, e_k]\n",
    "        new_entry = torch.cat([new_r, new_e], dim=1)\n",
    "        \n",
    "        # Bước 2b: Thực hiện trượt cửa sổ\n",
    "        # Cắt bỏ 2 phần tử đầu tiên (cũ nhất) của lịch sử hiện tại\n",
    "        # history_shifted shape: (Batch, (history_len - 1) * 2)\n",
    "        history_shifted = self.path_history[:, 2:]\n",
    "        \n",
    "        # Nối phần mới vào đuôi\n",
    "        # self.path_history mới shape: (Batch, history_len * 2)\n",
    "        self.path_history = torch.cat([history_shifted, new_entry], dim=1)\n",
    "        \n",
    "        # 3. Cập nhật vị trí hiện tại của Agent (Entity e_k)\n",
    "        self.current_entities = next_entities\n",
    "        \n",
    "        # 4. Kiểm tra điều kiện dừng\n",
    "        self.step_counter += 1\n",
    "        done = (self.step_counter >= self.max_path_len)\n",
    "        \n",
    "        # 5. Trả về State Embedding mới (s_{k+1})\n",
    "        return self._get_state_embedding(), done\n",
    "\n",
    "    def calculate_reward(self, time_aware_scoring_func):\n",
    "        \"\"\"\n",
    "        Reward Function (Eq 10): Soft Reward\n",
    "        R_K = g_R(e_K | u) / max(g_R(v | u))\n",
    "        \"\"\"\n",
    "        # e_K chính là self.current_entities tại bước cuối cùng\n",
    "        \n",
    "        # Tính tử số: Score của entity mà Agent dừng lại\n",
    "        final_scores = time_aware_scoring_func(self.current_users, self.current_entities)\n",
    "        \n",
    "        # Tính mẫu số: Max score có thể đạt được (thường đã pre-calculate hoặc xấp xỉ)\n",
    "        # Trong thực tế, để nhanh, người ta thường dùng Softmax hoặc Sigmoid của score \n",
    "        # thay vì chia cho max exact (vì tìm max tốn kém).\n",
    "        # Nhưng để đúng công thức Eq 10:\n",
    "        max_scores = time_aware_scoring_func.get_max_score_per_user(self.current_users)\n",
    "        \n",
    "        rewards = final_scores / (max_scores + 1e-9) # Tránh chia cho 0\n",
    "        \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef37e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. POLICY NETWORK (The Brain)\n",
    "# ==========================================\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # Input: [State_Emb; Relation_Emb; Next_Node_Emb]\n",
    "        # Output: Score scalar\n",
    "        self.fc1 = nn.Linear(state_dim * 3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1) # Điểm số cho hành động\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, state_emb, action_embs):\n",
    "        # state_emb: (1, dim)\n",
    "        # action_embs (Neighbors): (num_neighbors, dim * 2) gồm [Rel; Next_Node]\n",
    "        \n",
    "        # Expand state to match neighbors count\n",
    "        num_actions = action_embs.shape[0]\n",
    "        curr_state = state_emb.repeat(num_actions, 1) # (num_neighbors, dim)\n",
    "        \n",
    "        # Concat: [Current State, Relation, Next Node]\n",
    "        x = torch.cat([curr_state, action_embs], dim=1) # (num_neighbors, dim*3)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        scores = self.fc2(x) # (num_neighbors, 1)\n",
    "        \n",
    "        # Softmax để ra xác suất chọn từng neighbor\n",
    "        probs = F.softmax(scores.view(-1), dim=0)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. TRAINING LOOP (REINFORCE)\n",
    "# ==========================================\n",
    "def train(env, policy_net, train_df, episodes=50):\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "    \n",
    "    print(\"\\n>>> Start Training...\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Sample batch user để train cho nhanh (Stochastic)\n",
    "        batch_samples = train_df.sample(n=256) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = 0\n",
    "        \n",
    "        for _, row in batch_samples.iterrows():\n",
    "            curr_node = row['head_id']\n",
    "            target_node = row['tail_id']\n",
    "            \n",
    "            episode_log_probs = []\n",
    "            rewards = []\n",
    "            \n",
    "            # --- Walking ---\n",
    "            done = False\n",
    "            for step in range(MAX_STEPS):\n",
    "                # 1. Get State\n",
    "                state = env.get_state(curr_node, step) # (1, dim)\n",
    "                \n",
    "                # 2. Get Valid Actions (Neighbors)\n",
    "                neighbors, rels, neighbor_ids = env.get_valid_actions(curr_node)\n",
    "                \n",
    "                if len(neighbors) == 0:\n",
    "                    break # Dead end\n",
    "                \n",
    "                # 3. Tạo Embedding cho Actions để đưa vào mạng\n",
    "                neighbor_tensor = torch.tensor(neighbor_ids, dtype=torch.long)\n",
    "                rel_tensor = torch.tensor(rels, dtype=torch.long)\n",
    "                \n",
    "                neigh_embeds = env.node_embeds(neighbor_tensor)\n",
    "                rel_embeds = env.rel_embeds(rel_tensor)\n",
    "                action_features = torch.cat([rel_embeds, neigh_embeds], dim=1) # (num_neighbors, dim*2)\n",
    "                \n",
    "                # 4. Agent chọn hành động\n",
    "                probs = policy_net(state, action_features)\n",
    "                \n",
    "                # Sampling action dựa trên xác suất (Exploration)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                action_idx = dist.sample()\n",
    "                log_prob = dist.log_prob(action_idx)\n",
    "                \n",
    "                episode_log_probs.append(log_prob)\n",
    "                \n",
    "                # 5. Execute Action\n",
    "                next_node = neighbors[action_idx.item()]\n",
    "                curr_node = next_node\n",
    "                \n",
    "                # 6. Check Reward\n",
    "                if curr_node == target_node:\n",
    "                    rewards.append(1.0) # Tìm thấy!\n",
    "                    done = True\n",
    "                    break\n",
    "                else:\n",
    "                    rewards.append(0.0) # Chưa thấy\n",
    "            \n",
    "            # --- Tính Loss (Policy Gradient) ---\n",
    "            # Nếu tìm thấy đích ở bước cuối, phần thưởng lan truyền ngược lại\n",
    "            # Discounted Return\n",
    "            R = 0\n",
    "            returns = []\n",
    "            for r in reversed(rewards):\n",
    "                R = r + GAMMA * R\n",
    "                returns.insert(0, R)\n",
    "            \n",
    "            if done: # Chỉ học nếu tìm thấy đích (hoặc có thể phạt nhẹ nếu không tìm thấy)\n",
    "                total_reward += 1\n",
    "                for log_prob, R in zip(episode_log_probs, returns):\n",
    "                    batch_loss -= log_prob * R # Gradient Ascent -> Minimize Negative Reward\n",
    "        \n",
    "        # Update Weights\n",
    "        if batch_loss != 0:\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (episode+1) % 5 == 0:\n",
    "            print(f\"Episode {episode+1}/{episodes} | Hit Success: {total_reward}/256 | Batch Loss: {batch_loss:.4f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ee08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy_net, test_df, top_k=10):\n",
    "    print(\"\\n>>> Start Evaluation...\")\n",
    "    policy_net.eval() # Chuyển sang chế độ đánh giá (tắt dropout nếu có)\n",
    "    \n",
    "    hits = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Lấy mẫu ngẫu nhiên từ Test set để chạy cho nhanh (hoặc chạy hết nếu muốn chính xác)\n",
    "    test_samples = test_df.sample(n=100) if len(test_df) > 100 else test_df\n",
    "    \n",
    "    with torch.no_grad(): # Tắt tính toán gradient để tiết kiệm bộ nhớ\n",
    "        for _, row in tqdm(test_samples.iterrows(), total=len(test_samples)):\n",
    "            curr_node = row['head_id']\n",
    "            target_true = row['tail_id']\n",
    "            \n",
    "            # --- Beam Search (Giản lược: Greedy Best-First) ---\n",
    "            # Để đơn giản, ta cho Agent chọn top-1 đường đi tốt nhất\n",
    "            \n",
    "            for step in range(MAX_STEPS):\n",
    "                state = env.get_state(curr_node, step)\n",
    "                neighbors, rels, neighbor_ids = env.get_valid_actions(curr_node)\n",
    "                \n",
    "                if not neighbors: break \n",
    "                \n",
    "                # Chuẩn bị input cho mạng\n",
    "                neighbor_tensor = torch.tensor(neighbor_ids, dtype=torch.long)\n",
    "                rel_tensor = torch.tensor(rels, dtype=torch.long)\n",
    "                neigh_embeds = env.node_embeds(neighbor_tensor)\n",
    "                rel_embeds = env.rel_embeds(rel_tensor)\n",
    "                action_features = torch.cat([rel_embeds, neigh_embeds], dim=1)\n",
    "                \n",
    "                # Dự đoán xác suất\n",
    "                probs = policy_net(state, action_features)\n",
    "                \n",
    "                # --- KHÁC BIỆT: Chọn bước đi tốt nhất (Argmax) ---\n",
    "                # Thay vì random sample, ta chọn nước đi có xác suất cao nhất\n",
    "                best_action_idx = torch.argmax(probs).item()\n",
    "                \n",
    "                next_node = neighbors[best_action_idx]\n",
    "                curr_node = next_node\n",
    "                \n",
    "                # Kiểm tra xem node hiện tại có phải là đích không\n",
    "                if curr_node == target_true:\n",
    "                    hits += 1\n",
    "                    break\n",
    "            \n",
    "            total_samples += 1\n",
    "            \n",
    "    # Tính HR@1 (Hit Rate) - Tỉ lệ tìm thấy chính xác Item\n",
    "    acc = hits / total_samples if total_samples > 0 else 0\n",
    "    print(f\"Evaluation Rank 1 (Exact Match): {acc:.4f}\")\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e703784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# MAIN\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Data\n",
    "    static_kg, train_df, val_df, test_df = load_and_split_data()\n",
    "    \n",
    "    # 2. Init Env (Cần Embedding cho Node/Rel)\n",
    "    env = KGEnvironment(static_kg, train_df, embedding_dim=EMBEDDING_DIM)\n",
    "    \n",
    "    # 3. Init Agent\n",
    "    policy_net = PolicyNetwork(state_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM)\n",
    "    \n",
    "    # 4. Train\n",
    "    train(env, policy_net, train_df, episodes=50000) # Tăng episodes để thấy reward tăng\n",
    "\n",
    "    evaluate(env, policy_net, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
