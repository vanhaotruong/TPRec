{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GKjNoSnA8oy0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKjNoSnA8oy0",
        "outputId": "cf1f5917-7c54-48a1-afbe-4d0d12d3e07e"
      },
      "outputs": [],
      "source": [
        "# !pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d36101a0",
      "metadata": {
        "id": "d36101a0"
      },
      "outputs": [],
      "source": [
        "import random, os, pickle, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Set environment variables for reproducibility and safety\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "# 1. Configuration & Seeding\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edfb172",
      "metadata": {
        "id": "1edfb172"
      },
      "outputs": [],
      "source": [
        "name = 'book'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58552a75",
      "metadata": {},
      "source": [
        "#### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7252504",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # Đường dẫn dữ liệu\n",
        "    TCKG_PATH = f\"./data/{name}/{name}_TCKG.csv\"  # File CSV chứa đồ thị (như bạn đã gửi)\n",
        "    # TRAIN_USERS_PATH = \"data/train_users.csv\" # File chứa danh sách user ID dùng để train\n",
        "    SAVE_DIR = \"./checkpoints\"\n",
        "\n",
        "    ITEM_START_ID = 2891  # book: 2891 \n",
        "    ITEM_END_ID = 11584     # book: 11584\n",
        "\n",
        "    # Siêu tham số Model\n",
        "    EMBED_DIM = 64\n",
        "    HIDDEN_DIM = 256\n",
        "    HISTORY_LEN = 1   # k' (độ dài lịch sử)\n",
        "    MAX_PATH_LEN = 3  # K (số bước đi)\n",
        "\n",
        "    # Siêu tham số Training\n",
        "    BATCH_SIZE = 32 # Batch lớn giúp RL ổn định hơn\n",
        "    NUM_EPOCHS = 500\n",
        "    LEARNING_RATE = 1e-3\n",
        "    \n",
        "    # Thiết bị\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # IDs của các Relation Interaction (Quan trọng cho Reward)\n",
        "    # Ví dụ: 20=interacted_0, 21=interacted_1, 22=interacted_2\n",
        "    INTERACTION_CLUSTER_IDS = [21, 22, 23, 24]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N7_iFoFJGZT8",
      "metadata": {
        "id": "N7_iFoFJGZT8"
      },
      "source": [
        "#### TCKG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48479536",
      "metadata": {
        "id": "48479536"
      },
      "outputs": [],
      "source": [
        "class TCKG:\n",
        "    def __init__(self, tckg_csv_path):\n",
        "        self.adj_list = defaultdict(list)\n",
        "\n",
        "        print(f\"Loading TCKG from {tckg_csv_path}...\")\n",
        "        df_tckg = pd.read_csv(tckg_csv_path, usecols=['head_id', 'relation_id', 'tail_id'])\n",
        "\n",
        "        offset = int(df_tckg['relation_id'].max())\n",
        "\n",
        "        data = df_tckg[['head_id', 'relation_id', 'tail_id']].to_numpy() # Using numpy to speedup\n",
        "        for h, r, t in data:\n",
        "            h, r, t = int(h), int(r), int(t)\n",
        "\n",
        "            self.adj_list[h].append((r, t))\n",
        "            self.adj_list[t].append((r + offset, h)) # Ex: r=5 (watched) -> r_inv=105 (watched_by)\n",
        "\n",
        "        print(f\"TCKG Loaded successfully. Graph construction complete.\")\n",
        "\n",
        "    def get_neighbors(self, node_id):\n",
        "        return self.adj_list[node_id]\n",
        "\n",
        "    def get_all_nodes(self):\n",
        "        return list(self.adj_list.keys())\n",
        "        rels = self.rel_matrix[node_ids_tensor]\n",
        "        tails = self.tail_matrix[node_ids_tensor]\n",
        "        masks = self.mask_matrix[node_ids_tensor]\n",
        "        return rels, tails, masks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ux1Mk5j3GWNR",
      "metadata": {
        "id": "ux1Mk5j3GWNR"
      },
      "source": [
        "#### TimeAwareRewardFunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecfd9985",
      "metadata": {
        "id": "ecfd9985"
      },
      "outputs": [],
      "source": [
        "class TimeAwareRewardFunction(nn.Module):\n",
        "    def __init__(self, user_embs, entity_embs, relation_embs, interaction_cluster_ids, bias_embs=None, temperature= None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            user_embs (nn.Embedding): Embedding của User (e_u)self.max_neighbors = max(len(edges) for edges in self.adj_list.values())\n",
        "            entity_embs (nn.Embedding): Embedding của Item/Entity (e_v)\n",
        "            relation_embs (nn.Embedding): Embedding của Relation (dùng để lấy V_U)\n",
        "            interaction_cluster_ids (list or tensor): Danh sách các Relation ID đại diện cho Time Clusters.\n",
        "                                                      Ví dụ: [20, 21, 22] ứng với interacted_0, interacted_1...\n",
        "                                                      Đây chính là tập {V_U^1, ..., V_U^L}\n",
        "            bias_embs (nn.Embedding, optional): Bias của entity (b_v). Nếu None sẽ tự khởi tạo.\n",
        "        \"\"\"\n",
        "        super(TimeAwareRewardFunction, self).__init__()\n",
        "\n",
        "        self.user_embs = user_embs\n",
        "        self.entity_embs = entity_embs\n",
        "        self.relation_embs = relation_embs\n",
        "\n",
        "        # Danh sách ID của các cluster tương tác theo thời gian (V_U)\n",
        "        # Chuyển thành tensor để tính toán song song\n",
        "        self.register_buffer('cluster_ids', torch.tensor(interaction_cluster_ids, dtype=torch.long))\n",
        "\n",
        "        # Entity Bias (b_v) - Eq (11)\n",
        "        if bias_embs is None:\n",
        "            num_entities = entity_embs.num_embeddings - 1\n",
        "            self.bias_embs = nn.Embedding(num_entities + 1, 1, padding_idx = 0)\n",
        "            nn.init.zeros_(self.bias_embs.weight) # Khởi tạo bias bằng 0\n",
        "        else:\n",
        "            self.bias_embs = bias_embs\n",
        "\n",
        "        if temperature is None:\n",
        "            self.temperature = self.entity_embs.embedding_dim ** 0.5\n",
        "        else:\n",
        "            self.temperature = temperature\n",
        "\n",
        "    def calculate_weights(self, history_relation_ids):\n",
        "        \"\"\"\n",
        "        Thực hiện Eq (13): Tính trọng số W_hu dựa trên tần suất tương tác chiều Tiến.\n",
        "        \n",
        "        Args:\n",
        "            history_relation_ids: Tensor (Batch, Max_History_Len) - Các relation ID từ user history (đã pad 0).\n",
        "            \n",
        "        Returns:\n",
        "            weights: Tensor (Batch, L) - Vector trọng số thời gian cho L cụm.\n",
        "        \"\"\"\n",
        "        # 1. Chuẩn bị Tensor để Broadcast (So khớp song song trên GPU)\n",
        "        # hist_expanded: (Batch, History_Len, 1)\n",
        "        hist_expanded = history_relation_ids.unsqueeze(-1)\n",
        "        \n",
        "        # clusters_expanded: (1, 1, L) - Chứa [21, 22, 23, 24]\n",
        "        clusters_expanded = self.cluster_ids.view(1, 1, -1)\n",
        "        \n",
        "        # 2. So khớp và Đếm (Tử số của Eq 13)\n",
        "        # matches: (Batch, History_Len, L) -> True/1.0 nếu khớp ID cụm thời gian\n",
        "        matches = (hist_expanded == clusters_expanded).float()\n",
        "        \n",
        "        # counts: (Batch, L) -> Tổng số lần user tương tác vào mỗi cụm\n",
        "        counts = matches.sum(dim=1)\n",
        "\n",
        "        # 3. Tính tổng số tương tác thực tế q (Mẫu số của Eq 13)\n",
        "        # Bằng cách tính tổng của 'counts', ta tự động lờ đi toàn bộ Padding (ID 0) \n",
        "        # và các relation ID rác (nếu có), vì chúng không match với forward_clusters.\n",
        "        # q: (Batch, 1)\n",
        "        q = counts.sum(dim=1, keepdim=True)\n",
        "\n",
        "        # 4. Chuẩn hóa để ra trọng số (Thêm 1e-9 để tránh lỗi chia cho 0 nếu user chưa có lịch sử)\n",
        "        # weights: (Batch, L) -> Đảm bảo tổng các phần tử trên mỗi hàng luôn = 1.0 (hoặc 0)\n",
        "        weights = counts / (q + 1e-9)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def forward(self, user_ids, item_ids, history_relation_ids):\n",
        "        \"\"\"\n",
        "        Tính Reward Score g_R(v | u)\n",
        "\n",
        "        Args:\n",
        "            user_ids: (Batch,)\n",
        "            item_ids: (Batch,) - Item đích (v_hat) mà Agent dự đoán/dừng lại\n",
        "            history_relation_ids: (Batch, History_Len) - Lịch sử relation của user\n",
        "\n",
        "        Returns:\n",
        "            scores: (Batch,) - Điểm reward\n",
        "        \"\"\"\n",
        "        # --- BƯỚC 1: Lấy Embeddings cơ bản ---\n",
        "        u_e = self.user_embs(user_ids)       # (B, Dim) -> e_u\n",
        "        \n",
        "        v_e = self.entity_embs(item_ids)     # (B, Dim) -> e_v\n",
        "        v_b = self.bias_embs(item_ids).squeeze(-1) # (B,) -> b_v\n",
        "\n",
        "        # --- BƯỚC 2: Tính Personalized Interaction Relation (Eq 12) ---\n",
        "        # r_vu^T = W_hu * V_U\n",
        "\n",
        "        # a. Tính weights (B, L)\n",
        "        weights = self.calculate_weights(history_relation_ids)\n",
        "\n",
        "        # b. Lấy embedding của các cluster V_U^1...L\n",
        "        # cluster_embs shape: (L, Dim)\n",
        "        cluster_embs = self.relation_embs(self.cluster_ids)\n",
        "\n",
        "        # c. Tính tổng có trọng số\n",
        "        # (B, L) x (L, Dim) -> (B, Dim)\n",
        "        r_interaction = torch.matmul(weights, cluster_embs)\n",
        "\n",
        "        # --- BƯỚC 3: Tính Score (Eq 11 & Final Eq) ---\n",
        "        # g = (e_u + r_interaction) . e_v + b_v\n",
        "\n",
        "        # Dot product: (e_u + r) * e_v\n",
        "        query_vector = u_e + r_interaction # (B, Dim)\n",
        "        dot_product = torch.sum(query_vector * v_e, dim=1) # (B,)\n",
        "\n",
        "        scores = dot_product + v_b # Cộng bias\n",
        "\n",
        "        # 1. Scale Score (Chia cho nhiệt độ)\n",
        "        scaled_score = scores / self.temperature\n",
        "\n",
        "        # 2. Áp dụng Sigmoid\n",
        "        rewards = torch.sigmoid(scaled_score)\n",
        "\n",
        "        return rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CPZGKTCOGFVl",
      "metadata": {
        "id": "CPZGKTCOGFVl"
      },
      "source": [
        "#### TPRecEnvironment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "911afabe",
      "metadata": {
        "id": "911afabe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class TPRecEnvironment(nn.Module):\n",
        "    def __init__(self, tckg, entity_embeddings, relation_embeddings, reward_function, max_path_len, history_len, item_start_id, item_end_id):\n",
        "        \"\"\"\n",
        "        Môi trường TPRec được thiết kế chuẩn xác theo kiến trúc Sequence State\n",
        "        \"\"\"\n",
        "        super(TPRecEnvironment, self).__init__()\n",
        "        self.tckg = tckg\n",
        "        self.entity_embs = entity_embeddings \n",
        "        self.relation_embs = relation_embeddings\n",
        "        self.max_path_len = max_path_len\n",
        "        self.history_len = history_len\n",
        "        self.item_start_id = item_start_id\n",
        "        self.item_end_id = item_end_id\n",
        "\n",
        "        # LƯU REWARD FUNCTION ĐƯỢC TRUYỀN VÀO\n",
        "        self.reward_function = reward_function\n",
        "\n",
        "        # State tracking\n",
        "        self.current_entities = None\n",
        "        self.current_users = None\n",
        "        self.path_history = None\n",
        "        self.step_counter = 0\n",
        "\n",
        "    def reset(self, user_ids):\n",
        "        \"\"\"\n",
        "        Khởi tạo trạng thái s_0 = (u, u, ∅)\n",
        "        Cập nhật: Nạp thêm target_items để Trọng tài (Môi trường) cầm sẵn đáp án\n",
        "        \"\"\"\n",
        "        batch_size = user_ids.size(0)\n",
        "\n",
        "        self.current_users = user_ids\n",
        "        self.current_entities = user_ids\n",
        "\n",
        "        # History h_k: store (entity, relation) theo chuẩn paper\n",
        "        self.path_history = torch.zeros((batch_size, self.history_len * 2),     # history_len = k' in paper\n",
        "                                        dtype=torch.long,\n",
        "                                        device=user_ids.device)\n",
        "\n",
        "        self.step_counter = 0\n",
        "        return self._get_state_embedding()\n",
        "\n",
        "    def _get_state_embedding(self):\n",
        "        \"\"\"\n",
        "        Kết hợp u, h_k, e_k thành một chuỗi duy nhất đưa vào BLSTM\n",
        "        \"\"\"\n",
        "        # 1. Thêm chiều thời gian (unsqueeze) cho u và e_k\n",
        "        u_emb = self.entity_embs(self.current_users).unsqueeze(1)    # Shape: (B, 1, d)\n",
        "        e_emb = self.entity_embs(self.current_entities).unsqueeze(1) # Shape: (B, 1, d)\n",
        "\n",
        "        # 2. Lấy lịch sử e và r\n",
        "        e_indices = self.path_history[:, 0::2]\n",
        "        r_indices = self.path_history[:, 1::2]\n",
        "\n",
        "        e_vecs = self.entity_embs(e_indices)   # Shape: (B, L, d)\n",
        "        r_vecs = self.relation_embs(r_indices) # Shape: (B, L, d)\n",
        "\n",
        "        # 3. Trộn xen kẽ e và r thành chuỗi lịch sử [e1, r2, e2, r3]\n",
        "        B, L, d = e_vecs.shape\n",
        "        history_seq = torch.zeros((B, L * 2, d), device=e_vecs.device)\n",
        "        history_seq[:, 0::2, :] = e_vecs\n",
        "        history_seq[:, 1::2, :] = r_vecs\n",
        "\n",
        "        # 4. KẾT NỐI TOÀN BỘ THEO ĐÚNG CÔNG THỨC S_K TRONG PAPER\n",
        "        full_state_seq = torch.cat([u_emb, history_seq, e_emb], dim=1) # Shape: (B, Seq_Len, d)\n",
        "\n",
        "        # Trả về ĐÚNG 1 BIẾN DUY NHẤT đại diện cho S_k\n",
        "        return full_state_seq\n",
        "\n",
        "    def get_pruned_actions(self, epsilon=15):   # used by def get_action_space_batch\n",
        "        \"\"\"\n",
        "        Phiên bản Tối ưu hóa Vectorization (GPU Accelerated)\n",
        "        \"\"\"\n",
        "        batch_size = self.current_users.size(0)\n",
        "        device = self.current_users.device\n",
        "\n",
        "        # 1. KÉO VỀ CPU MỘT LẦN DUY NHẤT: Tránh gọi .item() N lần\n",
        "        curr_nodes_cpu = self.current_entities.tolist()\n",
        "\n",
        "        # Tra cứu láng giềng cực nhanh trên CPU bằng List Comprehension\n",
        "        batch_neighbors = [self.tckg.get_neighbors(node) for node in curr_nodes_cpu]\n",
        "\n",
        "\n",
        "        # Tìm node có số lượng láng giềng lớn nhất trong batch này\n",
        "        lengths = [len(n) for n in batch_neighbors]\n",
        "        max_len = max(lengths) if lengths else 0\n",
        "\n",
        "        valid_actions = []\n",
        "\n",
        "        # Nếu toàn bộ batch đều rơi vào ngõ cụt\n",
        "        if max_len == 0:\n",
        "            return [[] for _ in range(batch_size)]\n",
        "\n",
        "        # 2. TẠO MA TRẬN PADDING TRÊN CPU\n",
        "        batch_rels = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "        batch_next_nodes = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "        mask = torch.zeros((batch_size, max_len), dtype=torch.bool)\n",
        "\n",
        "        # Điền dữ liệu vào ma trận\n",
        "        for i, neighbors in enumerate(batch_neighbors):\n",
        "            num_n = lengths[i]\n",
        "            if num_n > 0:\n",
        "                batch_rels[i, :num_n] = torch.tensor([n[0] for n in neighbors])            #neighbor: head -> (r, t) rel use 0, node use 1\n",
        "                batch_next_nodes[i, :num_n] = torch.tensor([n[1] for n in neighbors])\n",
        "                mask[i, :num_n] = True\n",
        "\n",
        "        # 3. ĐẨY TOÀN BỘ MA TRẬN LÊN GPU MỘT LẦN DUY NHẤT\n",
        "        batch_rels = batch_rels.to(device)\n",
        "        batch_next_nodes = batch_next_nodes.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        # 4. TÍNH TOÁN SONG SONG BẰNG MA TRẬN TRÊN GPU\n",
        "        curr_emb = self.entity_embs(self.current_entities) # Lấy node HIỆN TẠI (Đã sửa)\n",
        "        r_emb = self.relation_embs(batch_rels)\n",
        "        n_emb = self.entity_embs(batch_next_nodes)\n",
        "\n",
        "        # Tính Query: Cần unsqueeze curr_emb để cộng broadcast với r_emb\n",
        "        query = curr_emb.unsqueeze(1) + r_emb\n",
        "\n",
        "        # LỖI CŨ CẦN XÓA: scores = torch.sum(query * n_emb, dim=-1)\n",
        "\n",
        "        # CÁCH CHUẨN MỚI DÀNH CHO TRANSE:\n",
        "        # Tính khoảng cách L2 (Euclidean distance) giữa (h+r) và t\n",
        "        # Khoảng cách càng nhỏ càng tốt -> Thêm dấu trừ (-) để hàm topk chọn giá trị gần 0 nhất\n",
        "        distances = torch.norm(query - n_emb, p=2, dim=-1)\n",
        "        scores = -distances\n",
        "\n",
        "        # CHE PADDING (Masking)\n",
        "        scores = scores.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "        # 5. CHỌN TOP-K CHO CẢ BATCH\n",
        "        k = min(epsilon, max_len)\n",
        "        top_scores, top_indices = torch.topk(scores, k, dim=1) # (B, k)\n",
        "\n",
        "        # 6. KÉO KẾT QUẢ VỀ LẠI CPU\n",
        "        top_indices_cpu = top_indices.tolist()\n",
        "        batch_rels_cpu = batch_rels.tolist()\n",
        "        batch_nodes_cpu = batch_next_nodes.tolist()\n",
        "\n",
        "        # Giải nén thành dạng List gốc\n",
        "        for i in range(batch_size):\n",
        "            actions_i = []\n",
        "            num_real = lengths[i]\n",
        "\n",
        "            if num_real > 0:\n",
        "                valid_k = min(k, num_real)\n",
        "                for j in range(valid_k):\n",
        "                    idx = top_indices_cpu[i][j]\n",
        "                    actions_i.append((batch_rels_cpu[i][idx], batch_nodes_cpu[i][idx]))\n",
        "\n",
        "            valid_actions.append(actions_i)\n",
        "\n",
        "        return valid_actions\n",
        "\n",
        "    def get_action_space_batch(self):\n",
        "        \"\"\"\n",
        "        Lấy không gian hành động cho cả Batch và Padding thành Tensor.\n",
        "        Tích hợp LUẬT CHỐNG LẶP VÒNG (Cycle Prevention) cực mượt.\n",
        "        \"\"\"\n",
        "        raw_actions_list = self.get_pruned_actions()\n",
        "        batch_size = len(raw_actions_list)\n",
        "\n",
        "        lengths = [len(acts) for acts in raw_actions_list]\n",
        "        max_len = max(lengths) if lengths else 0\n",
        "        if max_len == 0:\n",
        "            max_len = 1\n",
        "\n",
        "        device = self.current_entities.device\n",
        "\n",
        "        r_indices = torch.zeros((batch_size, max_len), dtype=torch.long, device=device)\n",
        "        e_indices = torch.zeros((batch_size, max_len), dtype=torch.long, device=device)\n",
        "        action_mask = torch.zeros((batch_size, max_len), dtype=torch.float, device=device)\n",
        "\n",
        "        # [MỚI] Lấy lịch sử các Entity đã đi qua (Chỉ lấy index chẵn trong path_history)\n",
        "        if self.path_history is not None:\n",
        "            visited_entities = self.path_history[:, 0::2].tolist()\n",
        "        else:\n",
        "            visited_entities = [[] for _ in range(batch_size)]\n",
        "\n",
        "        for i, actions in enumerate(raw_actions_list): # i là user thứ i trong batch\n",
        "            num_acts = len(actions)\n",
        "            if num_acts > 0:\n",
        "                # =====================================================================\n",
        "                # 1. TẠO DANH SÁCH ĐEN (BLACKLIST) CHO USER i\n",
        "                # =====================================================================\n",
        "                blacklist = set(visited_entities[i])            # Các node trong lịch sử\n",
        "                blacklist.add(self.current_users[i].item())     # Cấm quay về User gốc\n",
        "                blacklist.add(self.current_entities[i].item())  # Cấm dẫm tại chỗ (Self-loop)\n",
        "\n",
        "                rs = []\n",
        "                es = []\n",
        "                valid_masks = []\n",
        "\n",
        "                # =====================================================================\n",
        "                # 2. KIỂM TRA TỪNG HÀNH ĐỘNG VỚI BLACKLIST\n",
        "                # =====================================================================\n",
        "                for rel, target_node in actions:\n",
        "                    rs.append(rel)\n",
        "                    es.append(target_node)\n",
        "                    \n",
        "                    # Nếu target_node nằm trong blacklist -> Mask = 0.0 (Cấm cửa)\n",
        "                    # Nếu là node mới toanh -> Mask = 1.0 (Cho phép)\n",
        "                    if target_node in blacklist:\n",
        "                        valid_masks.append(0.0)\n",
        "                    else:\n",
        "                        valid_masks.append(1.0)\n",
        "\n",
        "                # =====================================================================\n",
        "                # 3. [QUAN TRỌNG] BẢO HIỂM CHỐNG CRASH (DEAD-END PREVENTION)\n",
        "                # =====================================================================\n",
        "                # Xử lý trường hợp Agent đi vào ngõ cụt (Tất cả các ngã rẽ đều đã đi qua)\n",
        "                # Nếu sum == 0, toàn bộ Mask là 0 -> Điểm -vô cực -> Hàm Softmax sẽ trả về NaN!\n",
        "                if sum(valid_masks) == 0.0:\n",
        "                    valid_masks[0] = 1.0 # Bắt buộc mở khóa 1 đường bất kỳ để mạng nơ-ron không bị nổ (Crash)\n",
        "\n",
        "                # 4. Gán dữ liệu vào Tensor\n",
        "                r_indices[i, :num_acts] = torch.tensor(rs, device=device)\n",
        "                e_indices[i, :num_acts] = torch.tensor(es, device=device)\n",
        "                action_mask[i, :num_acts] = torch.tensor(valid_masks, dtype=torch.float, device=device)\n",
        "\n",
        "        # 5. Lấy vector nhúng từ ID\n",
        "        r_emb = self.relation_embs(r_indices)\n",
        "        e_emb = self.entity_embs(e_indices)\n",
        "\n",
        "        # CHỐT: Ghép theo đúng định nghĩa Toán học Action = (Relation, Entity)\n",
        "        action_embs = torch.cat([r_emb, e_emb], dim=-1)             \n",
        "\n",
        "        return action_embs, action_mask, raw_actions_list\n",
        "\n",
        "    def step(self, actions):    # used  by def step_with_indices\n",
        "        \"\"\"\n",
        "        Transition function (Eq 9): Chuyển trạng thái sang bước k+1\n",
        "        \"\"\"\n",
        "        device = self.current_entities.device\n",
        "        batch_size = len(actions)\n",
        "\n",
        "        rels_list = [a[0] for a in actions]\n",
        "        ents_list = [a[1] for a in actions]\n",
        "\n",
        "        next_relations = torch.tensor(rels_list, dtype=torch.long, device=device)\n",
        "        next_entities = torch.tensor(ents_list, dtype=torch.long, device=device)\n",
        "\n",
        "        # Lấy Node hiện tại làm điểm xuất phát (e_{k-1})\n",
        "        curr_e = self.current_entities.unsqueeze(1)\n",
        "        new_r = next_relations.unsqueeze(1)\n",
        "\n",
        "        # Nối lại theo chuẩn Paper: [e_{k-1}, r_k]\n",
        "        new_entry = torch.cat([curr_e, new_r], dim=1)\n",
        "\n",
        "        history_shifted = self.path_history[:, 2:]\n",
        "        self.path_history = torch.cat([history_shifted, new_entry], dim=1)      \n",
        "\n",
        "        self.current_entities = next_entities #tensor([8673, 8614])\n",
        "        self.step_counter += 1\n",
        "        done = (self.step_counter >= self.max_path_len)\n",
        "\n",
        "        return self._get_state_embedding(), done\n",
        "\n",
        "    def step_with_indices(self, action_indices, raw_actions_list):\n",
        "        selected_real_actions = []\n",
        "        batch_size = len(action_indices)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            idx = action_indices[i].item()\n",
        "            \n",
        "            # [ĐÃ SỬA TÊN] Dùng 'node_acts' thay vì 'user_acts'\n",
        "            node_acts = raw_actions_list[i]\n",
        "\n",
        "            if len(node_acts) > 0:\n",
        "                idx = min(idx, len(node_acts) - 1)\n",
        "                real_action = node_acts[idx]\n",
        "            else:\n",
        "                curr_node = self.current_entities[i].item()\n",
        "                real_action = (0, curr_node) # Dead-end: Tự trỏ về chính nó\n",
        "\n",
        "            selected_real_actions.append(real_action)\n",
        "\n",
        "        return self.step(selected_real_actions)\n",
        "\n",
        "    def get_reward(self):   \n",
        "        \"\"\"\n",
        "        Gọi khi done=True (Hoặc ở các bước lẻ). \n",
        "        Tính Time-aware Reward g_R(v|u). Nếu không phải Item -> Reward = 0.0\n",
        "        \n",
        "        Lưu ý: tham số truyền vào nên là 1 Tensor chứa TOÀN BỘ các ID của Item có trong Đồ thị.\n",
        "        \"\"\"\n",
        "        user_ids = self.current_users\n",
        "        item_ids = self.current_entities\n",
        "\n",
        "        batch_history_relations = []\n",
        "        \n",
        "        for user_id in user_ids.tolist():\n",
        "            batch_neighbors = self.tckg.get_neighbors(user_id)            \n",
        "            user_rels = [rel for (rel, tail) in batch_neighbors]\n",
        "            user_rels_tensor = torch.tensor(user_rels, dtype=torch.long)\n",
        "            batch_history_relations.append(user_rels_tensor)\n",
        "        \n",
        "        history_relation_ids = pad_sequence(\n",
        "            batch_history_relations, \n",
        "            batch_first=True, \n",
        "            padding_value=0\n",
        "        ).to(self.current_users.device)\n",
        "\n",
        "        # 1. SOFT REWARD: Tính điểm dẫn đường cho TOÀN BỘ node hiện tại (bất chấp là Item hay Entity)\n",
        "        # rewards có shape: (Batch_size,)\n",
        "        rewards = self.reward_function(user_ids, item_ids, history_relation_ids)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 2. [MỚI] MASKING: TẮT REWARD VỀ 0.0 NẾU KHÔNG PHẢI LÀ ITEM\n",
        "        # =====================================================================\n",
        "        is_item_mask = (item_ids >= self.item_start_id) & (item_ids <= self.item_end_id)\n",
        "\n",
        "        rewards = torch.where(is_item_mask, rewards, torch.zeros_like(rewards))\n",
        "\n",
        "        return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J_EqQtvYGJid",
      "metadata": {
        "id": "J_EqQtvYGJid"
      },
      "source": [
        "#### TPRecPolicy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7faeaadc",
      "metadata": {
        "id": "7faeaadc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TPRecPolicy(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n",
        "        super(TPRecPolicy, self).__init__()\n",
        "\n",
        "        # BLSTM giờ đây chỉ cần nhận input_size = embed_dim (không cần nhân 2 nữa)\n",
        "        self.blstm = nn.LSTM(input_size=embed_dim,\n",
        "                             hidden_size=hidden_dim,\n",
        "                             num_layers=1,\n",
        "                             batch_first=True,\n",
        "                             bidirectional=True)\n",
        "\n",
        "        # W1 giờ đây chỉ cần nhận đầu ra của BLSTM\n",
        "        self.W1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.Wa = nn.Linear(hidden_dim, embed_dim * 2)\n",
        "        self.Wc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, full_state_seq, action_embs, action_mask):\n",
        "        # 1. Đưa toàn bộ s_k vào BLSTM như công thức (15)\n",
        "        lstm_out, _ = self.blstm(full_state_seq)\n",
        "\n",
        "        # # Lấy trạng thái ở bước thời gian cuối cùng đại diện cho toàn bộ chuỗi\n",
        "        # lstm_last = lstm_out[:, -1, :]\n",
        "\n",
        "        # Vì bidirectional=True, hidden_dim này thực chất là 2 phần ghép lại\n",
        "        half_dim = self.blstm.hidden_size\n",
        "\n",
        "        # Lấy trạng thái Tóm tắt Chiều đi tới (Ở index cuối cùng: -1)\n",
        "        forward_last = lstm_out[:, -1, :half_dim]\n",
        "\n",
        "        # Lấy trạng thái Tóm tắt Chiều đi lùi (Ở index đầu tiên: 0)\n",
        "        backward_last = lstm_out[:, 0, half_dim:]\n",
        "\n",
        "        # Ghép 2 tóm tắt này lại thành một Context Vector hoàn hảo\n",
        "        lstm_last = torch.cat([forward_last, backward_last], dim=-1)\n",
        "\n",
        "        # 2. Tính x_k theo đúng phương trình\n",
        "        x_k = self.dropout(torch.relu(self.W1(lstm_last)))\n",
        "\n",
        "        # --- (Phần tính Actor và Critic giữ nguyên) ---\n",
        "        query = self.Wa(x_k).unsqueeze(1)\n",
        "        scores = torch.sum(query * action_embs, dim=-1)\n",
        "        scores = scores.masked_fill(action_mask.bool() == False, float('-inf'))\n",
        "        probs = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        value_baseline = self.Wc(x_k).squeeze(-1)\n",
        "\n",
        "        return probs, value_baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3E6EF6goGNLD",
      "metadata": {
        "id": "3E6EF6goGNLD"
      },
      "source": [
        "#### TPRecLightningModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74f005c9",
      "metadata": {
        "id": "74f005c9"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "class TPRecLightningModel(pl.LightningModule):\n",
        "    def __init__(self, env, policy_net, learning_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        # Lưu lại hyperparameter (tự động log vào tensorboard nếu dùng)\n",
        "        self.save_hyperparameters(ignore=['env', 'policy_net'])\n",
        "\n",
        "        self.env = env\n",
        "        self.policy_net = policy_net\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, full_state_seq, action_embs, action_mask):\n",
        "        \"\"\"\n",
        "        Đạo diễn Lightning chuyển tiếp ĐÚNG 3 tham số cho Diễn viên Policy Network\n",
        "        \"\"\"\n",
        "        probs, value_baseline =  self.policy_net(full_state_seq, action_embs, action_mask)\n",
        "        return probs, value_baseline\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Phiên bản Thử nghiệm: Thưởng dọc đường (Multi-hop) nhưng BỎ QUA Bước 1.\n",
        "        Agent chỉ được nhận Reward khi đến các trạm Item ở t=2, t=4...\n",
        "        \"\"\"\n",
        "        batch_users, _ = batch[0], batch[1]   \n",
        "\n",
        "        batch_size = batch_users.size(0)\n",
        "        full_state_seq = self.env.reset(batch_users)\n",
        "\n",
        "        saved_log_probs = []\n",
        "        saved_values = []\n",
        "        saved_rewards = [] # Mảng hứng phần thưởng từng bước\n",
        "\n",
        "        for t in range(self.env.max_path_len):\n",
        "            action_embs, action_mask, raw_actions = self.env.get_action_space_batch()\n",
        "\n",
        "            probs, value_baseline = self(full_state_seq, action_embs, action_mask)\n",
        "\n",
        "            m = torch.distributions.Categorical(probs)\n",
        "            action_indices = m.sample()\n",
        "\n",
        "            saved_log_probs.append(m.log_prob(action_indices))\n",
        "            saved_values.append(value_baseline)\n",
        "\n",
        "            # Thực hiện bước đi\n",
        "            full_state_seq, done = self.env.step_with_indices(action_indices, raw_actions)\n",
        "            \n",
        "            # =====================================================================\n",
        "            # [CỐT LÕI] BỘ LỌC THỜI GIAN CHO PHẦN THƯỞNG\n",
        "            # =====================================================================\n",
        "            # Mặc định phần thưởng của bước này là 0.0 cho toàn bộ Batch\n",
        "            step_reward = torch.zeros(batch_size, device=self.device)\n",
        "            \n",
        "            # CHỈ cho phép gọi get_reward() tính điểm nếu:\n",
        "            # 1. Không phải bước đầu tiên (t > 0) -> Bỏ qua thiên vị Item cũ\n",
        "            # 2. Là bước chẵn (t % 2 == 0) -> Đảm bảo đang đứng ở tầng Item (Hop 3, Hop 5...)\n",
        "            if t > 0 and t % 2 == 0:\n",
        "                # Hàm get_reward() đã có sẵn mask kiểm tra Entity/Item bên trong nó\n",
        "                step_reward = self.env.get_reward().detach() \n",
        "            \n",
        "            saved_rewards.append(step_reward)\n",
        "\n",
        "        # =====================================================================\n",
        "        # TÍNH TOÁN LỢI TỨC G THEO ĐỊNH LÝ BELLMAN\n",
        "        # =====================================================================\n",
        "        gamma = 0.99 \n",
        "        returns = []\n",
        "        \n",
        "        # G khởi tạo bằng 0\n",
        "        G = torch.zeros_like(saved_rewards[0]) \n",
        "        \n",
        "        # Duyệt ngược từ bước cuối về bước đầu\n",
        "        for R_step in reversed(saved_rewards):\n",
        "            G = R_step + gamma * G  \n",
        "            returns.insert(0, G)\n",
        "\n",
        "        policy_loss = 0\n",
        "        value_loss = 0\n",
        "\n",
        "        # CÔNG THỨC 18: Tính Loss REINFORCE\n",
        "        for G_val, log_prob, value_baseline in zip(returns, saved_log_probs, saved_values):\n",
        "            advantage = G_val - value_baseline.detach()\n",
        "            \n",
        "            step_policy_loss = -log_prob * advantage\n",
        "            policy_loss += step_policy_loss.mean() \n",
        "            \n",
        "            step_value_loss = torch.nn.functional.mse_loss(value_baseline, G_val)\n",
        "            value_loss += step_value_loss\n",
        "\n",
        "        total_loss = policy_loss + value_loss \n",
        "\n",
        "        # Ghi nhận log: Tính tổng điểm thực tế gom được trong Episode\n",
        "        total_episode_reward = sum(saved_rewards)\n",
        "        self.log('train_reward', total_episode_reward.mean(), prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log('train_loss', total_loss, prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Vòng lặp Đánh giá (Validation) mô phỏng chính xác Section 3.4 của TPRec.\n",
        "        Bao gồm: Val Loss, Val Reward, và Re-ranking bằng Công thức (20) để tính HR/NDCG.\n",
        "        \"\"\"\n",
        "        batch_users, target_items = batch[0], batch[1]\n",
        "        batch_size = batch_users.size(0)\n",
        "\n",
        "        full_state_seq = self.env.reset(batch_users)\n",
        "\n",
        "        saved_log_probs = []\n",
        "        saved_values = []\n",
        "\n",
        "        # =====================================================================\n",
        "        # 1. PHASE 1: AGENT TÌM KIẾM ĐƯỜNG ĐI (Thu thập tập ứng viên V_hat)\n",
        "        # =====================================================================\n",
        "        for t in range(self.env.max_path_len):\n",
        "            action_embs, action_mask, raw_actions = self.env.get_action_space_batch()\n",
        "            probs, value_baseline = self(full_state_seq, action_embs, action_mask)\n",
        "\n",
        "            # Ở chế độ Val, ta dùng argmax (Greedy) để chọn đường đi tự tin nhất\n",
        "            # (Thay vì sample ngẫu nhiên như lúc Train)\n",
        "            action_indices = torch.argmax(probs, dim=-1)\n",
        "\n",
        "            # (Tùy chọn) Tính log_prob để tracking val_loss cho đồng bộ biểu đồ\n",
        "            m = torch.distributions.Categorical(probs)\n",
        "            saved_log_probs.append(m.log_prob(action_indices))\n",
        "            saved_values.append(value_baseline)\n",
        "\n",
        "            full_state_seq, done = self.env.step_with_indices(action_indices, raw_actions)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 2. PHASE 2: RE-RANKING BẰNG CÔNG THỨC (20) VÀ TÍNH HR/NDCG\n",
        "        # =====================================================================\n",
        "        # Tại t=2 (Bước cuối), action_embs chứa embedding của tập ứng viên e_V_hat\n",
        "        # action_embs có shape: (Batch, Max_Len, 2 * Embed_Dim) -> Nửa sau là Entity\n",
        "        embed_dim = self.env.entity_embs.embedding_dim\n",
        "        e_V_hat = action_embs[:, :, embed_dim:] # Shape: (Batch, Max_Len, Embed_Dim)\n",
        "\n",
        "        # Lấy User Embedding (e_u)\n",
        "        e_u = self.env.entity_embs(batch_users) # Shape: (Batch, Embed_Dim)\n",
        "\n",
        "        # Lấy Temporal Relation Vector (r_W_T) từ Công thức 19\n",
        "        # (Nếu bạn chưa code module GMM, hãy tạm dùng torch.zeros_like(e_u) làm placeholder)\n",
        "        if hasattr(self, 'get_temporal_relation'):\n",
        "            r_W_T = self.get_temporal_relation(batch_users) \n",
        "        else:\n",
        "            r_W_T = torch.zeros_like(e_u)\n",
        "\n",
        "        # Tạo Query Vector: (e_u + r_W_T)\n",
        "        query_vector = e_u + r_W_T # Shape: (Batch, Embed_Dim)\n",
        "\n",
        "        # Áp dụng Công thức (20): Phép nhân vô hướng (Dot Product)\n",
        "        # query.unsqueeze(1) để nhân broadcast với toàn bộ e_V_hat của từng User\n",
        "        scores = torch.sum(query_vector.unsqueeze(1) * e_V_hat, dim=-1) # Shape: (Batch, Max_Len)\n",
        "\n",
        "        # Masking: Ép điểm các node đệm (padding) hoặc node cấm về Âm vô cực\n",
        "        scores = scores.masked_fill(action_mask == 0, float('-inf'))\n",
        "\n",
        "        # Sort descending để lấy Top 10 (Chính là hàm Sort() trong Eq 20)\n",
        "        k = min(10, scores.size(1))\n",
        "        _, top10_indices = torch.topk(scores, k=k, dim=1)\n",
        "\n",
        "        # Tính toán Hit Ratio và NDCG\n",
        "        hits = 0.0\n",
        "        ndcg = 0.0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            user_acts = raw_actions[i]\n",
        "            target = target_items[i].item()\n",
        "\n",
        "            top10_ids = []\n",
        "            for idx in top10_indices[i].tolist():\n",
        "                if idx < len(user_acts):\n",
        "                    top10_ids.append(user_acts[idx][1]) # Index 1 là Entity ID (Item)\n",
        "\n",
        "            if target in top10_ids:\n",
        "                hits += 1.0\n",
        "                rank = top10_ids.index(target) + 1\n",
        "                ndcg += 1.0 / np.log2(rank + 1)\n",
        "\n",
        "        val_hr = hits / batch_size\n",
        "        val_ndcg = ndcg / batch_size\n",
        "\n",
        "        # =====================================================================\n",
        "        # 3. PHASE 3: TÍNH VAL LOSS VÀ VAL REWARD\n",
        "        # =====================================================================\n",
        "        final_rewards = self.env.get_reward().detach()\n",
        "        \n",
        "        gamma = 0.99 \n",
        "        returns = []\n",
        "        R = final_rewards \n",
        "        for step in reversed(range(self.env.max_path_len)):\n",
        "            returns.insert(0, R)\n",
        "            R = R * gamma \n",
        "\n",
        "        policy_loss = 0\n",
        "        value_loss = 0\n",
        "        for G_val, log_prob, value_baseline in zip(returns, saved_log_probs, saved_values):\n",
        "            advantage = G_val - value_baseline\n",
        "            policy_loss += (-log_prob * advantage).mean() \n",
        "            value_loss += torch.nn.functional.mse_loss(value_baseline, G_val)\n",
        "        val_loss = policy_loss + value_loss \n",
        "\n",
        "        # 4. GHI LOG LÊN TENSORBOARD\n",
        "        self.log('val_reward', final_rewards.mean(), prog_bar=True, on_epoch=True)\n",
        "        self.log('val_loss', val_loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('val_hr@10', val_hr, prog_bar=True, on_epoch=True)\n",
        "        self.log('val_ndcg@10', val_ndcg, prog_bar=True, on_epoch=True)\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Lọc ra CHỈ những tham số đang requires_grad=True (Mạng Policy)\n",
        "        # Giúp tránh lỗi tối ưu hóa các tham số đã bị freeze (như Embeddings)\n",
        "        trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n",
        "\n",
        "        optimizer = optim.Adam(trainable_params, lr=self.learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
        "\n",
        "        return [optimizer], [scheduler]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad71580",
      "metadata": {
        "id": "1ad71580"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c04d198e",
      "metadata": {
        "id": "c04d198e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class InteractionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        \"\"\"Nhận vào một DataFrame và chuyển đổi các cột cần thiết thành mảng NumPy\"\"\"\n",
        "        self.users = df['user_id'].values\n",
        "        self.entities = df['entity_id'].values # ĐÂY CHÍNH LÀ TARGET ITEM\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Trả về tổng số dòng dữ liệu\"\"\"\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Lấy dữ liệu tại vị trí idx và biến thành PyTorch Tensor\"\"\"\n",
        "        user_tensor = torch.tensor(self.users[idx], dtype=torch.long)\n",
        "        entity_tensor = torch.tensor(self.entities[idx], dtype=torch.long)\n",
        "\n",
        "        # ĐÃ SỬA: Trả về một Tuple gồm (User, Target_Item)\n",
        "        return user_tensor, entity_tensor\n",
        "\n",
        "# 2. Đọc file CSV thành Pandas DataFrame\n",
        "# (Lưu ý nhỏ: Mình giữ nguyên tên file của bạn, nhưng chữ 'interacions' hình như đang thiếu chữ 't', bạn nhớ kiểm tra lại tên file thật nhé)\n",
        "\n",
        "train_df = pd.read_csv(f'./data/{name}/{name}_train_interactions.csv')\n",
        "val_df = pd.read_csv(f'./data/{name}/{name}_val_interactions.csv')\n",
        "test_df = pd.read_csv(f'./data/{name}/{name}_test_interactions.csv')\n",
        "\n",
        "# 3. Bọc DataFrame vào class Dataset vừa tạo\n",
        "train_dataset = InteractionDataset(train_df)\n",
        "val_dataset = InteractionDataset(val_df)\n",
        "test_dataset = InteractionDataset(test_df)\n",
        "\n",
        "# 4. Đưa Dataset vào DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d12f9c9",
      "metadata": {
        "id": "2d12f9c9"
      },
      "source": [
        "### Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c700f1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c3cd0ac7d1404e35896fd27548060b99",
            "4315aa12aa7c432d8bc5cf28b1b26ce3"
          ]
        },
        "id": "2c700f1f",
        "outputId": "7f19b46b-54fe-43ea-ca70-86d45c3cfd56"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "cfg = Config()\n",
        "print(\"🔥 Starting TPRec Training with PyTorch Lightning...\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# BƯỚC 1: Xây dựng Đồ thị Tri thức (TCKG)\n",
        "# -------------------------------------------------\n",
        "# Lưu ý: Cần đảm bảo file CSV đã được map ID về dạng số nguyên liên tục (0, 1, 2...)\n",
        "print(\"Loading Knowledge Graph...\")\n",
        "# Tính offset tự động (như class TCKG tối ưu tôi đã viết)\n",
        "tckg = TCKG(cfg.TCKG_PATH)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# BƯỚC 2: Khởi tạo Embeddings từ file Pickle\n",
        "# -------------------------------------------------\n",
        "print(\"Loading Pre-trained TransE Embeddings...\")\n",
        "pickle_file_path = f'./pickle/{name}_transE_embeddings_2026-02-28_20-32-29.pkl'\n",
        "\n",
        "with open(pickle_file_path, 'rb') as f:\n",
        "    saved_data = pickle.load(f)\n",
        "\n",
        "pretrained_ent = saved_data['entity_embeddings']\n",
        "pretrained_rel = saved_data['relation_embeddings']\n",
        "\n",
        "# 2. Chuyển đổi sang PyTorch Tensor (ép kiểu Float32 để tính toán neural network)\n",
        "# Nếu data đang là Numpy array:\n",
        "if isinstance(pretrained_ent, np.ndarray):\n",
        "    ent_tensor = torch.tensor(pretrained_ent, dtype=torch.float32)\n",
        "    rel_tensor = torch.tensor(pretrained_rel, dtype=torch.float32)\n",
        "else:\n",
        "    # Nếu data đã là Tensor sẵn:\n",
        "    ent_tensor = pretrained_ent.clone().detach().float()\n",
        "    rel_tensor = pretrained_rel.clone().detach().float()\n",
        "\n",
        "# 3. Nạp vào nn.Embedding\n",
        "# freeze=False: Cho phép RL Agent tiếp tục cập nhật (fine-tune) vector trong lúc tìm đường\n",
        "# freeze=True: Khóa cứng vector, RL Agent chỉ học Policy Network (Học nhanh hơn, chống overfit)\n",
        "entity_embs = nn.Embedding.from_pretrained(ent_tensor, freeze=True, padding_idx=0)\n",
        "relation_embs = nn.Embedding.from_pretrained(rel_tensor, freeze=True, padding_idx=0)\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# BƯỚC 3: Khởi tạo Reward Function\n",
        "# -------------------------------------------------\n",
        "print(\"Setting up Reward Function...\")\n",
        "reward_func = TimeAwareRewardFunction(\n",
        "    user_embs=entity_embs,    # Chia sẻ trọng số với Env\n",
        "    entity_embs=entity_embs,\n",
        "    relation_embs=relation_embs,\n",
        "    interaction_cluster_ids=cfg.INTERACTION_CLUSTER_IDS,\n",
        "    bias_embs=None, # Tự tạo bias mới\n",
        "    temperature= 1\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# BƯỚC 4: Khởi tạo Môi trường (Environment)\n",
        "# -------------------------------------------------\n",
        "print(\"Setting up Environment...\")\n",
        "env = TPRecEnvironment(\n",
        "    tckg=tckg,\n",
        "    entity_embeddings=entity_embs,\n",
        "    relation_embeddings=relation_embs,\n",
        "    reward_function=reward_func, # Inject reward vào env\n",
        "    max_path_len=cfg.MAX_PATH_LEN,\n",
        "    history_len=cfg.HISTORY_LEN,\n",
        "    item_start_id=cfg.ITEM_START_ID,\n",
        "    item_end_id=cfg.ITEM_END_ID\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# BƯỚC 5: Khởi tạo Policy Network (Agent)\n",
        "# -------------------------------------------------\n",
        "print(\"Building Policy Network...\")\n",
        "policy_net = TPRecPolicy(\n",
        "        embed_dim=cfg.EMBED_DIM,      # Ví dụ: 64\n",
        "        hidden_dim=cfg.HIDDEN_DIM,    # Ví dụ: 128\n",
        "        dropout=0.1                   # Tỉ lệ dropout giúp chống Overfit (theo paper)\n",
        "    )\n",
        "\n",
        "# BƯỚC 6: ĐÓNG GÓI VÀO LIGHTNING MODEL\n",
        "print(\"Packing into Lightning Module...\")\n",
        "lightning_model = TPRecLightningModel(\n",
        "    env=env,\n",
        "    policy_net=policy_net,\n",
        "    learning_rate=cfg.LEARNING_RATE\n",
        ")\n",
        "\n",
        "# BƯỚC 7: CẤU HÌNH LƯU BEST MODEL (CHECKPOINT)\n",
        "# Tự động theo dõi 'train_reward' ở cuối mỗi epoch và lưu lại bản có điểm cao nhất\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=cfg.SAVE_DIR,\n",
        "    filename='tprec-best-{epoch:02d}-{train_reward:.4f}',\n",
        "    monitor='train_hr@10',\n",
        "    mode='max', # Lưu model có reward lớn nhất\n",
        "    save_top_k=1,\n",
        "    save_last=True # Lưu thêm model ở epoch cuối cùng để phòng hờ\n",
        ")\n",
        "\n",
        "# 2. THÊM CALLBACK EARLY STOPPING VÀO ĐÂY\n",
        "early_stop_callback = EarlyStopping(\n",
        "      monitor='val_hr@10',   # Phải cùng tên với biến monitor ở Checkpoint\n",
        "      min_delta=0.001,       # Sự thay đổi tối thiểu để được tính là \"có cải thiện\"\n",
        "      patience=500,           # Sức chịu đựng: Cho phép mô hình dậm chân tại chỗ tối đa 10 Epoch\n",
        "      verbose=True,          # Bật in thông báo ra màn hình khi Early Stop kích hoạt\n",
        "      mode='max'             # 'max' vì ta muốn chỉ số HR@10/Reward càng lớn càng tốt\n",
        "  )\n",
        "\n",
        "# BƯỚC 8: KHỞI TẠO TRAINER VÀ BẮT ĐẦU CHẠY\n",
        "print(\"Initializing Lightning Trainer...\")\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=cfg.NUM_EPOCHS,\n",
        "    accelerator=\"auto\", # Tự động tìm và dùng GPU nếu có\n",
        "    devices=1,\n",
        "    gradient_clip_val=1.0, # Tự động áp dụng Gradient Clipping\n",
        "    callbacks=[checkpoint_callback, early_stop_callback],\n",
        "    enable_progress_bar=True,\n",
        "    # log_every_n_steps=10,\n",
        "    num_sanity_val_steps=0\n",
        ")\n",
        "\n",
        "print(\"🚀 Bắt đầu huấn luyện...\")\n",
        "# DataLoader của bạn cần được truyền vào đây\n",
        "trainer.fit(\n",
        "        model=lightning_model,\n",
        "        train_dataloaders=train_loader,\n",
        "        val_dataloaders=val_loader\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OtAQonw0SQOa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "d7858607c63f4bb7a9f78fd1c2baf953",
            "73726383c10043b99fd821ceb16f5774"
          ]
        },
        "id": "OtAQonw0SQOa",
        "outputId": "213dcb58-8ec6-4bf1-a54f-dc094290f1b0"
      },
      "outputs": [],
      "source": [
        "trainer.validate(dataloaders=test_loader, ckpt_path='best')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rs_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4315aa12aa7c432d8bc5cf28b1b26ce3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73726383c10043b99fd821ceb16f5774": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3cd0ac7d1404e35896fd27548060b99": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4315aa12aa7c432d8bc5cf28b1b26ce3",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 68/499 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━</span> 27/74 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:10 • 0:00:19</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">2.58it/s</span> <span style=\"font-style: italic\">v_num: 0.000 val_reward: 0.567    </span>\n                                                                                 <span style=\"font-style: italic\">val_hr@10: 0.035 val_ndcg@10:     </span>\n                                                                                 <span style=\"font-style: italic\">0.018 train_hr@10: 0.036          </span>\n                                                                                 <span style=\"font-style: italic\">train_ndcg@10: 0.019 train_reward:</span>\n                                                                                 <span style=\"font-style: italic\">0.568                             </span>\n</pre>\n",
                  "text/plain": "Epoch 68/499 \u001b[38;2;98;6;224m━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━\u001b[0m 27/74 \u001b[2m0:00:10 • 0:00:19\u001b[0m \u001b[2;4m2.58it/s\u001b[0m \u001b[3mv_num: 0.000 val_reward: 0.567    \u001b[0m\n                                                                                 \u001b[3mval_hr@10: 0.035 val_ndcg@10:     \u001b[0m\n                                                                                 \u001b[3m0.018 train_hr@10: 0.036          \u001b[0m\n                                                                                 \u001b[3mtrain_ndcg@10: 0.019 train_reward:\u001b[0m\n                                                                                 \u001b[3m0.568                             \u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "d7858607c63f4bb7a9f78fd1c2baf953": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_73726383c10043b99fd821ceb16f5774",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Validation <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 12/12 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:03 • 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">3.15it/s</span>  \n</pre>\n",
                  "text/plain": "Validation \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 12/12 \u001b[2m0:00:03 • 0:00:00\u001b[0m \u001b[2;4m3.15it/s\u001b[0m  \n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
