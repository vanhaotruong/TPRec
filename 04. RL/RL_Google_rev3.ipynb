{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "GKjNoSnA8oy0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKjNoSnA8oy0",
        "outputId": "cf1f5917-7c54-48a1-afbe-4d0d12d3e07e"
      },
      "outputs": [],
      "source": [
        "# !pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d36101a0",
      "metadata": {
        "id": "d36101a0"
      },
      "outputs": [],
      "source": [
        "import random, os, pickle, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Set environment variables for reproducibility and safety\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "# 1. Configuration & Seeding\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1edfb172",
      "metadata": {
        "id": "1edfb172"
      },
      "outputs": [],
      "source": [
        "name = 'book'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N7_iFoFJGZT8",
      "metadata": {
        "id": "N7_iFoFJGZT8"
      },
      "source": [
        "#### TCKG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48479536",
      "metadata": {
        "id": "48479536"
      },
      "outputs": [],
      "source": [
        "class TCKG:\n",
        "    def __init__(self, tckg_csv_path):\n",
        "        self.adj_list = defaultdict(list)\n",
        "\n",
        "        print(f\"Loading TCKG from {tckg_csv_path}...\")\n",
        "        df_tckg = pd.read_csv(tckg_csv_path, usecols=['head_id', 'relation_id', 'tail_id'])\n",
        "\n",
        "        offset = df_tckg['relation_id'].max()\n",
        "\n",
        "        data = df_tckg[['head_id', 'relation_id', 'tail_id']].to_numpy() # Using numpy to speedup\n",
        "        for h, r, t in data:\n",
        "            h, r, t = int(h), int(r), int(t)\n",
        "\n",
        "            self.adj_list[h].append((r, t))\n",
        "            self.adj_list[t].append((r + offset, h)) # Ex: r=5 (watched) -> r_inv=105 (watched_by)\n",
        "\n",
        "        self._prepare_tensors()\n",
        "\n",
        "        print(f\"TCKG Loaded successfully. Graph construction complete.\")\n",
        "\n",
        "    def _prepare_tensors(self):\n",
        "        num_nodes = max(self.adj_list.keys())\n",
        "\n",
        "        self.max_edges = 0\n",
        "\n",
        "        for node, edges in self.adj_list.items():\n",
        "            if self.max_edge < len(edges):\n",
        "                self.max_edge = len(edges)\n",
        "\n",
        "        self.rel_matrix = torch.zeros((num_nodes, self.max_neighbors), dtype=torch.long)\n",
        "        self.tail_matrix = torch.zeros((num_nodes, self.max_neighbors), dtype=torch.long)\n",
        "        self.mask_matrix = torch.zeros((num_nodes, self.max_neighbors), dtype=torch.bool)\n",
        "\n",
        "        for node, edges in self.adj_list.items():\n",
        "            # Kh√¥ng c√≤n l·ªánh if c·∫Øt b·ªè (random.sample) ·ªü ƒë√¢y n·ªØa\n",
        "            \n",
        "            rels = [e[0] for e in edges]\n",
        "            tails = [e[1] for e in edges]\n",
        "            length = len(edges)\n",
        "\n",
        "            # G√°n v√†o ma tr·∫≠n\n",
        "            self.rel_matrix[node, :length] = torch.tensor(rels, dtype=torch.long)\n",
        "            self.tail_matrix[node, :length] = torch.tensor(tails, dtype=torch.long)\n",
        "            self.mask_matrix[node, :length] = True\n",
        "\n",
        "    def get_neighbors(self, node_id):\n",
        "        a = self.adj_list[node_id]\n",
        "        return self.adj_list[node_id]\n",
        "\n",
        "    def get_all_nodes(self):\n",
        "        return list(self.adj_list.keys())\n",
        "\n",
        "    def get_batched_neighbors(self, node_ids_tensor):\n",
        "        rels = self.rel_matrix[node_ids_tensor]\n",
        "        tails = self.tail_matrix[node_ids_tensor]\n",
        "        masks = self.mask_matrix[node_ids_tensor]\n",
        "        return rels, tails, masks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ux1Mk5j3GWNR",
      "metadata": {
        "id": "ux1Mk5j3GWNR"
      },
      "source": [
        "#### TimeAwareRewardFunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ecfd9985",
      "metadata": {
        "id": "ecfd9985"
      },
      "outputs": [],
      "source": [
        "class TimeAwareRewardFunction(nn.Module):\n",
        "    def __init__(self, user_embs, entity_embs, relation_embs, interaction_cluster_ids, bias_embs=None, temperature= None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            user_embs (nn.Embedding): Embedding c·ªßa User (e_u)\n",
        "            entity_embs (nn.Embedding): Embedding c·ªßa Item/Entity (e_v)\n",
        "            relation_embs (nn.Embedding): Embedding c·ªßa Relation (d√πng ƒë·ªÉ l·∫•y V_U)\n",
        "            interaction_cluster_ids (list or tensor): Danh s√°ch c√°c Relation ID ƒë·∫°i di·ªán cho Time Clusters.\n",
        "                                                      V√≠ d·ª•: [20, 21, 22] ·ª©ng v·ªõi interacted_0, interacted_1...\n",
        "                                                      ƒê√¢y ch√≠nh l√† t·∫≠p {V_U^1, ..., V_U^L}\n",
        "            bias_embs (nn.Embedding, optional): Bias c·ªßa entity (b_v). N·∫øu None s·∫Ω t·ª± kh·ªüi t·∫°o.\n",
        "        \"\"\"\n",
        "        super(TimeAwareRewardFunction, self).__init__()\n",
        "\n",
        "        self.user_embs = user_embs\n",
        "        self.entity_embs = entity_embs\n",
        "        self.relation_embs = relation_embs\n",
        "\n",
        "        # Danh s√°ch ID c·ªßa c√°c cluster t∆∞∆°ng t√°c theo th·ªùi gian (V_U)\n",
        "        # Chuy·ªÉn th√†nh tensor ƒë·ªÉ t√≠nh to√°n song song\n",
        "        self.register_buffer('cluster_ids', torch.tensor(interaction_cluster_ids, dtype=torch.long))\n",
        "\n",
        "        # Entity Bias (b_v) - Eq (11)\n",
        "        if bias_embs is None:\n",
        "            num_entities = entity_embs.num_embeddings - 1\n",
        "            self.bias_embs = nn.Embedding(num_entities + 1, 1, padding_idx = 0)\n",
        "            nn.init.zeros_(self.bias_embs.weight) # Kh·ªüi t·∫°o bias b·∫±ng 0\n",
        "        else:\n",
        "            self.bias_embs = bias_embs\n",
        "\n",
        "        if temperature is None:\n",
        "            self.temperature = self.entity_embs.embedding_dim ** 0.5\n",
        "        else:\n",
        "            self.temperature = temperature\n",
        "\n",
        "    def calculate_weights(self, history_relation_ids):\n",
        "        \"\"\"\n",
        "        Th·ª±c hi·ªán Eq (13): T√≠nh tr·ªçng s·ªë w_h d·ª±a tr√™n t·∫ßn su·∫•t xu·∫•t hi·ªán trong l·ªãch s·ª≠.\n",
        "\n",
        "        Args:\n",
        "            history_relation_ids: (Batch, Max_History_Len) - Ch·ª©a relation ID trong qu√° kh·ª© c·ªßa user.\n",
        "\n",
        "        Returns:\n",
        "            weights: (Batch, Num_Clusters) - Vector W_hu\n",
        "        \"\"\"\n",
        "        # 1. So kh·ªõp History v·ªõi Cluster IDs\n",
        "        # history: (B, H, 1)\n",
        "        # clusters: (1, 1, L)\n",
        "        # matches: (B, H, L) -> True n·∫øu relation t·∫°i history kh·ªõp v·ªõi cluster ID\n",
        "        hist_expanded = history_relation_ids.unsqueeze(-1)\n",
        "        clusters_expanded = self.cluster_ids.view(1, 1, -1)\n",
        "\n",
        "        matches = (hist_expanded == clusters_expanded).float()\n",
        "\n",
        "        # 2. ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán (T·ª≠ s·ªë Eq 13)\n",
        "        # Sum theo chi·ªÅu History (dim=1) -> (B, L)\n",
        "        counts = matches.sum(dim=1)\n",
        "\n",
        "        # 3. T√≠nh ƒë·ªô d√†i q th·ª±c t·∫ø (M·∫´u s·ªë Eq 13)\n",
        "        # q = t·ªïng s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa b·∫•t k·ª≥ cluster n√†o trong history (tr√°nh t√≠nh padding 0)\n",
        "        q = counts.sum(dim=1, keepdim=True)\n",
        "\n",
        "        # 4. Normalize ƒë·ªÉ ra tr·ªçng s·ªë (tr√°nh chia cho 0)\n",
        "        weights = counts / (q + 1e-9)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def forward(self, user_ids, item_ids, history_relation_ids):\n",
        "        \"\"\"\n",
        "        T√≠nh Reward Score g_R(v | u)\n",
        "\n",
        "        Args:\n",
        "            user_ids: (Batch,)\n",
        "            item_ids: (Batch,) - Item ƒë√≠ch (v_hat) m√† Agent d·ª± ƒëo√°n/d·ª´ng l·∫°i\n",
        "            history_relation_ids: (Batch, History_Len) - L·ªãch s·ª≠ relation c·ªßa user\n",
        "\n",
        "        Returns:\n",
        "            scores: (Batch,) - ƒêi·ªÉm reward\n",
        "        \"\"\"\n",
        "        # --- B∆Ø·ªöC 1: L·∫•y Embeddings c∆° b·∫£n ---\n",
        "        u_e = self.user_embs(user_ids)       # (B, Dim) -> e_u\n",
        "        \n",
        "        v_e = self.entity_embs(item_ids)     # (B, Dim) -> e_v\n",
        "        v_b = self.bias_embs(item_ids).squeeze(-1) # (B,) -> b_v\n",
        "\n",
        "        # --- B∆Ø·ªöC 2: T√≠nh Personalized Interaction Relation (Eq 12) ---\n",
        "        # r_vu^T = W_hu * V_U\n",
        "\n",
        "        # a. T√≠nh weights (B, L)\n",
        "        weights = self.calculate_weights(history_relation_ids)\n",
        "\n",
        "        # b. L·∫•y embedding c·ªßa c√°c cluster V_U^1...L\n",
        "        # cluster_embs shape: (L, Dim)\n",
        "        cluster_embs = self.relation_embs(self.cluster_ids)\n",
        "\n",
        "        # c. T√≠nh t·ªïng c√≥ tr·ªçng s·ªë\n",
        "        # (B, L) x (L, Dim) -> (B, Dim)\n",
        "        r_interaction = torch.matmul(weights, cluster_embs)\n",
        "\n",
        "        # --- B∆Ø·ªöC 3: T√≠nh Score (Eq 11 & Final Eq) ---\n",
        "        # g = (e_u + r_interaction) . e_v + b_v\n",
        "\n",
        "        # Dot product: (e_u + r) * e_v\n",
        "        query_vector = u_e + r_interaction # (B, Dim)\n",
        "        dot_product = torch.sum(query_vector * v_e, dim=1) # (B,)\n",
        "\n",
        "        scores = dot_product + v_b # C·ªông bias\n",
        "\n",
        "        # 1. Scale Score (Chia cho nhi·ªát ƒë·ªô)\n",
        "        scaled_score = scores / self.temperature\n",
        "\n",
        "        # 2. √Åp d·ª•ng Sigmoid\n",
        "        rewards = torch.sigmoid(scaled_score)\n",
        "\n",
        "        return rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CPZGKTCOGFVl",
      "metadata": {
        "id": "CPZGKTCOGFVl"
      },
      "source": [
        "#### TPRecEnvironment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "911afabe",
      "metadata": {
        "id": "911afabe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TPRecEnvironment(nn.Module):\n",
        "    def __init__(self, tckg, entity_embeddings, relation_embeddings, reward_function, max_path_len=3, history_len=3):\n",
        "        \"\"\"\n",
        "        M√¥i tr∆∞·ªùng TPRec ƒë∆∞·ª£c thi·∫øt k·∫ø chu·∫©n x√°c theo ki·∫øn tr√∫c Sequence State\n",
        "        \"\"\"\n",
        "        super(TPRecEnvironment, self).__init__()\n",
        "        self.tckg = tckg\n",
        "        self.entity_embs = entity_embeddings\n",
        "        self.relation_embs = relation_embeddings\n",
        "        self.max_path_len = max_path_len\n",
        "        self.history_len = history_len\n",
        "\n",
        "        # L∆ØU REWARD FUNCTION ƒê∆Ø·ª¢C TRUY·ªÄN V√ÄO\n",
        "        self.reward_function = reward_function\n",
        "\n",
        "        # State tracking\n",
        "        self.current_entities = None\n",
        "        self.current_users = None\n",
        "        self.path_history = None\n",
        "        self.step_counter = 0\n",
        "\n",
        "        # Bi·∫øn l∆∞u tr·ªØ ƒë√°p √°n m·ª•c ti√™u ƒë·ªÉ t√≠nh Terminal Bonus\n",
        "        self.target_items = None\n",
        "\n",
        "    def reset(self, user_ids, target_items=None):\n",
        "        \"\"\"\n",
        "        Kh·ªüi t·∫°o tr·∫°ng th√°i s_0 = (u, u, ‚àÖ)\n",
        "        C·∫≠p nh·∫≠t: N·∫°p th√™m target_items ƒë·ªÉ Tr·ªçng t√†i (M√¥i tr∆∞·ªùng) c·∫ßm s·∫µn ƒë√°p √°n\n",
        "        \"\"\"\n",
        "        batch_size = user_ids.size(0)\n",
        "\n",
        "        self.current_users = user_ids\n",
        "        self.current_entities = user_ids\n",
        "\n",
        "        # N·∫†P ƒê√ÅP √ÅN: L∆∞u l·∫°i Target Items cho b∆∞·ªõc t√≠nh Reward cu·ªëi c√πng\n",
        "        self.target_items = target_items\n",
        "\n",
        "        # History h_k: store (entity, relation) theo chu·∫©n paper\n",
        "        self.path_history = torch.zeros((batch_size, self.history_len * 2),\n",
        "                                        dtype=torch.long,\n",
        "                                        device=user_ids.device)\n",
        "\n",
        "        self.step_counter = 0\n",
        "        return self._get_state_embedding()\n",
        "\n",
        "    def _get_state_embedding(self):\n",
        "        \"\"\"\n",
        "        K·∫øt h·ª£p u, h_k, e_k th√†nh m·ªôt chu·ªói duy nh·∫•t ƒë∆∞a v√†o BLSTM\n",
        "        \"\"\"\n",
        "        # 1. Th√™m chi·ªÅu th·ªùi gian (unsqueeze) cho u v√† e_k\n",
        "        u_emb = self.entity_embs(self.current_users).unsqueeze(1)    # Shape: (B, 1, d)\n",
        "        e_emb = self.entity_embs(self.current_entities).unsqueeze(1) # Shape: (B, 1, d)\n",
        "\n",
        "        # 2. L·∫•y l·ªãch s·ª≠ e v√† r\n",
        "        e_indices = self.path_history[:, 0::2]\n",
        "        r_indices = self.path_history[:, 1::2]\n",
        "\n",
        "        e_vecs = self.entity_embs(e_indices)   # Shape: (B, L, d)\n",
        "        r_vecs = self.relation_embs(r_indices) # Shape: (B, L, d)\n",
        "\n",
        "        # 3. Tr·ªôn xen k·∫Ω e v√† r th√†nh chu·ªói l·ªãch s·ª≠ [e1, r1, e2, r2...]\n",
        "        B, L, d = e_vecs.shape\n",
        "        history_seq = torch.zeros((B, L * 2, d), device=e_vecs.device)\n",
        "        history_seq[:, 0::2, :] = e_vecs\n",
        "        history_seq[:, 1::2, :] = r_vecs\n",
        "\n",
        "        # 4. K·∫æT N·ªêI TO√ÄN B·ªò THEO ƒê√öNG C√îNG TH·ª®C S_K TRONG PAPER\n",
        "        full_state_seq = torch.cat([u_emb, history_seq, e_emb], dim=1) # Shape: (B, Seq_Len, d)\n",
        "\n",
        "        # Tr·∫£ v·ªÅ ƒê√öNG 1 BI·∫æN DUY NH·∫§T ƒë·∫°i di·ªán cho S_k\n",
        "        return full_state_seq\n",
        "\n",
        "    def get_pruned_actions(self, epsilon=15):\n",
        "        \"\"\"\n",
        "        Phi√™n b·∫£n T·ªëi ∆∞u h√≥a Vectorization (GPU Accelerated)\n",
        "        \"\"\"\n",
        "        batch_size = self.current_users.size(0)\n",
        "        device = self.current_users.device\n",
        "\n",
        "        # 1. K√âO V·ªÄ CPU M·ªòT L·∫¶N DUY NH·∫§T: Tr√°nh g·ªçi .item() N l·∫ßn\n",
        "        curr_nodes_cpu = self.current_entities.tolist()\n",
        "\n",
        "        # Tra c·ª©u l√°ng gi·ªÅng c·ª±c nhanh tr√™n CPU b·∫±ng List Comprehension\n",
        "        batch_neighbors = [self.tckg.get_neighbors(node) for node in curr_nodes_cpu]\n",
        "\n",
        "        # =====================================================================\n",
        "        # üõë NG·∫ÆT LU√îN LI√äN K·∫æT: CH·ªêNG R√í R·ªà D·ªÆ LI·ªÜU (TARGET LEAKAGE)\n",
        "        # =====================================================================\n",
        "        if self.target_items is not None:\n",
        "            targets_cpu = self.target_items.tolist()\n",
        "            users_cpu = self.current_users.tolist()\n",
        "            filtered_batch_neighbors = []\n",
        "\n",
        "            for i, neighbors in enumerate(batch_neighbors):\n",
        "                # N·∫æU Agent ƒëang ƒë·ª©ng t·∫°i ƒë√∫ng User g·ªëc c·ªßa n√≥\n",
        "                if curr_nodes_cpu[i] == users_cpu[i]:\n",
        "                    target_node = targets_cpu[i]\n",
        "                    # CH√âM B·ªé ƒê∆Ø·ªúNG T·∫ÆT: X√≥a Target Item kh·ªèi danh s√°ch l√°ng gi·ªÅng\n",
        "                    valid_neighbors = [n for n in neighbors if n[1] != target_node]\n",
        "                    filtered_batch_neighbors.append(valid_neighbors)\n",
        "                else:\n",
        "                    # N·∫øu Agent ƒëang ·ªü Node kh√°c (Item, T√°c gi·∫£...), cho ph√©p ƒëi b√¨nh th∆∞·ªùng\n",
        "                    filtered_batch_neighbors.append(neighbors)\n",
        "\n",
        "            batch_neighbors = filtered_batch_neighbors\n",
        "        # =====================================================================\n",
        "\n",
        "        # T√¨m node c√≥ s·ªë l∆∞·ª£ng l√°ng gi·ªÅng l·ªõn nh·∫•t trong batch n√†y\n",
        "        lengths = [len(n) for n in batch_neighbors]\n",
        "        max_len = max(lengths) if lengths else 0\n",
        "\n",
        "        valid_actions = []\n",
        "\n",
        "        # N·∫øu to√†n b·ªô batch ƒë·ªÅu r∆°i v√†o ng√µ c·ª•t\n",
        "        if max_len == 0:\n",
        "            return [[] for _ in range(batch_size)]\n",
        "\n",
        "        # 2. T·∫†O MA TR·∫¨N PADDING TR√äN CPU\n",
        "        batch_rels = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "        batch_next_nodes = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "        mask = torch.zeros((batch_size, max_len), dtype=torch.bool)\n",
        "\n",
        "        # ƒêi·ªÅn d·ªØ li·ªáu v√†o ma tr·∫≠n\n",
        "        for i, neighbors in enumerate(batch_neighbors):\n",
        "            num_n = lengths[i]\n",
        "            if num_n > 0:\n",
        "                batch_rels[i, :num_n] = torch.tensor([n[0] for n in neighbors])\n",
        "                batch_next_nodes[i, :num_n] = torch.tensor([n[1] for n in neighbors])\n",
        "                mask[i, :num_n] = True\n",
        "\n",
        "        # 3. ƒê·∫®Y TO√ÄN B·ªò MA TR·∫¨N L√äN GPU M·ªòT L·∫¶N DUY NH·∫§T\n",
        "        batch_rels = batch_rels.to(device)\n",
        "        batch_next_nodes = batch_next_nodes.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        # 4. T√çNH TO√ÅN SONG SONG B·∫∞NG MA TR·∫¨N TR√äN GPU\n",
        "        curr_emb = self.entity_embs(self.current_entities) # L·∫•y node HI·ªÜN T·∫†I (ƒê√£ s·ª≠a)\n",
        "        r_emb = self.relation_embs(batch_rels)\n",
        "        n_emb = self.entity_embs(batch_next_nodes)\n",
        "\n",
        "        # T√≠nh Query: C·∫ßn unsqueeze curr_emb ƒë·ªÉ c·ªông broadcast v·ªõi r_emb\n",
        "        query = curr_emb.unsqueeze(1) + r_emb\n",
        "\n",
        "        # L·ªñI C≈® C·∫¶N X√ìA: scores = torch.sum(query * n_emb, dim=-1)\n",
        "\n",
        "        # C√ÅCH CHU·∫®N M·ªöI D√ÄNH CHO TRANSE:\n",
        "        # T√≠nh kho·∫£ng c√°ch L2 (Euclidean distance) gi·ªØa (h+r) v√† t\n",
        "        # Kho·∫£ng c√°ch c√†ng nh·ªè c√†ng t·ªët -> Th√™m d·∫•u tr·ª´ (-) ƒë·ªÉ h√†m topk ch·ªçn gi√° tr·ªã g·∫ßn 0 nh·∫•t\n",
        "        distances = torch.norm(query - n_emb, p=2, dim=-1)\n",
        "        scores = -distances\n",
        "\n",
        "        # CHE PADDING (Masking)\n",
        "        scores = scores.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "        # 5. CH·ªåN TOP-K CHO C·∫¢ BATCH\n",
        "        k = min(epsilon, max_len)\n",
        "        top_scores, top_indices = torch.topk(scores, k, dim=1) # (B, k)\n",
        "\n",
        "        # 6. K√âO K·∫æT QU·∫¢ V·ªÄ L·∫†I CPU\n",
        "        top_indices_cpu = top_indices.tolist()\n",
        "        batch_rels_cpu = batch_rels.tolist()\n",
        "        batch_nodes_cpu = batch_next_nodes.tolist()\n",
        "\n",
        "        # Gi·∫£i n√©n th√†nh d·∫°ng List g·ªëc\n",
        "        for i in range(batch_size):\n",
        "            actions_i = []\n",
        "            num_real = lengths[i]\n",
        "\n",
        "            if num_real > 0:\n",
        "                valid_k = min(k, num_real)\n",
        "                for j in range(valid_k):\n",
        "                    idx = top_indices_cpu[i][j]\n",
        "                    actions_i.append((batch_rels_cpu[i][idx], batch_nodes_cpu[i][idx]))\n",
        "\n",
        "            valid_actions.append(actions_i)\n",
        "\n",
        "        return valid_actions\n",
        "\n",
        "    def get_action_space_batch(self):\n",
        "        \"\"\"\n",
        "        L·∫•y kh√¥ng gian h√†nh ƒë·ªông cho c·∫£ Batch v√† Padding th√†nh Tensor.\n",
        "        \"\"\"\n",
        "        raw_actions_list = self.get_pruned_actions()\n",
        "        batch_size = len(raw_actions_list)\n",
        "\n",
        "        lengths = [len(acts) for acts in raw_actions_list]\n",
        "        max_len = max(lengths) if lengths else 0\n",
        "        if max_len == 0:\n",
        "            max_len = 1\n",
        "\n",
        "        device = self.current_entities.device\n",
        "\n",
        "        r_indices = torch.zeros((batch_size, max_len), dtype=torch.long, device=device)\n",
        "        e_indices = torch.zeros((batch_size, max_len), dtype=torch.long, device=device)\n",
        "        action_mask = torch.zeros((batch_size, max_len), dtype=torch.float, device=device)\n",
        "\n",
        "        for i, actions in enumerate(raw_actions_list):\n",
        "            num_acts = len(actions)\n",
        "            if num_acts > 0:\n",
        "                rs = [a[0] for a in actions]\n",
        "                es = [a[1] for a in actions]\n",
        "\n",
        "                r_indices[i, :num_acts] = torch.tensor(rs, device=device)\n",
        "                e_indices[i, :num_acts] = torch.tensor(es, device=device)\n",
        "                action_mask[i, :num_acts] = 1.0\n",
        "\n",
        "        r_emb = self.relation_embs(r_indices)\n",
        "        e_emb = self.entity_embs(e_indices)\n",
        "\n",
        "        action_embs = torch.cat([r_emb, e_emb], dim=-1)\n",
        "\n",
        "        return action_embs, action_mask, raw_actions_list\n",
        "\n",
        "    def step(self, actions):\n",
        "        \"\"\"\n",
        "        Transition function (Eq 9): Chuy·ªÉn tr·∫°ng th√°i sang b∆∞·ªõc k+1\n",
        "        \"\"\"\n",
        "        device = self.current_entities.device\n",
        "        batch_size = len(actions)\n",
        "\n",
        "        rels_list = [a[0] for a in actions]\n",
        "        ents_list = [a[1] for a in actions]\n",
        "\n",
        "        next_relations = torch.tensor(rels_list, dtype=torch.long, device=device)\n",
        "        next_entities = torch.tensor(ents_list, dtype=torch.long, device=device)\n",
        "\n",
        "        # L·∫•y Node hi·ªán t·∫°i l√†m ƒëi·ªÉm xu·∫•t ph√°t (e_{k-1})\n",
        "        curr_e = self.current_entities.unsqueeze(1)\n",
        "        new_r = next_relations.unsqueeze(1)\n",
        "\n",
        "        # N·ªëi l·∫°i theo chu·∫©n Paper: [e_{k-1}, r_k]\n",
        "        new_entry = torch.cat([curr_e, new_r], dim=1)\n",
        "\n",
        "        history_shifted = self.path_history[:, 2:]\n",
        "        self.path_history = torch.cat([history_shifted, new_entry], dim=1)\n",
        "\n",
        "        self.current_entities = next_entities #tensor([8673, 8614])\n",
        "        self.step_counter += 1\n",
        "        done = (self.step_counter >= self.max_path_len)\n",
        "\n",
        "        return self._get_state_embedding(), done\n",
        "\n",
        "    def step_with_indices(self, action_indices, raw_actions_list):\n",
        "        \"\"\"\n",
        "        Th·ª±c hi·ªán b∆∞·ªõc ƒëi d·ª±a tr√™n Index (0, 1, 2...) m√† Agent ch·ªçn.\n",
        "        \"\"\"\n",
        "        selected_real_actions = []\n",
        "        batch_size = len(action_indices)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            idx = action_indices[i].item()\n",
        "            user_acts = raw_actions_list[i]\n",
        "\n",
        "            if len(user_acts) > 0:\n",
        "                idx = min(idx, len(user_acts) - 1)\n",
        "                real_action = user_acts[idx]\n",
        "            else:\n",
        "                curr_node = self.current_entities[i].item()\n",
        "                real_action = (0, curr_node) # Dead-end: T·ª± tr·ªè v·ªÅ ch√≠nh n√≥\n",
        "\n",
        "            selected_real_actions.append(real_action)\n",
        "\n",
        "        return self.step(selected_real_actions)\n",
        "\n",
        "    def get_reward(self):\n",
        "        \"\"\"\n",
        "        G·ªçi khi done=True. T√≠nh Time-aware Reward g_R(v|u) + Terminal Bonus\n",
        "        \"\"\"\n",
        "        user_ids = self.current_users\n",
        "        item_ids = self.current_entities\n",
        "\n",
        "        # history_relation_ids = []   \n",
        "        # for user_id in user_ids.tolist():   # .tolist() to transfer from gpu to cpu\n",
        "        #     batch_neighbors = self.tckg.get_neighbors(user_id)            \n",
        "        #     for (rel, tail) in batch_neighbors:\n",
        "                # history_relation_ids.append(rel)\n",
        "\n",
        "        batch_rels, batch_tails, batch_masks = self.tckg.get_batched_neighbors(user_ids.cpu())\n",
        "        \n",
        "        # 2. L·∫•y ra m·ªôt tensor ph·∫≥ng 1 chi·ªÅu (flattened) y h·ªát nh∆∞ code c≈© c·ªßa b·∫°n\n",
        "        # C√∫ ph√°p `[batch_masks]` s·∫Ω lo·∫°i b·ªè t·∫•t c·∫£ c√°c √¥ padding tr·ªëng, \n",
        "        # ch·ªâ gi·ªØ l·∫°i ƒë√∫ng c√°c relation h·ª£p l·ªá.\n",
        "        history_relation_tensor = batch_rels[batch_masks]\n",
        "\n",
        "        # 1. SOFT REWARD (ƒêi·ªÉm d·∫´n ƒë∆∞·ªùng)\n",
        "        rewards = self.reward_function(user_ids, item_ids, history_relation_ids)\n",
        "\n",
        "        return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J_EqQtvYGJid",
      "metadata": {
        "id": "J_EqQtvYGJid"
      },
      "source": [
        "#### TPRecPolicy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7faeaadc",
      "metadata": {
        "id": "7faeaadc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TPRecPolicy(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim=128, dropout=0.1):\n",
        "        super(TPRecPolicy, self).__init__()\n",
        "\n",
        "        # BLSTM gi·ªù ƒë√¢y ch·ªâ c·∫ßn nh·∫≠n input_size = embed_dim (kh√¥ng c·∫ßn nh√¢n 2 n·ªØa)\n",
        "        self.blstm = nn.LSTM(input_size=embed_dim,\n",
        "                             hidden_size=hidden_dim // 2,\n",
        "                             num_layers=1,\n",
        "                             batch_first=True,\n",
        "                             bidirectional=True)\n",
        "\n",
        "        # W1 gi·ªù ƒë√¢y ch·ªâ c·∫ßn nh·∫≠n ƒë·∫ßu ra c·ªßa BLSTM\n",
        "        self.W1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.Wa = nn.Linear(hidden_dim, embed_dim * 2)\n",
        "        self.Wc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, full_state_seq, action_embs, action_mask):\n",
        "        # 1. ƒê∆∞a to√†n b·ªô s_k v√†o BLSTM nh∆∞ c√¥ng th·ª©c (15)\n",
        "        lstm_out, _ = self.blstm(full_state_seq)\n",
        "\n",
        "        # # L·∫•y tr·∫°ng th√°i ·ªü b∆∞·ªõc th·ªùi gian cu·ªëi c√πng ƒë·∫°i di·ªán cho to√†n b·ªô chu·ªói\n",
        "        # lstm_last = lstm_out[:, -1, :]\n",
        "\n",
        "        # V√¨ bidirectional=True, hidden_dim n√†y th·ª±c ch·∫•t l√† 2 ph·∫ßn gh√©p l·∫°i\n",
        "        half_dim = self.blstm.hidden_size\n",
        "\n",
        "        # L·∫•y tr·∫°ng th√°i T√≥m t·∫Øt Chi·ªÅu ƒëi t·ªõi (·ªû index cu·ªëi c√πng: -1)\n",
        "        forward_last = lstm_out[:, -1, :half_dim]\n",
        "\n",
        "        # L·∫•y tr·∫°ng th√°i T√≥m t·∫Øt Chi·ªÅu ƒëi l√πi (·ªû index ƒë·∫ßu ti√™n: 0)\n",
        "        backward_last = lstm_out[:, 0, half_dim:]\n",
        "\n",
        "        # Gh√©p 2 t√≥m t·∫Øt n√†y l·∫°i th√†nh m·ªôt Context Vector ho√†n h·∫£o\n",
        "        lstm_last = torch.cat([forward_last, backward_last], dim=-1)\n",
        "\n",
        "        # 2. T√≠nh x_k theo ƒë√∫ng ph∆∞∆°ng tr√¨nh\n",
        "        x_k = self.dropout(torch.relu(self.W1(lstm_last)))\n",
        "\n",
        "        # --- (Ph·∫ßn t√≠nh Actor v√† Critic gi·ªØ nguy√™n) ---\n",
        "        query = self.Wa(x_k).unsqueeze(1)\n",
        "        scores = torch.sum(query * action_embs, dim=-1)\n",
        "        scores = scores.masked_fill(action_mask.bool() == False, float('-inf'))\n",
        "        probs = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        value_baseline = self.Wc(x_k).squeeze(-1)\n",
        "\n",
        "        return probs, value_baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3E6EF6goGNLD",
      "metadata": {
        "id": "3E6EF6goGNLD"
      },
      "source": [
        "#### TPRecLightningModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "74f005c9",
      "metadata": {
        "id": "74f005c9"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "class TPRecLightningModel(pl.LightningModule):\n",
        "    def __init__(self, env, policy_net, learning_rate=1e-3, beta_entropy=0.01):\n",
        "        super().__init__()\n",
        "\n",
        "        # L∆∞u l·∫°i hyperparameter (t·ª± ƒë·ªông log v√†o tensorboard n·∫øu d√πng)\n",
        "        self.save_hyperparameters(ignore=['env', 'policy_net'])\n",
        "\n",
        "        self.env = env\n",
        "        self.policy_net = policy_net\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta_entropy = beta_entropy\n",
        "\n",
        "    def forward(self, full_state_seq, action_embs, action_mask):\n",
        "        \"\"\"\n",
        "        ƒê·∫°o di·ªÖn Lightning chuy·ªÉn ti·∫øp ƒê√öNG 3 tham s·ªë cho Di·ªÖn vi√™n Policy Network\n",
        "        \"\"\"\n",
        "        return self.policy_net(full_state_seq, action_embs, action_mask)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        N∆°i di·ªÖn ra to√†n b·ªô chuy·∫øn ƒëi c·ªßa Agent trong 1 Batch\n",
        "        B·ªï sung: T√≠nh to√°n HR@10 v√† NDCG@10 tr√™n t·∫≠p Train\n",
        "        \"\"\"\n",
        "        # 1. H·ª©ng c·∫£ User v√† Target Item t·ª´ DataLoader\n",
        "        if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
        "            batch_users, target_items = batch[0], batch[1]\n",
        "        else:\n",
        "            batch_users = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "            target_items = None\n",
        "\n",
        "        batch_size = batch_users.size(0)\n",
        "\n",
        "        # 2. G·ªçi reset v√† truy·ªÅn c·∫£ target_items ƒë·ªÉ Env t√≠nh Bonus cu·ªëi c√πng\n",
        "        full_state_seq = self.env.reset(batch_users, target_items)\n",
        "\n",
        "        saved_log_probs = []\n",
        "        saved_values = []\n",
        "        saved_entropies = []\n",
        "\n",
        "        for t in range(self.env.max_path_len):\n",
        "            action_embs, action_mask, raw_actions = self.env.get_action_space_batch()\n",
        "\n",
        "            # 3. Truy·ªÅn 1 bi·∫øn state duy nh·∫•t v√†o Policy\n",
        "            probs, value_baseline = self(full_state_seq, action_embs, action_mask)\n",
        "\n",
        "            # --- [ƒêO·∫†N M·ªöI TH√äM] T√çNH TRAIN METRICS ·ªû B∆Ø·ªöC CU·ªêI C√ôNG ---\n",
        "            if t == self.env.max_path_len - 1 and target_items is not None:\n",
        "                k = min(10, probs.size(1))\n",
        "                _, topk_indices = torch.topk(probs, k=k, dim=1) # (Batch, k)\n",
        "\n",
        "                top10_list = []\n",
        "                for i in range(batch_size):\n",
        "                    user_acts = raw_actions[i]\n",
        "                    items_i = []\n",
        "                    for idx in topk_indices[i].tolist():\n",
        "                        if idx < len(user_acts):\n",
        "                            items_i.append(user_acts[idx][1])\n",
        "                    while len(items_i) < 10:\n",
        "                        items_i.append(0)\n",
        "                    top10_list.append(items_i)\n",
        "\n",
        "                final_items_top10 = torch.tensor(top10_list, device=self.device)\n",
        "                target_expanded = target_items.unsqueeze(1)\n",
        "\n",
        "                # Ch·∫•m ƒëi·ªÉm Hit Ratio\n",
        "                hits_matrix = (final_items_top10 == target_expanded).float()\n",
        "                hr_at_10 = hits_matrix.sum(dim=1).clamp(max=1.0).mean()\n",
        "\n",
        "                # Ch·∫•m ƒëi·ªÉm NDCG\n",
        "                ranks = torch.arange(1, 11, device=self.device).float()\n",
        "                discount = 1.0 / torch.log2(ranks + 1)\n",
        "                ndcg_at_10 = (hits_matrix * discount).sum(dim=1).mean()\n",
        "\n",
        "                # Ghi nh·∫≠n v√†o h·ªá th·ªëng Log\n",
        "                self.log('train_hr@10', hr_at_10, prog_bar=True, on_step=False, on_epoch=True)\n",
        "                self.log('train_ndcg@10', ndcg_at_10, prog_bar=True, on_step=False, on_epoch=True)\n",
        "            # -----------------------------------------------------------\n",
        "\n",
        "            # 4. Agent v·∫´n l·∫•y m·∫´u ng·∫´u nhi√™n (sample) ƒë·ªÉ ph·ª•c v·ª• cho RL\n",
        "            m = torch.distributions.Categorical(probs)\n",
        "            action_indices = m.sample()\n",
        "\n",
        "            saved_log_probs.append(m.log_prob(action_indices))\n",
        "            saved_values.append(value_baseline)\n",
        "            saved_entropies.append(m.entropy())\n",
        "\n",
        "            # 5. H·ª©ng State m·ªõi v√† b∆∞·ªõc sang ch·∫∑ng ti·∫øp theo\n",
        "            full_state_seq, done = self.env.step_with_indices(action_indices, raw_actions)\n",
        "\n",
        "        # =====================================================================\n",
        "        # T√çNH TO√ÅN LOSS THEO C√îNG TH·ª®C (17) V√Ä (18) C·ª¶A B√ÄI B√ÅO\n",
        "        # =====================================================================\n",
        "        final_rewards = self.env.get_reward().detach() # Shape: (Batch,)\n",
        "        \n",
        "        # 1. C√îNG TH·ª®C 17: T√≠nh Expected Reward (G) c√≥ chi·∫øt kh·∫•u \\gamma\n",
        "        # \\sum_{t=0}^{K-1} \\gamma^t R_{k+1}\n",
        "        gamma = 0.99 # H·ªá s·ªë chi·∫øt kh·∫•u chu·∫©n trong RL\n",
        "        returns = []\n",
        "        R = final_rewards # Ph·∫ßn th∆∞·ªüng nh·∫≠n ƒë∆∞·ª£c ·ªü b∆∞·ªõc cu·ªëi c√πng\n",
        "        \n",
        "        # T√≠nh ng∆∞·ª£c t·ª´ b∆∞·ªõc cu·ªëi v·ªÅ b∆∞·ªõc ƒë·∫ßu\n",
        "        for step in reversed(range(self.env.max_path_len)):\n",
        "            returns.insert(0, R)\n",
        "            R = R * gamma # Chi·∫øt kh·∫•u l√πi v·ªÅ qu√° kh·ª© 1 b∆∞·ªõc\n",
        "\n",
        "        policy_loss = 0\n",
        "        value_loss = 0\n",
        "\n",
        "        # 2. C√îNG TH·ª®C 18: T·ªëi ∆∞u REINFORCE Algorithm\n",
        "        # \\nabla_\\Theta \\log \\pi_\\Theta (G - \\hat{c}(s_k))\n",
        "        for G, log_prob, value_baseline in zip(returns, saved_log_probs, saved_values):\n",
        "            \n",
        "            # (G - \\hat{c}(s_k)): T√≠nh Advantage (S·ª± ch√™nh l·ªách gi·ªØa Th·ª±c t·∫ø G v√† K·ª≥ v·ªçng baseline)\n",
        "            advantage = G - value_baseline.detach()\n",
        "            \n",
        "            # \\nabla_\\Theta \\log \\pi_\\Theta * Advantage\n",
        "            # Th√™m d·∫•u TR·ª™ v√¨ PyTorch t·ª± ƒë·ªông Minimize Loss, trong khi ta mu·ªën Maximize Reward\n",
        "            step_policy_loss = -log_prob * advantage\n",
        "            policy_loss += step_policy_loss.mean() # L·∫•y trung b√¨nh cho c·∫£ Batch\n",
        "            \n",
        "            # Hu·∫•n luy·ªán m·∫°ng Critic (Wa) ƒë·ªÉ d·ª± ƒëo√°n G chu·∫©n x√°c h∆°n b·∫±ng MSE\n",
        "            step_value_loss = torch.nn.functional.mse_loss(value_baseline, G)\n",
        "            value_loss += step_value_loss\n",
        "\n",
        "        # T·ªïng h·ª£p Loss cho to√†n b·ªô ƒë∆∞·ªùng ƒëi\n",
        "        # (B·∫°n c√≥ th·ªÉ tr·ª´ ƒëi entropy_bonus ·ªü ƒë√¢y n·∫øu mu·ªën Agent kh√°m ph√° nhi·ªÅu h∆°n)\n",
        "        total_loss = policy_loss + value_loss \n",
        "\n",
        "        # Ghi nh·∫≠n log\n",
        "        self.log('train_reward', final_rewards.mean(), prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log('train_loss', total_loss, prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        ƒê√°nh gi√° m√¥ h√¨nh: T√≠nh HR@10 v√† NDCG@10 t·∫°i b∆∞·ªõc cu·ªëi c√πng.\n",
        "        \"\"\"\n",
        "        # 1. H·ª©ng bi·∫øn t·ª´ Dataloader\n",
        "        if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
        "            batch_users, target_items = batch[0], batch[1]\n",
        "        else:\n",
        "            batch_users = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "            target_items = None\n",
        "\n",
        "        # 2. Reset Env (Truy·ªÅn target_items ƒë·ªÉ t√≠nh reward gi·ªëng Train)\n",
        "        full_state_seq = self.env.reset(batch_users, target_items)\n",
        "        batch_size = batch_users.size(0)\n",
        "\n",
        "        final_items_top10 = None\n",
        "\n",
        "        # B·∫ÆT ƒê·∫¶U ƒêI T√åM ƒê∆Ø·ªúNG\n",
        "        for t in range(self.env.max_path_len):\n",
        "            action_embs, action_mask, raw_actions = self.env.get_action_space_batch()\n",
        "\n",
        "            # 3. Truy·ªÅn 1 bi·∫øn state duy nh·∫•t v√†o Policy\n",
        "            probs, _ = self(full_state_seq, action_embs, action_mask)\n",
        "\n",
        "            # N·∫æU L√Ä B∆Ø·ªöC CU·ªêI C√ôNG: L·∫•y Top 10 thay v√¨ Top 1\n",
        "            if t == self.env.max_path_len - 1:\n",
        "                k = min(10, probs.size(1))\n",
        "                _, topk_indices = torch.topk(probs, k=k, dim=1) # (Batch, k)\n",
        "\n",
        "                top10_list = []\n",
        "                for i in range(batch_size):\n",
        "                    user_acts = raw_actions[i]\n",
        "                    items_i = []\n",
        "\n",
        "                    for idx in topk_indices[i].tolist():\n",
        "                        if idx < len(user_acts):\n",
        "                            items_i.append(user_acts[idx][1])\n",
        "\n",
        "                    while len(items_i) < 10:\n",
        "                        items_i.append(0)\n",
        "\n",
        "                    top10_list.append(items_i)\n",
        "\n",
        "                final_items_top10 = torch.tensor(top10_list, device=self.device)\n",
        "                action_indices = topk_indices[:, 0]\n",
        "            else:\n",
        "                action_indices = torch.argmax(probs, dim=1)\n",
        "\n",
        "            # 4. H·ª©ng ƒë√∫ng 2 bi·∫øn gi·ªëng h·ªát b√™n Train\n",
        "            full_state_seq, done = self.env.step_with_indices(action_indices, raw_actions)\n",
        "\n",
        "        # T√≠nh to√°n v√† Log ph·∫ßn th∆∞·ªüng (Reward)\n",
        "        val_rewards = self.env.get_reward().detach()\n",
        "        self.log('val_reward', val_rewards.mean(), prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "\n",
        "        # --- ƒê√ÅNH GI√Å HIT RATIO @10 & NDCG @10 ---\n",
        "        if target_items is not None and final_items_top10 is not None:\n",
        "            target_expanded = target_items.unsqueeze(1)\n",
        "            hits_matrix = (final_items_top10 == target_expanded).float()\n",
        "\n",
        "            hits_per_user = hits_matrix.sum(dim=1).clamp(max=1.0)\n",
        "            hr_at_10 = hits_per_user.mean()\n",
        "            self.log('val_hr@10', hr_at_10, prog_bar=True, on_epoch=True, sync_dist=True)\n",
        "\n",
        "            ranks = torch.arange(1, 11, device=self.device).float()\n",
        "            discount = 1.0 / torch.log2(ranks + 1)\n",
        "\n",
        "            ndcg_per_user = (hits_matrix * discount).sum(dim=1)\n",
        "            ndcg_at_10 = ndcg_per_user.mean()\n",
        "            self.log('val_ndcg@10', ndcg_at_10, prog_bar=True, on_epoch=True, sync_dist=True)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # L·ªçc ra CH·ªà nh·ªØng tham s·ªë ƒëang requires_grad=True (M·∫°ng Policy)\n",
        "        # Gi√∫p tr√°nh l·ªói t·ªëi ∆∞u h√≥a c√°c tham s·ªë ƒë√£ b·ªã freeze (nh∆∞ Embeddings)\n",
        "        trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n",
        "\n",
        "        optimizer = optim.Adam(trainable_params, lr=self.learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
        "\n",
        "        return [optimizer], [scheduler]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e4d82a",
      "metadata": {
        "id": "86e4d82a"
      },
      "source": [
        "### Class Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3bd0c46e",
      "metadata": {
        "id": "3bd0c46e"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu\n",
        "    TCKG_PATH = f\"./data/{name}/{name}_TCKG.csv\"  # File CSV ch·ª©a ƒë·ªì th·ªã (nh∆∞ b·∫°n ƒë√£ g·ª≠i)\n",
        "    # TRAIN_USERS_PATH = \"data/train_users.csv\" # File ch·ª©a danh s√°ch user ID d√πng ƒë·ªÉ train\n",
        "    SAVE_DIR = \"./checkpoints\"\n",
        "\n",
        "    # Si√™u tham s·ªë Model\n",
        "    EMBED_DIM = 64\n",
        "    HIDDEN_DIM = 128\n",
        "    HISTORY_LEN = 3   # k' (ƒë·ªô d√†i l·ªãch s·ª≠)\n",
        "    MAX_PATH_LEN = 3  # K (s·ªë b∆∞·ªõc ƒëi)\n",
        "\n",
        "    # Si√™u tham s·ªë Training\n",
        "    BATCH_SIZE = 512 # Batch l·ªõn gi√∫p RL ·ªïn ƒë·ªãnh h∆°n\n",
        "    NUM_EPOCHS = 500\n",
        "    LEARNING_RATE = 1e-3\n",
        "    BETA_ENTROPY = 0.01\n",
        "\n",
        "    # Thi·∫øt b·ªã\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # IDs c·ªßa c√°c Relation Interaction (Quan tr·ªçng cho Reward)\n",
        "    # V√≠ d·ª•: 20=interacted_0, 21=interacted_1, 22=interacted_2\n",
        "    INTERACTION_CLUSTER_IDS = [21, 22, 23, 24, 45, 46, 47, 48]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad71580",
      "metadata": {
        "id": "1ad71580"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c04d198e",
      "metadata": {
        "id": "c04d198e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class InteractionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        \"\"\"Nh·∫≠n v√†o m·ªôt DataFrame v√† chuy·ªÉn ƒë·ªïi c√°c c·ªôt c·∫ßn thi·∫øt th√†nh m·∫£ng NumPy\"\"\"\n",
        "        self.users = df['user_id'].values\n",
        "        self.entities = df['entity_id'].values # ƒê√ÇY CH√çNH L√Ä TARGET ITEM\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Tr·∫£ v·ªÅ t·ªïng s·ªë d√≤ng d·ªØ li·ªáu\"\"\"\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"L·∫•y d·ªØ li·ªáu t·∫°i v·ªã tr√≠ idx v√† bi·∫øn th√†nh PyTorch Tensor\"\"\"\n",
        "        user_tensor = torch.tensor(self.users[idx], dtype=torch.long)\n",
        "        entity_tensor = torch.tensor(self.entities[idx], dtype=torch.long)\n",
        "\n",
        "        # ƒê√É S·ª¨A: Tr·∫£ v·ªÅ m·ªôt Tuple g·ªìm (User, Target_Item)\n",
        "        return user_tensor, entity_tensor\n",
        "\n",
        "# 2. ƒê·ªçc file CSV th√†nh Pandas DataFrame\n",
        "# (L∆∞u √Ω nh·ªè: M√¨nh gi·ªØ nguy√™n t√™n file c·ªßa b·∫°n, nh∆∞ng ch·ªØ 'interacions' h√¨nh nh∆∞ ƒëang thi·∫øu ch·ªØ 't', b·∫°n nh·ªõ ki·ªÉm tra l·∫°i t√™n file th·∫≠t nh√©)\n",
        "\n",
        "train_df = pd.read_csv(f'./data/{name}/{name}_train_interactions.csv')\n",
        "val_df = pd.read_csv(f'./data/{name}/{name}_val_interactions.csv')\n",
        "test_df = pd.read_csv(f'./data/{name}/{name}_test_interactions.csv')\n",
        "\n",
        "# 3. B·ªçc DataFrame v√†o class Dataset v·ª´a t·∫°o\n",
        "train_dataset = InteractionDataset(train_df)\n",
        "val_dataset = InteractionDataset(val_df)\n",
        "test_dataset = InteractionDataset(test_df)\n",
        "\n",
        "# 4. ƒê∆∞a Dataset v√†o DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d12f9c9",
      "metadata": {
        "id": "2d12f9c9"
      },
      "source": [
        "### Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2c700f1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c3cd0ac7d1404e35896fd27548060b99",
            "4315aa12aa7c432d8bc5cf28b1b26ce3"
          ]
        },
        "id": "2c700f1f",
        "outputId": "7f19b46b-54fe-43ea-ca70-86d45c3cfd56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî• Starting TPRec Training with PyTorch Lightning...\n",
            "Loading Knowledge Graph...\n",
            "Loading TCKG from ./data/book/book_TCKG.csv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TCKG Loaded successfully. Graph construction complete.\n",
            "Loading Pre-trained TransE Embeddings...\n",
            "Setting up Reward Function...\n",
            "Setting up Environment...\n",
            "Building Policy Network...\n",
            "--> State Dim: 512 | Action Dim: 128\n",
            "Packing into Lightning Module...\n",
            "Initializing Lightning Trainer...\n",
            "üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name       | Type             | Params | Mode  | FLOPs\n",
            "----------------------------------------------------------------\n",
            "0 | env        | TPRecEnvironment | 2.5 M  | train | 0    \n",
            "1 | policy_net | TPRecPolicy      | 99.7 K | train | 0    \n",
            "----------------------------------------------------------------\n",
            "138 K     Trainable params\n",
            "2.5 M     Non-trainable params\n",
            "2.6 M     Total params\n",
            "10.522    Total estimated model params size (MB)\n",
            "11        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "0         Total Flops\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/18757 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'TCKG' object has no attribute 'rel_matrix'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 135\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# DataLoader c·ªßa b·∫°n c·∫ßn ƒë∆∞·ª£c truy·ªÅn v√†o ƒë√¢y\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlightning_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:584\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:630\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[39m\n\u001b[32m    623\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    624\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    625\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    626\u001b[39m     ckpt_path,\n\u001b[32m    627\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    628\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    629\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    633\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1079\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path, weights_only)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1076\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1077\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1078\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1079\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1083\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1084\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1123\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1121\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:217\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:465\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:153\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:352\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    351\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    354\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         closure()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:177\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    180\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1368\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1338\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1339\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1342\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1343\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1344\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1345\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1346\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1366\u001b[39m \n\u001b[32m   1367\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:133\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m opt = opt_ref()\n\u001b[32m    132\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/torch/optim/optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/torch/optim/optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/torch/optim/adam.py:226\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    229\u001b[39m     params_with_grad: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_closure\u001b[39m(\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     98\u001b[39m     model: \u001b[33m\"\u001b[39m\u001b[33mpl.LightningModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m     optimizer: Steppable,\n\u001b[32m    100\u001b[39m     closure: Callable[[], Any],\n\u001b[32m    101\u001b[39m ) -> Any:\n\u001b[32m    102\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    hook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    134\u001b[39m         \u001b[38;5;28mself\u001b[39m.warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \n\u001b[32m    316\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer.world_size > \u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:329\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    332\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mTPRecLightningModel.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     92\u001b[39m     full_state_seq, done = \u001b[38;5;28mself\u001b[39m.env.step_with_indices(action_indices, raw_actions)\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# =====================================================================\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# T√çNH TO√ÅN LOSS THEO C√îNG TH·ª®C (17) V√Ä (18) C·ª¶A B√ÄI B√ÅO\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# =====================================================================\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m final_rewards = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.detach() \u001b[38;5;66;03m# Shape: (Batch,)\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# 1. C√îNG TH·ª®C 17: T√≠nh Expected Reward (G) c√≥ chi·∫øt kh·∫•u \\gamma\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# \\sum_{t=0}^{K-1} \\gamma^t R_{k+1}\u001b[39;00m\n\u001b[32m    101\u001b[39m gamma = \u001b[32m0.99\u001b[39m \u001b[38;5;66;03m# H·ªá s·ªë chi·∫øt kh·∫•u chu·∫©n trong RL\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 281\u001b[39m, in \u001b[36mTPRecEnvironment.get_reward\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m item_ids = \u001b[38;5;28mself\u001b[39m.current_entities\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# history_relation_ids = []   \u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# for user_id in user_ids.tolist():   # .tolist() to transfer from gpu to cpu\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m#     batch_neighbors = self.tckg.get_neighbors(user_id)            \u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m#     for (rel, tail) in batch_neighbors:\u001b[39;00m\n\u001b[32m    279\u001b[39m         \u001b[38;5;66;03m# history_relation_ids.append(rel)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m batch_rels, batch_tails, batch_masks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtckg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_batched_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# 2. L·∫•y ra m·ªôt tensor ph·∫≥ng 1 chi·ªÅu (flattened) y h·ªát nh∆∞ code c≈© c·ªßa b·∫°n\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# C√∫ ph√°p `[batch_masks]` s·∫Ω lo·∫°i b·ªè t·∫•t c·∫£ c√°c √¥ padding tr·ªëng, \u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# ch·ªâ gi·ªØ l·∫°i ƒë√∫ng c√°c relation h·ª£p l·ªá.\u001b[39;00m\n\u001b[32m    286\u001b[39m history_relation_tensor = batch_rels[batch_masks]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mTCKG.get_batched_neighbors\u001b[39m\u001b[34m(self, node_ids_tensor)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_batched_neighbors\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_ids_tensor):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     rels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrel_matrix\u001b[49m[node_ids_tensor]\n\u001b[32m     51\u001b[39m     tails = \u001b[38;5;28mself\u001b[39m.tail_matrix[node_ids_tensor]\n\u001b[32m     52\u001b[39m     masks = \u001b[38;5;28mself\u001b[39m.mask_matrix[node_ids_tensor]\n",
            "\u001b[31mAttributeError\u001b[39m: 'TCKG' object has no attribute 'rel_matrix'"
          ]
        }
      ],
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "cfg = Config()\n",
        "print(\"üî• Starting TPRec Training with PyTorch Lightning...\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# B∆Ø·ªöC 1: X√¢y d·ª±ng ƒê·ªì th·ªã Tri th·ª©c (TCKG)\n",
        "# -------------------------------------------------\n",
        "# L∆∞u √Ω: C·∫ßn ƒë·∫£m b·∫£o file CSV ƒë√£ ƒë∆∞·ª£c map ID v·ªÅ d·∫°ng s·ªë nguy√™n li√™n t·ª•c (0, 1, 2...)\n",
        "print(\"Loading Knowledge Graph...\")\n",
        "# T√≠nh offset t·ª± ƒë·ªông (nh∆∞ class TCKG t·ªëi ∆∞u t√¥i ƒë√£ vi·∫øt)\n",
        "tckg = TCKG(cfg.TCKG_PATH)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# B∆Ø·ªöC 2: Kh·ªüi t·∫°o Embeddings t·ª´ file Pickle\n",
        "# -------------------------------------------------\n",
        "print(\"Loading Pre-trained TransE Embeddings...\")\n",
        "pickle_file_path = f'./pickle/{name}_transE_embeddings_2026-02-22_13-17-36.pkl'\n",
        "\n",
        "with open(pickle_file_path, 'rb') as f:\n",
        "    saved_data = pickle.load(f)\n",
        "\n",
        "pretrained_ent = saved_data['entity_embeddings']\n",
        "pretrained_rel = saved_data['relation_embeddings']\n",
        "\n",
        "# 2. Chuy·ªÉn ƒë·ªïi sang PyTorch Tensor (√©p ki·ªÉu Float32 ƒë·ªÉ t√≠nh to√°n neural network)\n",
        "# N·∫øu data ƒëang l√† Numpy array:\n",
        "if isinstance(pretrained_ent, np.ndarray):\n",
        "    ent_tensor = torch.tensor(pretrained_ent, dtype=torch.float32)\n",
        "    rel_tensor = torch.tensor(pretrained_rel, dtype=torch.float32)\n",
        "else:\n",
        "    # N·∫øu data ƒë√£ l√† Tensor s·∫µn:\n",
        "    ent_tensor = pretrained_ent.clone().detach().float()\n",
        "    rel_tensor = pretrained_rel.clone().detach().float()\n",
        "\n",
        "# 3. N·∫°p v√†o nn.Embedding\n",
        "# freeze=False: Cho ph√©p RL Agent ti·∫øp t·ª•c c·∫≠p nh·∫≠t (fine-tune) vector trong l√∫c t√¨m ƒë∆∞·ªùng\n",
        "# freeze=True: Kh√≥a c·ª©ng vector, RL Agent ch·ªâ h·ªçc Policy Network (H·ªçc nhanh h∆°n, ch·ªëng overfit)\n",
        "entity_embs = nn.Embedding.from_pretrained(ent_tensor, freeze=True, padding_idx=0)\n",
        "relation_embs = nn.Embedding.from_pretrained(rel_tensor, freeze=True, padding_idx=0)\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# B∆Ø·ªöC 3: Kh·ªüi t·∫°o Reward Function\n",
        "# -------------------------------------------------\n",
        "print(\"Setting up Reward Function...\")\n",
        "reward_func = TimeAwareRewardFunction(\n",
        "    user_embs=entity_embs,    # Chia s·∫ª tr·ªçng s·ªë v·ªõi Env\n",
        "    entity_embs=entity_embs,\n",
        "    relation_embs=relation_embs,\n",
        "    interaction_cluster_ids=cfg.INTERACTION_CLUSTER_IDS,\n",
        "    bias_embs=None, # T·ª± t·∫°o bias m·ªõi\n",
        "    temperature= None\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# B∆Ø·ªöC 4: Kh·ªüi t·∫°o M√¥i tr∆∞·ªùng (Environment)\n",
        "# -------------------------------------------------\n",
        "print(\"Setting up Environment...\")\n",
        "env = TPRecEnvironment(\n",
        "    tckg=tckg,\n",
        "    entity_embeddings=entity_embs,\n",
        "    relation_embeddings=relation_embs,\n",
        "    reward_function=reward_func, # Inject reward v√†o env\n",
        "    max_path_len=cfg.MAX_PATH_LEN,\n",
        "    history_len=cfg.HISTORY_LEN\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# B∆Ø·ªöC 5: Kh·ªüi t·∫°o Policy Network (Agent)\n",
        "# -------------------------------------------------\n",
        "print(\"Building Policy Network...\")\n",
        "\n",
        "# T√≠nh to√°n k√≠ch th∆∞·ªõc State v√† Action theo c√¥ng th·ª©c chu·∫©n\n",
        "# Action = Relation + Entity\n",
        "action_dim = cfg.EMBED_DIM + cfg.EMBED_DIM\n",
        "\n",
        "# State = User + Flattened_History + Current_Entity\n",
        "# Flattened_History = k' * (Relation + Entity)\n",
        "history_flat_dim = cfg.HISTORY_LEN * (cfg.EMBED_DIM + cfg.EMBED_DIM)\n",
        "state_dim = cfg.EMBED_DIM + history_flat_dim + cfg.EMBED_DIM\n",
        "\n",
        "print(f\"--> State Dim: {state_dim} | Action Dim: {action_dim}\")\n",
        "\n",
        "policy_net = TPRecPolicy(\n",
        "        embed_dim=cfg.EMBED_DIM,      # V√≠ d·ª•: 64\n",
        "        hidden_dim=cfg.HIDDEN_DIM,    # V√≠ d·ª•: 128\n",
        "        dropout=0.1                   # T·ªâ l·ªá dropout gi√∫p ch·ªëng Overfit (theo paper)\n",
        "    )\n",
        "\n",
        "# B∆Ø·ªöC 6: ƒê√ìNG G√ìI V√ÄO LIGHTNING MODEL\n",
        "print(\"Packing into Lightning Module...\")\n",
        "lightning_model = TPRecLightningModel(\n",
        "    env=env,\n",
        "    policy_net=policy_net,\n",
        "    learning_rate=cfg.LEARNING_RATE,\n",
        "    beta_entropy=cfg.BETA_ENTROPY\n",
        ")\n",
        "\n",
        "# B∆Ø·ªöC 7: C·∫§U H√åNH L∆ØU BEST MODEL (CHECKPOINT)\n",
        "# T·ª± ƒë·ªông theo d√µi 'train_reward' ·ªü cu·ªëi m·ªói epoch v√† l∆∞u l·∫°i b·∫£n c√≥ ƒëi·ªÉm cao nh·∫•t\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=cfg.SAVE_DIR,\n",
        "    filename='tprec-best-{epoch:02d}-{train_reward:.4f}',\n",
        "    monitor='train_hr@10',\n",
        "    mode='max', # L∆∞u model c√≥ reward l·ªõn nh·∫•t\n",
        "    save_top_k=1,\n",
        "    save_last=True # L∆∞u th√™m model ·ªü epoch cu·ªëi c√πng ƒë·ªÉ ph√≤ng h·ªù\n",
        ")\n",
        "\n",
        "# 2. TH√äM CALLBACK EARLY STOPPING V√ÄO ƒê√ÇY\n",
        "early_stop_callback = EarlyStopping(\n",
        "      monitor='val_hr@10',   # Ph·∫£i c√πng t√™n v·ªõi bi·∫øn monitor ·ªü Checkpoint\n",
        "      min_delta=0.001,       # S·ª± thay ƒë·ªïi t·ªëi thi·ªÉu ƒë·ªÉ ƒë∆∞·ª£c t√≠nh l√† \"c√≥ c·∫£i thi·ªán\"\n",
        "      patience=500,           # S·ª©c ch·ªãu ƒë·ª±ng: Cho ph√©p m√¥ h√¨nh d·∫≠m ch√¢n t·∫°i ch·ªó t·ªëi ƒëa 10 Epoch\n",
        "      verbose=True,          # B·∫≠t in th√¥ng b√°o ra m√†n h√¨nh khi Early Stop k√≠ch ho·∫°t\n",
        "      mode='max'             # 'max' v√¨ ta mu·ªën ch·ªâ s·ªë HR@10/Reward c√†ng l·ªõn c√†ng t·ªët\n",
        "  )\n",
        "\n",
        "# B∆Ø·ªöC 8: KH·ªûI T·∫†O TRAINER V√Ä B·∫ÆT ƒê·∫¶U CH·∫†Y\n",
        "print(\"Initializing Lightning Trainer...\")\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=cfg.NUM_EPOCHS,\n",
        "    accelerator=\"auto\", # T·ª± ƒë·ªông t√¨m v√† d√πng GPU n·∫øu c√≥\n",
        "    devices=1,\n",
        "    gradient_clip_val=1.0, # T·ª± ƒë·ªông √°p d·ª•ng Gradient Clipping\n",
        "    callbacks=[checkpoint_callback, early_stop_callback],\n",
        "    enable_progress_bar=True,\n",
        "    # log_every_n_steps=10,\n",
        "    num_sanity_val_steps=0\n",
        ")\n",
        "\n",
        "print(\"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...\")\n",
        "# DataLoader c·ªßa b·∫°n c·∫ßn ƒë∆∞·ª£c truy·ªÅn v√†o ƒë√¢y\n",
        "trainer.fit(\n",
        "        model=lightning_model,\n",
        "        train_dataloaders=train_loader,\n",
        "        val_dataloaders=val_loader\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OtAQonw0SQOa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "d7858607c63f4bb7a9f78fd1c2baf953",
            "73726383c10043b99fd821ceb16f5774"
          ]
        },
        "id": "OtAQonw0SQOa",
        "outputId": "213dcb58-8ec6-4bf1-a54f-dc094290f1b0"
      },
      "outputs": [],
      "source": [
        "trainer.validate(dataloaders=test_loader, ckpt_path='best')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rs_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4315aa12aa7c432d8bc5cf28b1b26ce3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73726383c10043b99fd821ceb16f5774": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3cd0ac7d1404e35896fd27548060b99": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4315aa12aa7c432d8bc5cf28b1b26ce3",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 68/499 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span> 27/74 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:10 ‚Ä¢ 0:00:19</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">2.58it/s</span> <span style=\"font-style: italic\">v_num: 0.000 val_reward: 0.567    </span>\n                                                                                 <span style=\"font-style: italic\">val_hr@10: 0.035 val_ndcg@10:     </span>\n                                                                                 <span style=\"font-style: italic\">0.018 train_hr@10: 0.036          </span>\n                                                                                 <span style=\"font-style: italic\">train_ndcg@10: 0.019 train_reward:</span>\n                                                                                 <span style=\"font-style: italic\">0.568                             </span>\n</pre>\n",
                  "text/plain": "Epoch 68/499 \u001b[38;2;98;6;224m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;5;237m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m 27/74 \u001b[2m0:00:10 ‚Ä¢ 0:00:19\u001b[0m \u001b[2;4m2.58it/s\u001b[0m \u001b[3mv_num: 0.000 val_reward: 0.567    \u001b[0m\n                                                                                 \u001b[3mval_hr@10: 0.035 val_ndcg@10:     \u001b[0m\n                                                                                 \u001b[3m0.018 train_hr@10: 0.036          \u001b[0m\n                                                                                 \u001b[3mtrain_ndcg@10: 0.019 train_reward:\u001b[0m\n                                                                                 \u001b[3m0.568                             \u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "d7858607c63f4bb7a9f78fd1c2baf953": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_73726383c10043b99fd821ceb16f5774",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Validation <span style=\"color: #6206e0; text-decoration-color: #6206e0\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span> 12/12 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:03 ‚Ä¢ 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">3.15it/s</span>  \n</pre>\n",
                  "text/plain": "Validation \u001b[38;2;98;6;224m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m 12/12 \u001b[2m0:00:03 ‚Ä¢ 0:00:00\u001b[0m \u001b[2;4m3.15it/s\u001b[0m  \n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
