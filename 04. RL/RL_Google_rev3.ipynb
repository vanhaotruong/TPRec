{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "GKjNoSnA8oy0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKjNoSnA8oy0",
        "outputId": "cf1f5917-7c54-48a1-afbe-4d0d12d3e07e"
      },
      "outputs": [],
      "source": [
        "# !pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "d36101a0",
      "metadata": {
        "id": "d36101a0"
      },
      "outputs": [],
      "source": [
        "import random, os, pickle, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Set environment variables for reproducibility and safety\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "# 1. Configuration & Seeding\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "1edfb172",
      "metadata": {
        "id": "1edfb172"
      },
      "outputs": [],
      "source": [
        "name = 'book'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N7_iFoFJGZT8",
      "metadata": {
        "id": "N7_iFoFJGZT8"
      },
      "source": [
        "#### TCKG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "48479536",
      "metadata": {
        "id": "48479536"
      },
      "outputs": [],
      "source": [
        "class TCKG:\n",
        "    def __init__(self, tckg_csv_path):\n",
        "        self.adj_list = defaultdict(list)\n",
        "\n",
        "        print(f\"Loading TCKG from {tckg_csv_path}...\")\n",
        "        df_tckg = pd.read_csv(tckg_csv_path, usecols=['head_id', 'relation_id', 'tail_id'])\n",
        "\n",
        "        offset = int(df_tckg['relation_id'].max())\n",
        "\n",
        "        data = df_tckg[['head_id', 'relation_id', 'tail_id']].to_numpy() # Using numpy to speedup\n",
        "        for h, r, t in data:\n",
        "            h, r, t = int(h), int(r), int(t)\n",
        "\n",
        "            self.adj_list[h].append((r, t))\n",
        "            self.adj_list[t].append((r + offset, h)) # Ex: r=5 (watched) -> r_inv=105 (watched_by)\n",
        "\n",
        "        print(f\"TCKG Loaded successfully. Graph construction complete.\")\n",
        "\n",
        "    def get_neighbors(self, node_id):\n",
        "        return self.adj_list[node_id]\n",
        "\n",
        "    def get_all_nodes(self):\n",
        "        return list(self.adj_list.keys())\n",
        "        rels = self.rel_matrix[node_ids_tensor]\n",
        "        tails = self.tail_matrix[node_ids_tensor]\n",
        "        masks = self.mask_matrix[node_ids_tensor]\n",
        "        return rels, tails, masks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ux1Mk5j3GWNR",
      "metadata": {
        "id": "ux1Mk5j3GWNR"
      },
      "source": [
        "#### TimeAwareRewardFunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "ecfd9985",
      "metadata": {
        "id": "ecfd9985"
      },
      "outputs": [],
      "source": [
        "class TimeAwareRewardFunction(nn.Module):\n",
        "    def __init__(self, user_embs, entity_embs, relation_embs, interaction_cluster_ids, bias_embs=None, temperature= None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            user_embs (nn.Embedding): Embedding c·ªßa User (e_u)self.max_neighbors = max(len(edges) for edges in self.adj_list.values())\n",
        "            entity_embs (nn.Embedding): Embedding c·ªßa Item/Entity (e_v)\n",
        "            relation_embs (nn.Embedding): Embedding c·ªßa Relation (d√πng ƒë·ªÉ l·∫•y V_U)\n",
        "            interaction_cluster_ids (list or tensor): Danh s√°ch c√°c Relation ID ƒë·∫°i di·ªán cho Time Clusters.\n",
        "                                                      V√≠ d·ª•: [20, 21, 22] ·ª©ng v·ªõi interacted_0, interacted_1...\n",
        "                                                      ƒê√¢y ch√≠nh l√† t·∫≠p {V_U^1, ..., V_U^L}\n",
        "            bias_embs (nn.Embedding, optional): Bias c·ªßa entity (b_v). N·∫øu None s·∫Ω t·ª± kh·ªüi t·∫°o.\n",
        "        \"\"\"\n",
        "        super(TimeAwareRewardFunction, self).__init__()\n",
        "\n",
        "        self.user_embs = user_embs\n",
        "        self.entity_embs = entity_embs\n",
        "        self.relation_embs = relation_embs\n",
        "\n",
        "        # Danh s√°ch ID c·ªßa c√°c cluster t∆∞∆°ng t√°c theo th·ªùi gian (V_U)\n",
        "        # Chuy·ªÉn th√†nh tensor ƒë·ªÉ t√≠nh to√°n song song\n",
        "        self.register_buffer('cluster_ids', torch.tensor(interaction_cluster_ids, dtype=torch.long))\n",
        "\n",
        "        # Entity Bias (b_v) - Eq (11)\n",
        "        if bias_embs is None:\n",
        "            num_entities = entity_embs.num_embeddings - 1\n",
        "            self.bias_embs = nn.Embedding(num_entities + 1, 1, padding_idx = 0)\n",
        "            nn.init.zeros_(self.bias_embs.weight) # Kh·ªüi t·∫°o bias b·∫±ng 0\n",
        "        else:\n",
        "            self.bias_embs = bias_embs\n",
        "\n",
        "        if temperature is None:\n",
        "            self.temperature = self.entity_embs.embedding_dim ** 0.5\n",
        "        else:\n",
        "            self.temperature = temperature\n",
        "\n",
        "    def calculate_weights(self, history_relation_ids):\n",
        "        \"\"\"\n",
        "        Th·ª±c hi·ªán Eq (13): T√≠nh tr·ªçng s·ªë W_hu d·ª±a tr√™n t·∫ßn su·∫•t t∆∞∆°ng t√°c chi·ªÅu Ti·∫øn.\n",
        "        \n",
        "        Args:\n",
        "            history_relation_ids: Tensor (Batch, Max_History_Len) - C√°c relation ID t·ª´ user history (ƒë√£ pad 0).\n",
        "            \n",
        "        Returns:\n",
        "            weights: Tensor (Batch, L) - Vector tr·ªçng s·ªë th·ªùi gian cho L c·ª•m.\n",
        "        \"\"\"\n",
        "        # 1. Chu·∫©n b·ªã Tensor ƒë·ªÉ Broadcast (So kh·ªõp song song tr√™n GPU)\n",
        "        # hist_expanded: (Batch, History_Len, 1)\n",
        "        hist_expanded = history_relation_ids.unsqueeze(-1)\n",
        "        \n",
        "        # clusters_expanded: (1, 1, L) - Ch·ª©a [21, 22, 23, 24]\n",
        "        clusters_expanded = self.cluster_ids.view(1, 1, -1)\n",
        "        \n",
        "        # 2. So kh·ªõp v√† ƒê·∫øm (T·ª≠ s·ªë c·ªßa Eq 13)\n",
        "        # matches: (Batch, History_Len, L) -> True/1.0 n·∫øu kh·ªõp ID c·ª•m th·ªùi gian\n",
        "        matches = (hist_expanded == clusters_expanded).float()\n",
        "        \n",
        "        # counts: (Batch, L) -> T·ªïng s·ªë l·∫ßn user t∆∞∆°ng t√°c v√†o m·ªói c·ª•m\n",
        "        counts = matches.sum(dim=1)\n",
        "\n",
        "        # 3. T√≠nh t·ªïng s·ªë t∆∞∆°ng t√°c th·ª±c t·∫ø q (M·∫´u s·ªë c·ªßa Eq 13)\n",
        "        # B·∫±ng c√°ch t√≠nh t·ªïng c·ªßa 'counts', ta t·ª± ƒë·ªông l·ªù ƒëi to√†n b·ªô Padding (ID 0) \n",
        "        # v√† c√°c relation ID r√°c (n·∫øu c√≥), v√¨ ch√∫ng kh√¥ng match v·ªõi forward_clusters.\n",
        "        # q: (Batch, 1)\n",
        "        q = counts.sum(dim=1, keepdim=True)\n",
        "\n",
        "        # 4. Chu·∫©n h√≥a ƒë·ªÉ ra tr·ªçng s·ªë (Th√™m 1e-9 ƒë·ªÉ tr√°nh l·ªói chia cho 0 n·∫øu user ch∆∞a c√≥ l·ªãch s·ª≠)\n",
        "        # weights: (Batch, L) -> ƒê·∫£m b·∫£o t·ªïng c√°c ph·∫ßn t·ª≠ tr√™n m·ªói h√†ng lu√¥n = 1.0 (ho·∫∑c 0)\n",
        "        weights = counts / (q + 1e-9)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def forward(self, user_ids, item_ids, history_relation_ids):\n",
        "        \"\"\"\n",
        "        T√≠nh Reward Score g_R(v | u)\n",
        "\n",
        "        Args:\n",
        "            user_ids: (Batch,)\n",
        "            item_ids: (Batch,) - Item ƒë√≠ch (v_hat) m√† Agent d·ª± ƒëo√°n/d·ª´ng l·∫°i\n",
        "            history_relation_ids: (Batch, History_Len) - L·ªãch s·ª≠ relation c·ªßa user\n",
        "\n",
        "        Returns:\n",
        "            scores: (Batch,) - ƒêi·ªÉm reward\n",
        "        \"\"\"\n",
        "        # --- B∆Ø·ªöC 1: L·∫•y Embeddings c∆° b·∫£n ---\n",
        "        u_e = self.user_embs(user_ids)       # (B, Dim) -> e_u\n",
        "        \n",
        "        v_e = self.entity_embs(item_ids)     # (B, Dim) -> e_v\n",
        "        v_b = self.bias_embs(item_ids).squeeze(-1) # (B,) -> b_v\n",
        "\n",
        "        # --- B∆Ø·ªöC 2: T√≠nh Personalized Interaction Relation (Eq 12) ---\n",
        "        # r_vu^T = W_hu * V_U\n",
        "\n",
        "        # a. T√≠nh weights (B, L)\n",
        "        weights = self.calculate_weights(history_relation_ids)\n",
        "\n",
        "        # b. L·∫•y embedding c·ªßa c√°c cluster V_U^1...L\n",
        "        # cluster_embs shape: (L, Dim)\n",
        "        cluster_embs = self.relation_embs(self.cluster_ids)\n",
        "\n",
        "        # c. T√≠nh t·ªïng c√≥ tr·ªçng s·ªë\n",
        "        # (B, L) x (L, Dim) -> (B, Dim)\n",
        "        r_interaction = torch.matmul(weights, cluster_embs)\n",
        "\n",
        "        # --- B∆Ø·ªöC 3: T√≠nh Score (Eq 11 & Final Eq) ---\n",
        "        # g = (e_u + r_interaction) . e_v + b_v\n",
        "\n",
        "        # Dot product: (e_u + r) * e_v\n",
        "        query_vector = u_e + r_interaction # (B, Dim)\n",
        "        dot_product = torch.sum(query_vector * v_e, dim=1) # (B,)\n",
        "\n",
        "        scores = dot_product + v_b # C·ªông bias\n",
        "\n",
        "        # 1. Scale Score (Chia cho nhi·ªát ƒë·ªô)\n",
        "        scaled_score = scores / self.temperature\n",
        "\n",
        "        # 2. √Åp d·ª•ng Sigmoid\n",
        "        rewards = torch.sigmoid(scaled_score)\n",
        "\n",
        "        return rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CPZGKTCOGFVl",
      "metadata": {
        "id": "CPZGKTCOGFVl"
      },
      "source": [
        "#### TPRecEnvironment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "911afabe",
      "metadata": {
        "id": "911afabe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class TPRecEnvironment(nn.Module):\n",
        "    def __init__(self, tckg, entity_embeddings, relation_embeddings, reward_function, max_path_len=3, history_len=3):\n",
        "        \"\"\"\n",
        "        M√¥i tr∆∞·ªùng TPRec ƒë∆∞·ª£c thi·∫øt k·∫ø chu·∫©n x√°c theo ki·∫øn tr√∫c Sequence State\n",
        "        \"\"\"\n",
        "        super(TPRecEnvironment, self).__init__()\n",
        "        self.tckg = tckg\n",
        "        self.entity_embs = entity_embeddings\n",
        "        self.relation_embs = relation_embeddings\n",
        "        self.max_path_len = max_path_len\n",
        "        self.history_len = history_len\n",
        "\n",
        "        # L∆ØU REWARD FUNCTION ƒê∆Ø·ª¢C TRUY·ªÄN V√ÄO\n",
        "        self.reward_function = reward_function\n",
        "\n",
        "        # State tracking\n",
        "        self.current_entities = None\n",
        "        self.current_users = None\n",
        "        self.path_history = None\n",
        "        self.step_counter = 0\n",
        "\n",
        "        # Bi·∫øn l∆∞u tr·ªØ ƒë√°p √°n m·ª•c ti√™u ƒë·ªÉ t√≠nh Terminal Bonus\n",
        "        self.target_items = None\n",
        "\n",
        "    def reset(self, user_ids, target_items=None):\n",
        "        \"\"\"\n",
        "        Kh·ªüi t·∫°o tr·∫°ng th√°i s_0 = (u, u, ‚àÖ)\n",
        "        C·∫≠p nh·∫≠t: N·∫°p th√™m target_items ƒë·ªÉ Tr·ªçng t√†i (M√¥i tr∆∞·ªùng) c·∫ßm s·∫µn ƒë√°p √°n\n",
        "        \"\"\"\n",
        "        batch_size = user_ids.size(0)\n",
        "\n",
        "        self.current_users = user_ids\n",
        "        self.current_entities = user_ids\n",
        "\n",
        "        # N·∫†P ƒê√ÅP √ÅN: L∆∞u l·∫°i Target Items cho b∆∞·ªõc t√≠nh Reward cu·ªëi c√πng\n",
        "        self.target_items = target_items\n",
        "\n",
        "        # History h_k: store (entity, relation) theo chu·∫©n paper\n",
        "        self.path_history = torch.zeros((batch_size, self.history_len * 2),\n",
        "                                        dtype=torch.long,\n",
        "                                        device=user_ids.device)\n",
        "\n",
        "        self.step_counter = 0\n",
        "        return self._get_state_embedding()\n",
        "\n",
        "    def _get_state_embedding(self):\n",
        "        \"\"\"\n",
        "        K·∫øt h·ª£p u, h_k, e_k th√†nh m·ªôt chu·ªói duy nh·∫•t ƒë∆∞a v√†o BLSTM\n",
        "        \"\"\"\n",
        "        # 1. Th√™m chi·ªÅu th·ªùi gian (unsqueeze) cho u v√† e_k\n",
        "        u_emb = self.entity_embs(self.current_users).unsqueeze(1)    # Shape: (B, 1, d)\n",
        "        e_emb = self.entity_embs(self.current_entities).unsqueeze(1) # Shape: (B, 1, d)\n",
        "\n",
        "        # 2. L·∫•y l·ªãch s·ª≠ e v√† r\n",
        "        e_indices = self.path_history[:, 0::2]\n",
        "        r_indices = self.path_history[:, 1::2]\n",
        "\n",
        "        e_vecs = self.entity_embs(e_indices)   # Shape: (B, L, d)\n",
        "        r_vecs = self.relation_embs(r_indices) # Shape: (B, L, d)\n",
        "\n",
        "        # 3. Tr·ªôn xen k·∫Ω e v√† r th√†nh chu·ªói l·ªãch s·ª≠ [e1, r1, e2, r2...]\n",
        "        B, L, d = e_vecs.shape\n",
        "        history_seq = torch.zeros((B, L * 2, d), device=e_vecs.device)\n",
        "        history_seq[:, 0::2, :] = e_vecs\n",
        "        history_seq[:, 1::2, :] = r_vecs\n",
        "\n",
        "        # 4. K·∫æT N·ªêI TO√ÄN B·ªò THEO ƒê√öNG C√îNG TH·ª®C S_K TRONG PAPER\n",
        "        full_state_seq = torch.cat([u_emb, history_seq, e_emb], dim=1) # Shape: (B, Seq_Len, d)\n",
        "\n",
        "        # Tr·∫£ v·ªÅ ƒê√öNG 1 BI·∫æN DUY NH·∫§T ƒë·∫°i di·ªán cho S_k\n",
        "        return full_state_seq\n",
        "\n",
        "    def get_pruned_actions(self, epsilon=15):\n",
        "        \"\"\"\n",
        "        Phi√™n b·∫£n T·ªëi ∆∞u h√≥a Vectorization (GPU Accelerated)\n",
        "        \"\"\"\n",
        "        batch_size = self.current_users.size(0)\n",
        "        device = self.current_users.device\n",
        "\n",
        "        # 1. K√âO V·ªÄ CPU M·ªòT L·∫¶N DUY NH·∫§T: Tr√°nh g·ªçi .item() N l·∫ßn\n",
        "        curr_nodes_cpu = self.current_entities.tolist()\n",
        "\n",
        "        # Tra c·ª©u l√°ng gi·ªÅng c·ª±c nhanh tr√™n CPU b·∫±ng List Comprehension\n",
        "        batch_neighbors = [self.tckg.get_neighbors(node) for node in curr_nodes_cpu]\n",
        "\n",
        "        # =====================================================================\n",
        "        # üõë NG·∫ÆT LU√îN LI√äN K·∫æT: CH·ªêNG R√í R·ªà D·ªÆ LI·ªÜU (TARGET LEAKAGE)\n",
        "        # =====================================================================\n",
        "        if self.target_items is not None:\n",
        "            targets_cpu = self.target_items.tolist()\n",
        "            users_cpu = self.current_users.tolist()\n",
        "            filtered_batch_neighbors = []\n",
        "\n",
        "            for i, neighbors in enumerate(batch_neighbors):\n",
        "                # N·∫æU Agent ƒëang ƒë·ª©ng t·∫°i ƒë√∫ng User g·ªëc c·ªßa n√≥\n",
        "                if curr_nodes_cpu[i] == users_cpu[i]:\n",
        "                    target_node = targets_cpu[i]\n",
        "                    # CH√âM B·ªé ƒê∆Ø·ªúNG T·∫ÆT: X√≥a Target Item kh·ªèi danh s√°ch l√°ng gi·ªÅng\n",
        "                    valid_neighbors = [n for n in neighbors if n[1] != target_node]\n",
        "                    filtered_batch_neighbors.append(valid_neighbors)\n",
        "                else:\n",
        "                    # N·∫øu Agent ƒëang ·ªü Node kh√°c (Item, T√°c gi·∫£...), cho ph√©p ƒëi b√¨nh th∆∞·ªùng\n",
        "                    filtered_batch_neighbors.append(neighbors)\n",
        "\n",
        "            batch_neighbors = filtered_batch_neighbors\n",
        "        # =====================================================================\n",
        "\n",
        "        # T√¨m node c√≥ s·ªë l∆∞·ª£ng l√°ng gi·ªÅng l·ªõn nh·∫•t trong batch n√†y\n",
        "        lengths = [len(n) for n in batch_neighbors]\n",
        "        max_len = max(lengths) if lengths else 0\n",
        "\n",
        "        valid_actions = []\n",
        "\n",
        "        # N·∫øu to√†n b·ªô batch ƒë·ªÅu r∆°i v√†o ng√µ c·ª•t\n",
        "        if max_len == 0:\n",
        "            return [[] for _ in range(batch_size)]\n",
        "\n",
        "        # 2. T·∫†O MA TR·∫¨N PADDING TR√äN CPU\n",
        "        batch_rels = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "        batch_next_nodes = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "        mask = torch.zeros((batch_size, max_len), dtype=torch.bool)\n",
        "\n",
        "        # ƒêi·ªÅn d·ªØ li·ªáu v√†o ma tr·∫≠n\n",
        "        for i, neighbors in enumerate(batch_neighbors):\n",
        "            num_n = lengths[i]\n",
        "            if num_n > 0:\n",
        "                batch_rels[i, :num_n] = torch.tensor([n[0] for n in neighbors])\n",
        "                batch_next_nodes[i, :num_n] = torch.tensor([n[1] for n in neighbors])\n",
        "                mask[i, :num_n] = True\n",
        "\n",
        "        # 3. ƒê·∫®Y TO√ÄN B·ªò MA TR·∫¨N L√äN GPU M·ªòT L·∫¶N DUY NH·∫§T\n",
        "        batch_rels = batch_rels.to(device)\n",
        "        batch_next_nodes = batch_next_nodes.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        # 4. T√çNH TO√ÅN SONG SONG B·∫∞NG MA TR·∫¨N TR√äN GPU\n",
        "        curr_emb = self.entity_embs(self.current_entities) # L·∫•y node HI·ªÜN T·∫†I (ƒê√£ s·ª≠a)\n",
        "        r_emb = self.relation_embs(batch_rels)\n",
        "        n_emb = self.entity_embs(batch_next_nodes)\n",
        "\n",
        "        # T√≠nh Query: C·∫ßn unsqueeze curr_emb ƒë·ªÉ c·ªông broadcast v·ªõi r_emb\n",
        "        query = curr_emb.unsqueeze(1) + r_emb\n",
        "\n",
        "        # L·ªñI C≈® C·∫¶N X√ìA: scores = torch.sum(query * n_emb, dim=-1)\n",
        "\n",
        "        # C√ÅCH CHU·∫®N M·ªöI D√ÄNH CHO TRANSE:\n",
        "        # T√≠nh kho·∫£ng c√°ch L2 (Euclidean distance) gi·ªØa (h+r) v√† t\n",
        "        # Kho·∫£ng c√°ch c√†ng nh·ªè c√†ng t·ªët -> Th√™m d·∫•u tr·ª´ (-) ƒë·ªÉ h√†m topk ch·ªçn gi√° tr·ªã g·∫ßn 0 nh·∫•t\n",
        "        distances = torch.norm(query - n_emb, p=2, dim=-1)\n",
        "        scores = -distances\n",
        "\n",
        "        # CHE PADDING (Masking)\n",
        "        scores = scores.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "        # 5. CH·ªåN TOP-K CHO C·∫¢ BATCH\n",
        "        k = min(epsilon, max_len)\n",
        "        top_scores, top_indices = torch.topk(scores, k, dim=1) # (B, k)\n",
        "\n",
        "        # 6. K√âO K·∫æT QU·∫¢ V·ªÄ L·∫†I CPU\n",
        "        top_indices_cpu = top_indices.tolist()\n",
        "        batch_rels_cpu = batch_rels.tolist()\n",
        "        batch_nodes_cpu = batch_next_nodes.tolist()\n",
        "\n",
        "        # Gi·∫£i n√©n th√†nh d·∫°ng List g·ªëc\n",
        "        for i in range(batch_size):\n",
        "            actions_i = []\n",
        "            num_real = lengths[i]\n",
        "\n",
        "            if num_real > 0:\n",
        "                valid_k = min(k, num_real)\n",
        "                for j in range(valid_k):\n",
        "                    idx = top_indices_cpu[i][j]\n",
        "                    actions_i.append((batch_rels_cpu[i][idx], batch_nodes_cpu[i][idx]))\n",
        "\n",
        "            valid_actions.append(actions_i)\n",
        "\n",
        "        return valid_actions\n",
        "\n",
        "    def get_action_space_batch(self):\n",
        "        \"\"\"\n",
        "        L·∫•y kh√¥ng gian h√†nh ƒë·ªông cho c·∫£ Batch v√† Padding th√†nh Tensor.\n",
        "        \"\"\"\n",
        "        raw_actions_list = self.get_pruned_actions()\n",
        "        batch_size = len(raw_actions_list)\n",
        "\n",
        "        lengths = [len(acts) for acts in raw_actions_list]\n",
        "        max_len = max(lengths) if lengths else 0\n",
        "        if max_len == 0:\n",
        "            max_len = 1\n",
        "\n",
        "        device = self.current_entities.device\n",
        "\n",
        "        r_indices = torch.zeros((batch_size, max_len), dtype=torch.long, device=device)\n",
        "        e_indices = torch.zeros((batch_size, max_len), dtype=torch.long, device=device)\n",
        "        action_mask = torch.zeros((batch_size, max_len), dtype=torch.float, device=device)\n",
        "\n",
        "        for i, actions in enumerate(raw_actions_list):\n",
        "            num_acts = len(actions)\n",
        "            if num_acts > 0:\n",
        "                rs = [a[0] for a in actions]\n",
        "                es = [a[1] for a in actions]\n",
        "\n",
        "                r_indices[i, :num_acts] = torch.tensor(rs, device=device)\n",
        "                e_indices[i, :num_acts] = torch.tensor(es, device=device)\n",
        "                action_mask[i, :num_acts] = 1.0\n",
        "\n",
        "        r_emb = self.relation_embs(r_indices)\n",
        "        e_emb = self.entity_embs(e_indices)\n",
        "\n",
        "        action_embs = torch.cat([r_emb, e_emb], dim=-1)\n",
        "\n",
        "        return action_embs, action_mask, raw_actions_list\n",
        "\n",
        "    def step(self, actions):\n",
        "        \"\"\"\n",
        "        Transition function (Eq 9): Chuy·ªÉn tr·∫°ng th√°i sang b∆∞·ªõc k+1\n",
        "        \"\"\"\n",
        "        device = self.current_entities.device\n",
        "        batch_size = len(actions)\n",
        "\n",
        "        rels_list = [a[0] for a in actions]\n",
        "        ents_list = [a[1] for a in actions]\n",
        "\n",
        "        next_relations = torch.tensor(rels_list, dtype=torch.long, device=device)\n",
        "        next_entities = torch.tensor(ents_list, dtype=torch.long, device=device)\n",
        "\n",
        "        # L·∫•y Node hi·ªán t·∫°i l√†m ƒëi·ªÉm xu·∫•t ph√°t (e_{k-1})\n",
        "        curr_e = self.current_entities.unsqueeze(1)\n",
        "        new_r = next_relations.unsqueeze(1)\n",
        "\n",
        "        # N·ªëi l·∫°i theo chu·∫©n Paper: [e_{k-1}, r_k]\n",
        "        new_entry = torch.cat([curr_e, new_r], dim=1)\n",
        "\n",
        "        history_shifted = self.path_history[:, 2:]\n",
        "        self.path_history = torch.cat([history_shifted, new_entry], dim=1)\n",
        "\n",
        "        self.current_entities = next_entities #tensor([8673, 8614])\n",
        "        self.step_counter += 1\n",
        "        done = (self.step_counter >= self.max_path_len)\n",
        "\n",
        "        return self._get_state_embedding(), done\n",
        "\n",
        "    def step_with_indices(self, action_indices, raw_actions_list):\n",
        "        \"\"\"\n",
        "        Th·ª±c hi·ªán b∆∞·ªõc ƒëi d·ª±a tr√™n Index (0, 1, 2...) m√† Agent ch·ªçn.\n",
        "        \"\"\"\n",
        "        selected_real_actions = []\n",
        "        batch_size = len(action_indices)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            idx = action_indices[i].item()\n",
        "            user_acts = raw_actions_list[i]\n",
        "\n",
        "            if len(user_acts) > 0:\n",
        "                idx = min(idx, len(user_acts) - 1)\n",
        "                real_action = user_acts[idx]\n",
        "            else:\n",
        "                curr_node = self.current_entities[i].item()\n",
        "                real_action = (0, curr_node) # Dead-end: T·ª± tr·ªè v·ªÅ ch√≠nh n√≥\n",
        "\n",
        "            selected_real_actions.append(real_action)\n",
        "\n",
        "        return self.step(selected_real_actions)\n",
        "\n",
        "    def get_reward(self):\n",
        "        \"\"\"\n",
        "        G·ªçi khi done=True. T√≠nh Time-aware Reward g_R(v|u) + Terminal Bonus\n",
        "        \"\"\"\n",
        "        user_ids = self.current_users\n",
        "        item_ids = self.current_entities\n",
        "\n",
        "        batch_history_relations = [] # Ch·ª©a danh s√°ch c√°c tensor c·ªßa t·ª´ng user\n",
        "        \n",
        "        for user_id in user_ids.tolist():   # .tolist() to transfer from gpu to cpu\n",
        "            batch_neighbors = self.tckg.get_neighbors(user_id)            \n",
        "            \n",
        "            # 1. L·∫•y to√†n b·ªô relation c·ªßa user HI·ªÜN T·∫†I\n",
        "            user_rels = [rel for (rel, tail) in batch_neighbors]\n",
        "            \n",
        "            # 2. Chuy·ªÉn list c·ªßa user n√†y th√†nh tensor 1D v√† ƒë∆∞a v√†o danh s√°ch batch\n",
        "            user_rels_tensor = torch.tensor(user_rels, dtype=torch.long)\n",
        "            batch_history_relations.append(user_rels_tensor)\n",
        "        \n",
        "        # 3. Padding v√† g·ªôp th√†nh batch (Tensor 2D)\n",
        "        # batch_first=True ƒë·ªÉ output c√≥ d·∫°ng [batch_size, max_seq_len]\n",
        "        # padding_value=0 (Thay s·ªë 0 b·∫±ng ID padding th·ª±c t·∫ø c·ªßa b·∫°n n·∫øu c·∫ßn)\n",
        "        history_relation_ids = pad_sequence(\n",
        "            batch_history_relations, \n",
        "            batch_first=True, \n",
        "            padding_value=0\n",
        "        )\n",
        "        \n",
        "        # 4. (T√πy ch·ªçn) ƒê∆∞a tensor tr·∫£ v·ªÅ ƒë√∫ng thi·∫øt b·ªã (GPU) c·ªßa m√¥ h√¨nh\n",
        "        history_relation_ids = history_relation_ids.to(self.current_users.device)\n",
        "\n",
        "        # 1. SOFT REWARD (ƒêi·ªÉm d·∫´n ƒë∆∞·ªùng)\n",
        "        rewards = self.reward_function(user_ids, item_ids, history_relation_ids)\n",
        "\n",
        "        return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J_EqQtvYGJid",
      "metadata": {
        "id": "J_EqQtvYGJid"
      },
      "source": [
        "#### TPRecPolicy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "7faeaadc",
      "metadata": {
        "id": "7faeaadc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TPRecPolicy(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim=128, dropout=0.1):\n",
        "        super(TPRecPolicy, self).__init__()\n",
        "\n",
        "        # BLSTM gi·ªù ƒë√¢y ch·ªâ c·∫ßn nh·∫≠n input_size = embed_dim (kh√¥ng c·∫ßn nh√¢n 2 n·ªØa)\n",
        "        self.blstm = nn.LSTM(input_size=embed_dim,\n",
        "                             hidden_size=hidden_dim // 2,\n",
        "                             num_layers=1,\n",
        "                             batch_first=True,\n",
        "                             bidirectional=True)\n",
        "\n",
        "        # W1 gi·ªù ƒë√¢y ch·ªâ c·∫ßn nh·∫≠n ƒë·∫ßu ra c·ªßa BLSTM\n",
        "        self.W1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.Wa = nn.Linear(hidden_dim, embed_dim * 2)\n",
        "        self.Wc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, full_state_seq, action_embs, action_mask):\n",
        "        # 1. ƒê∆∞a to√†n b·ªô s_k v√†o BLSTM nh∆∞ c√¥ng th·ª©c (15)\n",
        "        lstm_out, _ = self.blstm(full_state_seq)\n",
        "\n",
        "        # # L·∫•y tr·∫°ng th√°i ·ªü b∆∞·ªõc th·ªùi gian cu·ªëi c√πng ƒë·∫°i di·ªán cho to√†n b·ªô chu·ªói\n",
        "        # lstm_last = lstm_out[:, -1, :]\n",
        "\n",
        "        # V√¨ bidirectional=True, hidden_dim n√†y th·ª±c ch·∫•t l√† 2 ph·∫ßn gh√©p l·∫°i\n",
        "        half_dim = self.blstm.hidden_size\n",
        "\n",
        "        # L·∫•y tr·∫°ng th√°i T√≥m t·∫Øt Chi·ªÅu ƒëi t·ªõi (·ªû index cu·ªëi c√πng: -1)\n",
        "        forward_last = lstm_out[:, -1, :half_dim]\n",
        "\n",
        "        # L·∫•y tr·∫°ng th√°i T√≥m t·∫Øt Chi·ªÅu ƒëi l√πi (·ªû index ƒë·∫ßu ti√™n: 0)\n",
        "        backward_last = lstm_out[:, 0, half_dim:]\n",
        "\n",
        "        # Gh√©p 2 t√≥m t·∫Øt n√†y l·∫°i th√†nh m·ªôt Context Vector ho√†n h·∫£o\n",
        "        lstm_last = torch.cat([forward_last, backward_last], dim=-1)\n",
        "\n",
        "        # 2. T√≠nh x_k theo ƒë√∫ng ph∆∞∆°ng tr√¨nh\n",
        "        x_k = self.dropout(torch.relu(self.W1(lstm_last)))\n",
        "\n",
        "        # --- (Ph·∫ßn t√≠nh Actor v√† Critic gi·ªØ nguy√™n) ---\n",
        "        query = self.Wa(x_k).unsqueeze(1)\n",
        "        scores = torch.sum(query * action_embs, dim=-1)\n",
        "        scores = scores.masked_fill(action_mask.bool() == False, float('-inf'))\n",
        "        probs = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        value_baseline = self.Wc(x_k).squeeze(-1)\n",
        "\n",
        "        return probs, value_baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3E6EF6goGNLD",
      "metadata": {
        "id": "3E6EF6goGNLD"
      },
      "source": [
        "#### TPRecLightningModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74f005c9",
      "metadata": {
        "id": "74f005c9"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "class TPRecLightningModel(pl.LightningModule):\n",
        "    def __init__(self, env, policy_net, learning_rate=1e-3, beta_entropy=0.01):\n",
        "        super().__init__()\n",
        "\n",
        "        # L∆∞u l·∫°i hyperparameter (t·ª± ƒë·ªông log v√†o tensorboard n·∫øu d√πng)\n",
        "        self.save_hyperparameters(ignore=['env', 'policy_net'])\n",
        "\n",
        "        self.env = env\n",
        "        self.policy_net = policy_net\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta_entropy = beta_entropy\n",
        "\n",
        "    def forward(self, full_state_seq, action_embs, action_mask):\n",
        "        \"\"\"\n",
        "        ƒê·∫°o di·ªÖn Lightning chuy·ªÉn ti·∫øp ƒê√öNG 3 tham s·ªë cho Di·ªÖn vi√™n Policy Network\n",
        "        \"\"\"\n",
        "        return self.policy_net(full_state_seq, action_embs, action_mask)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        N∆°i di·ªÖn ra to√†n b·ªô chuy·∫øn ƒëi c·ªßa Agent trong 1 Batch\n",
        "        B·ªï sung: T√≠nh to√°n HR@10 v√† NDCG@10 tr√™n t·∫≠p Train\n",
        "        \"\"\"\n",
        "        # 1. H·ª©ng c·∫£ User v√† Target Item t·ª´ DataLoader\n",
        "        if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
        "            batch_users, target_items = batch[0], batch[1]\n",
        "        else:\n",
        "            batch_users = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "            target_items = None\n",
        "\n",
        "        batch_size = batch_users.size(0)\n",
        "\n",
        "        # 2. G·ªçi reset v√† truy·ªÅn c·∫£ target_items ƒë·ªÉ Env t√≠nh Bonus cu·ªëi c√πng\n",
        "        full_state_seq = self.env.reset(batch_users, target_items)\n",
        "\n",
        "        saved_log_probs = []\n",
        "        saved_values = []\n",
        "        saved_entropies = []\n",
        "\n",
        "        for t in range(self.env.max_path_len):\n",
        "            action_embs, action_mask, raw_actions = self.env.get_action_space_batch()\n",
        "\n",
        "            # 3. Truy·ªÅn 1 bi·∫øn state duy nh·∫•t v√†o Policy\n",
        "            probs, value_baseline = self(full_state_seq, action_embs, action_mask)\n",
        "\n",
        "            # --- [ƒêO·∫†N M·ªöI TH√äM] T√çNH TRAIN METRICS ·ªû B∆Ø·ªöC CU·ªêI C√ôNG ---\n",
        "            if t == self.env.max_path_len - 1 and target_items is not None:\n",
        "                k = min(10, probs.size(1))\n",
        "                _, topk_indices = torch.topk(probs, k=k, dim=1) # (Batch, k)\n",
        "\n",
        "                top10_list = []\n",
        "                for i in range(batch_size):\n",
        "                    user_acts = raw_actions[i]\n",
        "                    items_i = []\n",
        "                    for idx in topk_indices[i].tolist():\n",
        "                        if idx < len(user_acts):\n",
        "                            items_i.append(user_acts[idx][1])\n",
        "                    while len(items_i) < 10:\n",
        "                        items_i.append(0)\n",
        "                    top10_list.append(items_i)\n",
        "\n",
        "                final_items_top10 = torch.tensor(top10_list, device=self.device)\n",
        "                target_expanded = target_items.unsqueeze(1)\n",
        "\n",
        "                # Ch·∫•m ƒëi·ªÉm Hit Ratio\n",
        "                hits_matrix = (final_items_top10 == target_expanded).float()\n",
        "                hr_at_10 = hits_matrix.sum(dim=1).clamp(max=1.0).mean()\n",
        "\n",
        "                # Ch·∫•m ƒëi·ªÉm NDCG\n",
        "                ranks = torch.arange(1, 11, device=self.device).float()\n",
        "                discount = 1.0 / torch.log2(ranks + 1)\n",
        "                ndcg_at_10 = (hits_matrix * discount).sum(dim=1).mean()\n",
        "\n",
        "                # Ghi nh·∫≠n v√†o h·ªá th·ªëng Log\n",
        "                self.log('train_hr@10', hr_at_10, prog_bar=True, on_step=False, on_epoch=True)\n",
        "                self.log('train_ndcg@10', ndcg_at_10, prog_bar=True, on_step=False, on_epoch=True)\n",
        "            # -----------------------------------------------------------\n",
        "\n",
        "            # 4. Agent v·∫´n l·∫•y m·∫´u ng·∫´u nhi√™n (sample) ƒë·ªÉ ph·ª•c v·ª• cho RL\n",
        "            m = torch.distributions.Categorical(probs)\n",
        "            action_indices = m.sample()\n",
        "\n",
        "            saved_log_probs.append(m.log_prob(action_indices))\n",
        "            saved_values.append(value_baseline)\n",
        "            saved_entropies.append(m.entropy())\n",
        "\n",
        "            # 5. H·ª©ng State m·ªõi v√† b∆∞·ªõc sang ch·∫∑ng ti·∫øp theo\n",
        "            full_state_seq, done = self.env.step_with_indices(action_indices, raw_actions)\n",
        "\n",
        "        # =====================================================================\n",
        "        # T√çNH TO√ÅN LOSS THEO C√îNG TH·ª®C (17) V√Ä (18) C·ª¶A B√ÄI B√ÅO\n",
        "        # =====================================================================\n",
        "        final_rewards = self.env.M().detach() # Shape: (Batch,)\n",
        "        \n",
        "        # 1. C√îNG TH·ª®C 17: T√≠nh Expected Reward (G) c√≥ chi·∫øt kh·∫•u \\gamma\n",
        "        # \\sum_{t=0}^{K-1} \\gamma^t R_{k+1}\n",
        "        gamma = 0.99 # H·ªá s·ªë chi·∫øt kh·∫•u chu·∫©n trong RL\n",
        "        returns = []\n",
        "        R = final_rewards # Ph·∫ßn th∆∞·ªüng nh·∫≠n ƒë∆∞·ª£c ·ªü b∆∞·ªõc cu·ªëi c√πng\n",
        "        \n",
        "        # T√≠nh ng∆∞·ª£c t·ª´ b∆∞·ªõc cu·ªëi v·ªÅ b∆∞·ªõc ƒë·∫ßu\n",
        "        for step in reversed(range(self.env.max_path_len)):\n",
        "            returns.insert(0, R)\n",
        "            R = R * gamma # Chi·∫øt kh·∫•u l√πi v·ªÅ qu√° kh·ª© 1 b∆∞·ªõc\n",
        "\n",
        "        policy_loss = 0\n",
        "        value_loss = 0\n",
        "\n",
        "        # 2. C√îNG TH·ª®C 18: T·ªëi ∆∞u REINFORCE Algorithm\n",
        "        # \\nabla_\\Theta \\log \\pi_\\Theta (G - \\hat{c}(s_k))\n",
        "        for G, log_prob, value_baseline in zip(returns, saved_log_probs, saved_values):\n",
        "            \n",
        "            # (G - \\hat{c}(s_k)): T√≠nh Advantage (S·ª± ch√™nh l·ªách gi·ªØa Th·ª±c t·∫ø G v√† K·ª≥ v·ªçng baseline)\n",
        "            advantage = G - value_baseline.detach()\n",
        "            \n",
        "            # \\nabla_\\Theta \\log \\pi_\\Theta * Advantage\n",
        "            # Th√™m d·∫•u TR·ª™ v√¨ PyTorch t·ª± ƒë·ªông Minimize Loss, trong khi ta mu·ªën Maximize Reward\n",
        "            step_policy_loss = -log_prob * advantage\n",
        "            policy_loss += step_policy_loss.mean() # L·∫•y trung b√¨nh cho c·∫£ Batch\n",
        "            \n",
        "            # Hu·∫•n luy·ªán m·∫°ng Critic (Wa) ƒë·ªÉ d·ª± ƒëo√°n G chu·∫©n x√°c h∆°n b·∫±ng MSE\n",
        "            step_value_loss = torch.nn.functional.mse_loss(value_baseline, G)\n",
        "            value_loss += step_value_loss\n",
        "\n",
        "        # T·ªïng h·ª£p Loss cho to√†n b·ªô ƒë∆∞·ªùng ƒëi\n",
        "        # (B·∫°n c√≥ th·ªÉ tr·ª´ ƒëi entropy_bonus ·ªü ƒë√¢y n·∫øu mu·ªën Agent kh√°m ph√° nhi·ªÅu h∆°n)\n",
        "        total_loss = policy_loss + value_loss \n",
        "\n",
        "        # Ghi nh·∫≠n log\n",
        "        self.log('train_reward', final_rewards.mean(), prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log('train_loss', total_loss, prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        ƒê√°nh gi√° m√¥ h√¨nh: T√≠nh HR@10 v√† NDCG@10 t·∫°i b∆∞·ªõc cu·ªëi c√πng.\n",
        "        \"\"\"\n",
        "        # 1. H·ª©ng bi·∫øn t·ª´ Dataloader\n",
        "        if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
        "            batch_users, target_items = batch[0], batch[1]\n",
        "        else:\n",
        "            batch_users = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "            target_items = None\n",
        "\n",
        "        # 2. Reset Env (Truy·ªÅn target_items ƒë·ªÉ t√≠nh reward gi·ªëng Train)\n",
        "        full_state_seq = self.env.reset(batch_users, target_items)\n",
        "        batch_size = batch_users.size(0)\n",
        "\n",
        "        final_items_top10 = None\n",
        "\n",
        "        # B·∫ÆT ƒê·∫¶U ƒêI T√åM ƒê∆Ø·ªúNG\n",
        "        for t in range(self.env.max_path_len):\n",
        "            action_embs, action_mask, raw_actions = self.env.get_action_space_batch()\n",
        "\n",
        "            # 3. Truy·ªÅn 1 bi·∫øn state duy nh·∫•t v√†o Policy\n",
        "            probs, _ = self(full_state_seq, action_embs, action_mask)\n",
        "\n",
        "            # N·∫æU L√Ä B∆Ø·ªöC CU·ªêI C√ôNG: L·∫•y Top 10 thay v√¨ Top 1\n",
        "            if t == self.env.max_path_len - 1:\n",
        "                k = min(10, probs.size(1))\n",
        "                _, topk_indices = torch.topk(probs, k=k, dim=1) # (Batch, k)\n",
        "\n",
        "                top10_list = []\n",
        "                for i in range(batch_size):\n",
        "                    user_acts = raw_actions[i]\n",
        "                    items_i = []\n",
        "\n",
        "                    for idx in topk_indices[i].tolist():\n",
        "                        if idx < len(user_acts):\n",
        "                            items_i.append(user_acts[idx][1])\n",
        "\n",
        "                    while len(items_i) < 10:\n",
        "                        items_i.append(0)\n",
        "\n",
        "                    top10_list.append(items_i)\n",
        "\n",
        "                final_items_top10 = torch.tensor(top10_list, device=self.device)\n",
        "                action_indices = topk_indices[:, 0]\n",
        "            else:\n",
        "                action_indices = torch.argmax(probs, dim=1)\n",
        "\n",
        "            # 4. H·ª©ng ƒë√∫ng 2 bi·∫øn gi·ªëng h·ªát b√™n Train\n",
        "            full_state_seq, done = self.env.step_with_indices(action_indices, raw_actions)\n",
        "\n",
        "        # T√≠nh to√°n v√† Log ph·∫ßn th∆∞·ªüng (Reward)\n",
        "        val_rewards = self.env.get_reward().detach()\n",
        "        self.log('val_reward', val_rewards.mean(), prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "\n",
        "        # --- ƒê√ÅNH GI√Å HIT RATIO @10 & NDCG @10 ---\n",
        "        if target_items is not None and final_items_top10 is not None:\n",
        "            target_expanded = target_items.unsqueeze(1)\n",
        "            hits_matrix = (final_items_top10 == target_expanded).float()\n",
        "\n",
        "            hits_per_user = hits_matrix.sum(dim=1).clamp(max=1.0)\n",
        "            hr_at_10 = hits_per_user.mean()\n",
        "            self.log('val_hr@10', hr_at_10, prog_bar=True, on_epoch=True, sync_dist=True)\n",
        "\n",
        "            ranks = torch.arange(1, 11, device=self.device).float()\n",
        "            discount = 1.0 / torch.log2(ranks + 1)\n",
        "\n",
        "            ndcg_per_user = (hits_matrix * discount).sum(dim=1)\n",
        "            ndcg_at_10 = ndcg_per_user.mean()\n",
        "            self.log('val_ndcg@10', ndcg_at_10, prog_bar=True, on_epoch=True, sync_dist=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # L·ªçc ra CH·ªà nh·ªØng tham s·ªë ƒëang requires_grad=True (M·∫°ng Policy)\n",
        "        # Gi√∫p tr√°nh l·ªói t·ªëi ∆∞u h√≥a c√°c tham s·ªë ƒë√£ b·ªã freeze (nh∆∞ Embeddings)\n",
        "        trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n",
        "\n",
        "        optimizer = optim.Adam(trainable_params, lr=self.learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
        "\n",
        "        return [optimizer], [scheduler]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e4d82a",
      "metadata": {
        "id": "86e4d82a"
      },
      "source": [
        "### Class Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "3bd0c46e",
      "metadata": {
        "id": "3bd0c46e"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu\n",
        "    TCKG_PATH = f\"./data/{name}/{name}_TCKG.csv\"  # File CSV ch·ª©a ƒë·ªì th·ªã (nh∆∞ b·∫°n ƒë√£ g·ª≠i)\n",
        "    # TRAIN_USERS_PATH = \"data/train_users.csv\" # File ch·ª©a danh s√°ch user ID d√πng ƒë·ªÉ train\n",
        "    SAVE_DIR = \"./checkpoints\"\n",
        "\n",
        "    # Si√™u tham s·ªë Model\n",
        "    EMBED_DIM = 64\n",
        "    HIDDEN_DIM = 128\n",
        "    HISTORY_LEN = 3   # k' (ƒë·ªô d√†i l·ªãch s·ª≠)\n",
        "    MAX_PATH_LEN = 3  # K (s·ªë b∆∞·ªõc ƒëi)\n",
        "\n",
        "    # Si√™u tham s·ªë Training\n",
        "    BATCH_SIZE = 512 # Batch l·ªõn gi√∫p RL ·ªïn ƒë·ªãnh h∆°n\n",
        "    NUM_EPOCHS = 500\n",
        "    LEARNING_RATE = 1e-3\n",
        "    BETA_ENTROPY = 0.01\n",
        "\n",
        "    # Thi·∫øt b·ªã\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # IDs c·ªßa c√°c Relation Interaction (Quan tr·ªçng cho Reward)\n",
        "    # V√≠ d·ª•: 20=interacted_0, 21=interacted_1, 22=interacted_2\n",
        "    INTERACTION_CLUSTER_IDS = [21, 22, 23, 24]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad71580",
      "metadata": {
        "id": "1ad71580"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "id": "c04d198e",
      "metadata": {
        "id": "c04d198e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class InteractionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        \"\"\"Nh·∫≠n v√†o m·ªôt DataFrame v√† chuy·ªÉn ƒë·ªïi c√°c c·ªôt c·∫ßn thi·∫øt th√†nh m·∫£ng NumPy\"\"\"\n",
        "        self.users = df['user_id'].values\n",
        "        self.entities = df['entity_id'].values # ƒê√ÇY CH√çNH L√Ä TARGET ITEM\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Tr·∫£ v·ªÅ t·ªïng s·ªë d√≤ng d·ªØ li·ªáu\"\"\"\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"L·∫•y d·ªØ li·ªáu t·∫°i v·ªã tr√≠ idx v√† bi·∫øn th√†nh PyTorch Tensor\"\"\"\n",
        "        user_tensor = torch.tensor(self.users[idx], dtype=torch.long)\n",
        "        entity_tensor = torch.tensor(self.entities[idx], dtype=torch.long)\n",
        "\n",
        "        # ƒê√É S·ª¨A: Tr·∫£ v·ªÅ m·ªôt Tuple g·ªìm (User, Target_Item)\n",
        "        return user_tensor, entity_tensor\n",
        "\n",
        "# 2. ƒê·ªçc file CSV th√†nh Pandas DataFrame\n",
        "# (L∆∞u √Ω nh·ªè: M√¨nh gi·ªØ nguy√™n t√™n file c·ªßa b·∫°n, nh∆∞ng ch·ªØ 'interacions' h√¨nh nh∆∞ ƒëang thi·∫øu ch·ªØ 't', b·∫°n nh·ªõ ki·ªÉm tra l·∫°i t√™n file th·∫≠t nh√©)\n",
        "\n",
        "train_df = pd.read_csv(f'./data/{name}/{name}_train_interactions.csv')\n",
        "val_df = pd.read_csv(f'./data/{name}/{name}_val_interactions.csv')\n",
        "test_df = pd.read_csv(f'./data/{name}/{name}_test_interactions.csv')\n",
        "\n",
        "# 3. B·ªçc DataFrame v√†o class Dataset v·ª´a t·∫°o\n",
        "train_dataset = InteractionDataset(train_df)\n",
        "val_dataset = InteractionDataset(val_df)\n",
        "test_dataset = InteractionDataset(test_df)\n",
        "\n",
        "# 4. ƒê∆∞a Dataset v√†o DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d12f9c9",
      "metadata": {
        "id": "2d12f9c9"
      },
      "source": [
        "### Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "id": "2c700f1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c3cd0ac7d1404e35896fd27548060b99",
            "4315aa12aa7c432d8bc5cf28b1b26ce3"
          ]
        },
        "id": "2c700f1f",
        "outputId": "7f19b46b-54fe-43ea-ca70-86d45c3cfd56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî• Starting TPRec Training with PyTorch Lightning...\n",
            "Loading Knowledge Graph...\n",
            "Loading TCKG from ./data/book/book_TCKG.csv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\n",
            "  | Name       | Type             | Params | Mode  | FLOPs\n",
            "----------------------------------------------------------------\n",
            "0 | env        | TPRecEnvironment | 2.5 M  | train | 0    \n",
            "1 | policy_net | TPRecPolicy      | 99.7 K | train | 0    \n",
            "----------------------------------------------------------------\n",
            "138 K     Trainable params\n",
            "2.5 M     Non-trainable params\n",
            "2.6 M     Total params\n",
            "10.522    Total estimated model params size (MB)\n",
            "11        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "0         Total Flops\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TCKG Loaded successfully. Graph construction complete.\n",
            "Loading Pre-trained TransE Embeddings...\n",
            "Setting up Reward Function...\n",
            "Setting up Environment...\n",
            "Building Policy Network...\n",
            "--> State Dim: 512 | Action Dim: 128\n",
            "Packing into Lightning Module...\n",
            "Initializing Lightning Trainer...\n",
            "üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...\n",
            "Epoch 0:   1%|          | 1/147 [00:00<01:05,  2.22it/s, v_num=40]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3662.67s - thread._ident is None in _get_related_thread!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/18757 [07:53<?, ?it/s]77it/s, v_num=40]\n",
            "Epoch 0:   0%|          | 1/18757 [03:29<1091:27:50,  0.00it/s, v_num=38]\n",
            "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 147/147 [00:37<00:00,  3.93it/s, v_num=40]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3699.70s - thread._ident is None in _get_related_thread!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3700.14s - thread._ident is None in _get_related_thread!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 147/147 [00:39<00:00,  3.75it/s, v_num=40, val_reward=0.501, val_hr@10=0.0301, val_ndcg@10=0.0141, train_hr@10=0.0261, train_ndcg@10=0.0124, train_reward=0.501]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_hr@10 improved. New best score: 0.030\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 147/147 [00:42<00:00,  3.49it/s, v_num=40, val_reward=0.502, val_hr@10=0.0329, val_ndcg@10=0.0159, train_hr@10=0.0299, train_ndcg@10=0.0157, train_reward=0.501]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_hr@10 improved by 0.003 >= min_delta = 0.001. New best score: 0.033\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3:   0%|          | 0/147 [00:00<?, ?it/s, v_num=40, val_reward=0.502, val_hr@10=0.0329, val_ndcg@10=0.0159, train_hr@10=0.0299, train_ndcg@10=0.0157, train_reward=0.501]          "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3785.52s - thread._ident is None in _get_related_thread!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 147/147 [00:38<00:00,  3.79it/s, v_num=40, val_reward=0.502, val_hr@10=0.0329, val_ndcg@10=0.0159, train_hr@10=0.0299, train_ndcg@10=0.0157, train_reward=0.501]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3824.30s - thread._ident is None in _get_related_thread!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 147/147 [00:41<00:00,  3.53it/s, v_num=40, val_reward=0.502, val_hr@10=0.0339, val_ndcg@10=0.0168, train_hr@10=0.0322, train_ndcg@10=0.0174, train_reward=0.501]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_hr@10 improved by 0.001 >= min_delta = 0.001. New best score: 0.034\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 147/147 [00:33<00:00,  4.34it/s, v_num=40, val_reward=0.502, val_hr@10=0.0301, val_ndcg@10=0.0155, train_hr@10=0.0321, train_ndcg@10=0.0168, train_reward=0.501]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4016.79s - thread._ident is None in _get_related_thread!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11:   0%|          | 0/147 [00:00<?, ?it/s, v_num=40, val_reward=0.502, val_hr@10=0.0308, val_ndcg@10=0.0161, train_hr@10=0.0324, train_ndcg@10=0.0174, train_reward=0.501]          "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4092.08s - thread._ident is None in _get_related_thread!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14:   0%|          | 0/147 [00:00<?, ?it/s, v_num=40, val_reward=0.502, val_hr@10=0.028, val_ndcg@10=0.0148, train_hr@10=0.0329, train_ndcg@10=0.0174, train_reward=0.501]           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4211.61s - thread._ident is None in _get_related_thread!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15:   1%|          | 1/147 [00:00<02:04,  1.18it/s, v_num=40, val_reward=0.502, val_hr@10=0.0291, val_ndcg@10=0.0153, train_hr@10=0.0333, train_ndcg@10=0.018, train_reward=0.501]   "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4252.16s - thread._ident is None in _get_related_thread!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25:   0%|          | 0/147 [00:00<?, ?it/s, v_num=40, val_reward=0.502, val_hr@10=0.0291, val_ndcg@10=0.0158, train_hr@10=0.0348, train_ndcg@10=0.0185, train_reward=0.501]          "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4606.81s - thread._ident is None in _get_related_thread!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30:   0%|          | 0/147 [00:00<?, ?it/s, v_num=40, val_reward=0.502, val_hr@10=0.0308, val_ndcg@10=0.0167, train_hr@10=0.033, train_ndcg@10=0.0175, train_reward=0.502]           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4785.48s - thread._ident is None in _get_related_thread!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 81/147 [00:21<00:17,  3.78it/s, v_num=40, val_reward=0.502, val_hr@10=0.0308, val_ndcg@10=0.0167, train_hr@10=0.033, train_ndcg@10=0.0173, train_reward=0.502]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "1",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
          ]
        }
      ],
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "cfg = Config()\n",
        "print(\"üî• Starting TPRec Training with PyTorch Lightning...\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# B∆Ø·ªöC 1: X√¢y d·ª±ng ƒê·ªì th·ªã Tri th·ª©c (TCKG)\n",
        "# -------------------------------------------------\n",
        "# L∆∞u √Ω: C·∫ßn ƒë·∫£m b·∫£o file CSV ƒë√£ ƒë∆∞·ª£c map ID v·ªÅ d·∫°ng s·ªë nguy√™n li√™n t·ª•c (0, 1, 2...)\n",
        "print(\"Loading Knowledge Graph...\")\n",
        "# T√≠nh offset t·ª± ƒë·ªông (nh∆∞ class TCKG t·ªëi ∆∞u t√¥i ƒë√£ vi·∫øt)\n",
        "tckg = TCKG(cfg.TCKG_PATH)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# B∆Ø·ªöC 2: Kh·ªüi t·∫°o Embeddings t·ª´ file Pickle\n",
        "# -------------------------------------------------\n",
        "print(\"Loading Pre-trained TransE Embeddings...\")\n",
        "pickle_file_path = f'./pickle/{name}_transE_embeddings_2026-02-22_13-17-36.pkl'\n",
        "\n",
        "with open(pickle_file_path, 'rb') as f:\n",
        "    saved_data = pickle.load(f)\n",
        "\n",
        "pretrained_ent = saved_data['entity_embeddings']\n",
        "pretrained_rel = saved_data['relation_embeddings']\n",
        "\n",
        "# 2. Chuy·ªÉn ƒë·ªïi sang PyTorch Tensor (√©p ki·ªÉu Float32 ƒë·ªÉ t√≠nh to√°n neural network)\n",
        "# N·∫øu data ƒëang l√† Numpy array:\n",
        "if isinstance(pretrained_ent, np.ndarray):\n",
        "    ent_tensor = torch.tensor(pretrained_ent, dtype=torch.float32)\n",
        "    rel_tensor = torch.tensor(pretrained_rel, dtype=torch.float32)\n",
        "else:\n",
        "    # N·∫øu data ƒë√£ l√† Tensor s·∫µn:\n",
        "    ent_tensor = pretrained_ent.clone().detach().float()\n",
        "    rel_tensor = pretrained_rel.clone().detach().float()\n",
        "\n",
        "# 3. N·∫°p v√†o nn.Embedding\n",
        "# freeze=False: Cho ph√©p RL Agent ti·∫øp t·ª•c c·∫≠p nh·∫≠t (fine-tune) vector trong l√∫c t√¨m ƒë∆∞·ªùng\n",
        "# freeze=True: Kh√≥a c·ª©ng vector, RL Agent ch·ªâ h·ªçc Policy Network (H·ªçc nhanh h∆°n, ch·ªëng overfit)\n",
        "entity_embs = nn.Embedding.from_pretrained(ent_tensor, freeze=True, padding_idx=0)\n",
        "relation_embs = nn.Embedding.from_pretrained(rel_tensor, freeze=True, padding_idx=0)\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# B∆Ø·ªöC 3: Kh·ªüi t·∫°o Reward Function\n",
        "# -------------------------------------------------\n",
        "print(\"Setting up Reward Function...\")\n",
        "reward_func = TimeAwareRewardFunction(\n",
        "    user_embs=entity_embs,    # Chia s·∫ª tr·ªçng s·ªë v·ªõi Env\n",
        "    entity_embs=entity_embs,\n",
        "    relation_embs=relation_embs,\n",
        "    interaction_cluster_ids=cfg.INTERACTION_CLUSTER_IDS,\n",
        "    bias_embs=None, # T·ª± t·∫°o bias m·ªõi\n",
        "    temperature= None\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# B∆Ø·ªöC 4: Kh·ªüi t·∫°o M√¥i tr∆∞·ªùng (Environment)\n",
        "# -------------------------------------------------\n",
        "print(\"Setting up Environment...\")\n",
        "env = TPRecEnvironment(\n",
        "    tckg=tckg,\n",
        "    entity_embeddings=entity_embs,\n",
        "    relation_embeddings=relation_embs,\n",
        "    reward_function=reward_func, # Inject reward v√†o env\n",
        "    max_path_len=cfg.MAX_PATH_LEN,\n",
        "    history_len=cfg.HISTORY_LEN\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# B∆Ø·ªöC 5: Kh·ªüi t·∫°o Policy Network (Agent)\n",
        "# -------------------------------------------------\n",
        "print(\"Building Policy Network...\")\n",
        "\n",
        "# T√≠nh to√°n k√≠ch th∆∞·ªõc State v√† Action theo c√¥ng th·ª©c chu·∫©n\n",
        "# Action = Relation + Entity\n",
        "action_dim = cfg.EMBED_DIM + cfg.EMBED_DIM\n",
        "\n",
        "# State = User + Flattened_History + Current_Entity\n",
        "# Flattened_History = k' * (Relation + Entity)\n",
        "history_flat_dim = cfg.HISTORY_LEN * (cfg.EMBED_DIM + cfg.EMBED_DIM)\n",
        "state_dim = cfg.EMBED_DIM + history_flat_dim + cfg.EMBED_DIM\n",
        "\n",
        "print(f\"--> State Dim: {state_dim} | Action Dim: {action_dim}\")\n",
        "\n",
        "policy_net = TPRecPolicy(\n",
        "        embed_dim=cfg.EMBED_DIM,      # V√≠ d·ª•: 64\n",
        "        hidden_dim=cfg.HIDDEN_DIM,    # V√≠ d·ª•: 128\n",
        "        dropout=0.1                   # T·ªâ l·ªá dropout gi√∫p ch·ªëng Overfit (theo paper)\n",
        "    )\n",
        "\n",
        "# B∆Ø·ªöC 6: ƒê√ìNG G√ìI V√ÄO LIGHTNING MODEL\n",
        "print(\"Packing into Lightning Module...\")\n",
        "lightning_model = TPRecLightningModel(\n",
        "    env=env,\n",
        "    policy_net=policy_net,\n",
        "    learning_rate=cfg.LEARNING_RATE,\n",
        "    beta_entropy=cfg.BETA_ENTROPY\n",
        ")\n",
        "\n",
        "# B∆Ø·ªöC 7: C·∫§U H√åNH L∆ØU BEST MODEL (CHECKPOINT)\n",
        "# T·ª± ƒë·ªông theo d√µi 'train_reward' ·ªü cu·ªëi m·ªói epoch v√† l∆∞u l·∫°i b·∫£n c√≥ ƒëi·ªÉm cao nh·∫•t\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=cfg.SAVE_DIR,\n",
        "    filename='tprec-best-{epoch:02d}-{train_reward:.4f}',\n",
        "    monitor='train_hr@10',\n",
        "    mode='max', # L∆∞u model c√≥ reward l·ªõn nh·∫•t\n",
        "    save_top_k=1,\n",
        "    save_last=True # L∆∞u th√™m model ·ªü epoch cu·ªëi c√πng ƒë·ªÉ ph√≤ng h·ªù\n",
        ")\n",
        "\n",
        "# 2. TH√äM CALLBACK EARLY STOPPING V√ÄO ƒê√ÇY\n",
        "early_stop_callback = EarlyStopping(\n",
        "      monitor='val_hr@10',   # Ph·∫£i c√πng t√™n v·ªõi bi·∫øn monitor ·ªü Checkpoint\n",
        "      min_delta=0.001,       # S·ª± thay ƒë·ªïi t·ªëi thi·ªÉu ƒë·ªÉ ƒë∆∞·ª£c t√≠nh l√† \"c√≥ c·∫£i thi·ªán\"\n",
        "      patience=500,           # S·ª©c ch·ªãu ƒë·ª±ng: Cho ph√©p m√¥ h√¨nh d·∫≠m ch√¢n t·∫°i ch·ªó t·ªëi ƒëa 10 Epoch\n",
        "      verbose=True,          # B·∫≠t in th√¥ng b√°o ra m√†n h√¨nh khi Early Stop k√≠ch ho·∫°t\n",
        "      mode='max'             # 'max' v√¨ ta mu·ªën ch·ªâ s·ªë HR@10/Reward c√†ng l·ªõn c√†ng t·ªët\n",
        "  )\n",
        "\n",
        "# B∆Ø·ªöC 8: KH·ªûI T·∫†O TRAINER V√Ä B·∫ÆT ƒê·∫¶U CH·∫†Y\n",
        "print(\"Initializing Lightning Trainer...\")\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=cfg.NUM_EPOCHS,\n",
        "    accelerator=\"auto\", # T·ª± ƒë·ªông t√¨m v√† d√πng GPU n·∫øu c√≥\n",
        "    devices=1,\n",
        "    gradient_clip_val=1.0, # T·ª± ƒë·ªông √°p d·ª•ng Gradient Clipping\n",
        "    callbacks=[checkpoint_callback, early_stop_callback],\n",
        "    enable_progress_bar=True,\n",
        "    # log_every_n_steps=10,\n",
        "    num_sanity_val_steps=0\n",
        ")\n",
        "\n",
        "print(\"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...\")\n",
        "# DataLoader c·ªßa b·∫°n c·∫ßn ƒë∆∞·ª£c truy·ªÅn v√†o ƒë√¢y\n",
        "trainer.fit(\n",
        "        model=lightning_model,\n",
        "        train_dataloaders=train_loader,\n",
        "        val_dataloaders=val_loader\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OtAQonw0SQOa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "d7858607c63f4bb7a9f78fd1c2baf953",
            "73726383c10043b99fd821ceb16f5774"
          ]
        },
        "id": "OtAQonw0SQOa",
        "outputId": "213dcb58-8ec6-4bf1-a54f-dc094290f1b0"
      },
      "outputs": [],
      "source": [
        "trainer.validate(dataloaders=test_loader, ckpt_path='best')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rs_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4315aa12aa7c432d8bc5cf28b1b26ce3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73726383c10043b99fd821ceb16f5774": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3cd0ac7d1404e35896fd27548060b99": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4315aa12aa7c432d8bc5cf28b1b26ce3",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 68/499 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span> 27/74 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:10 ‚Ä¢ 0:00:19</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">2.58it/s</span> <span style=\"font-style: italic\">v_num: 0.000 val_reward: 0.567    </span>\n                                                                                 <span style=\"font-style: italic\">val_hr@10: 0.035 val_ndcg@10:     </span>\n                                                                                 <span style=\"font-style: italic\">0.018 train_hr@10: 0.036          </span>\n                                                                                 <span style=\"font-style: italic\">train_ndcg@10: 0.019 train_reward:</span>\n                                                                                 <span style=\"font-style: italic\">0.568                             </span>\n</pre>\n",
                  "text/plain": "Epoch 68/499 \u001b[38;2;98;6;224m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;5;237m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m 27/74 \u001b[2m0:00:10 ‚Ä¢ 0:00:19\u001b[0m \u001b[2;4m2.58it/s\u001b[0m \u001b[3mv_num: 0.000 val_reward: 0.567    \u001b[0m\n                                                                                 \u001b[3mval_hr@10: 0.035 val_ndcg@10:     \u001b[0m\n                                                                                 \u001b[3m0.018 train_hr@10: 0.036          \u001b[0m\n                                                                                 \u001b[3mtrain_ndcg@10: 0.019 train_reward:\u001b[0m\n                                                                                 \u001b[3m0.568                             \u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "d7858607c63f4bb7a9f78fd1c2baf953": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_73726383c10043b99fd821ceb16f5774",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Validation <span style=\"color: #6206e0; text-decoration-color: #6206e0\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span> 12/12 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:03 ‚Ä¢ 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">3.15it/s</span>  \n</pre>\n",
                  "text/plain": "Validation \u001b[38;2;98;6;224m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m 12/12 \u001b[2m0:00:03 ‚Ä¢ 0:00:00\u001b[0m \u001b[2;4m3.15it/s\u001b[0m  \n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
