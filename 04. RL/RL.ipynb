{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d36101a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, pickle, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Set environment variables for reproducibility and safety\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 1. Configuration & Seeding\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1edfb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'book'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48479536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCKG:\n",
    "    def __init__(self, tckg_csv_path):\n",
    "        self.adj_list = defaultdict(list)\n",
    "        \n",
    "        print(f\"Loading TCKG from {tckg_csv_path}...\")\n",
    "        df_tckg = pd.read_csv(tckg_csv_path, usecols=['head_id', 'relation_id', 'tail_id'])\n",
    "        \n",
    "        offset = df_tckg['relation_id'].max()\n",
    "            \n",
    "        # self.num_relations = offset * 2 + 1# Total relation_id (bidirection)\n",
    "        # print(f\"Inverse Relation Offset: {offset}. Total Relation Space: {self.num_relations}\")\n",
    "\n",
    "        data = df_tckg[['head_id', 'relation_id', 'tail_id']].to_numpy() # Using numpy to speedup\n",
    "        \n",
    "        for h, r, t in data:\n",
    "            h, r, t = int(h), int(r), int(t)\n",
    "\n",
    "            self.adj_list[h].append((r, t))\n",
    "            self.adj_list[t].append((r + offset, h)) # Ex: r=5 (watched) -> r_inv=105 (watched_by)\n",
    "\n",
    "        print(f\"TCKG Loaded successfully. Graph construction complete.\")\n",
    "\n",
    "    def get_neighbors(self, node_id):\n",
    "        return self.adj_list[node_id]\n",
    "\n",
    "    def get_all_nodes(self):\n",
    "        return list(self.adj_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecfd9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAwareRewardFunction(nn.Module):\n",
    "    def __init__(self, user_embs, entity_embs, relation_embs, interaction_cluster_ids, bias_embs=None, temperature= None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_embs (nn.Embedding): Embedding c·ªßa User (e_u)\n",
    "            entity_embs (nn.Embedding): Embedding c·ªßa Item/Entity (e_v)\n",
    "            relation_embs (nn.Embedding): Embedding c·ªßa Relation (d√πng ƒë·ªÉ l·∫•y V_U)\n",
    "            interaction_cluster_ids (list or tensor): Danh s√°ch c√°c Relation ID ƒë·∫°i di·ªán cho Time Clusters.\n",
    "                                                      V√≠ d·ª•: [20, 21, 22] ·ª©ng v·ªõi interacted_0, interacted_1...\n",
    "                                                      ƒê√¢y ch√≠nh l√† t·∫≠p {V_U^1, ..., V_U^L}\n",
    "            bias_embs (nn.Embedding, optional): Bias c·ªßa entity (b_v). N·∫øu None s·∫Ω t·ª± kh·ªüi t·∫°o.\n",
    "        \"\"\"\n",
    "        super(TimeAwareRewardFunction, self).__init__()\n",
    "        \n",
    "        self.user_embs = user_embs\n",
    "        self.entity_embs = entity_embs\n",
    "        self.relation_embs = relation_embs\n",
    "        \n",
    "        # Danh s√°ch ID c·ªßa c√°c cluster t∆∞∆°ng t√°c theo th·ªùi gian (V_U)\n",
    "        # Chuy·ªÉn th√†nh tensor ƒë·ªÉ t√≠nh to√°n song song\n",
    "        self.register_buffer('cluster_ids', torch.tensor(interaction_cluster_ids, dtype=torch.long))\n",
    "        \n",
    "        # Entity Bias (b_v) - Eq (11)\n",
    "        if bias_embs is None:\n",
    "            num_entities = entity_embs.num_embeddings\n",
    "            self.bias_embs = nn.Embedding(num_entities, 1)\n",
    "            nn.init.zeros_(self.bias_embs.weight) # Kh·ªüi t·∫°o bias b·∫±ng 0\n",
    "        else:\n",
    "            self.bias_embs = bias_embs\n",
    "\n",
    "        if temperature is None:\n",
    "            self.temperature = self.entity_embs.embedding_dim ** 0.5\n",
    "        else:\n",
    "            self.temperature = temperature\n",
    "\n",
    "    def calculate_weights(self, history_relation_ids):\n",
    "        \"\"\"\n",
    "        Th·ª±c hi·ªán Eq (13): T√≠nh tr·ªçng s·ªë w_h d·ª±a tr√™n t·∫ßn su·∫•t xu·∫•t hi·ªán trong l·ªãch s·ª≠.\n",
    "        \n",
    "        Args:\n",
    "            history_relation_ids: (Batch, Max_History_Len) - Ch·ª©a relation ID trong qu√° kh·ª© c·ªßa user.\n",
    "            \n",
    "        Returns:\n",
    "            weights: (Batch, Num_Clusters) - Vector W_hu\n",
    "        \"\"\"\n",
    "        # 1. So kh·ªõp History v·ªõi Cluster IDs\n",
    "        # history: (B, H, 1)\n",
    "        # clusters: (1, 1, L)\n",
    "        # matches: (B, H, L) -> True n·∫øu relation t·∫°i history kh·ªõp v·ªõi cluster ID\n",
    "        hist_expanded = history_relation_ids.unsqueeze(-1)\n",
    "        clusters_expanded = self.cluster_ids.view(1, 1, -1)\n",
    "        \n",
    "        matches = (hist_expanded == clusters_expanded).float()\n",
    "        \n",
    "        # 2. ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán (T·ª≠ s·ªë Eq 13)\n",
    "        # Sum theo chi·ªÅu History (dim=1) -> (B, L)\n",
    "        counts = matches.sum(dim=1) \n",
    "        \n",
    "        # 3. T√≠nh ƒë·ªô d√†i q th·ª±c t·∫ø (M·∫´u s·ªë Eq 13)\n",
    "        # q = t·ªïng s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa b·∫•t k·ª≥ cluster n√†o trong history (tr√°nh t√≠nh padding 0)\n",
    "        q = counts.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # 4. Normalize ƒë·ªÉ ra tr·ªçng s·ªë (tr√°nh chia cho 0)\n",
    "        weights = counts / (q + 1e-9)\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    def forward(self, user_ids, item_ids, history_relation_ids):\n",
    "        \"\"\"\n",
    "        T√≠nh Reward Score g_R(v | u)\n",
    "        \n",
    "        Args:\n",
    "            user_ids: (Batch,)\n",
    "            item_ids: (Batch,) - Item ƒë√≠ch (v_hat) m√† Agent d·ª± ƒëo√°n/d·ª´ng l·∫°i\n",
    "            history_relation_ids: (Batch, History_Len) - L·ªãch s·ª≠ relation c·ªßa user\n",
    "            \n",
    "        Returns:\n",
    "            scores: (Batch,) - ƒêi·ªÉm reward\n",
    "        \"\"\"\n",
    "        # --- B∆Ø·ªöC 1: L·∫•y Embeddings c∆° b·∫£n ---\n",
    "        u_e = self.user_embs(user_ids)       # (B, Dim) -> e_u\n",
    "        v_e = self.entity_embs(item_ids)     # (B, Dim) -> e_v\n",
    "        v_b = self.bias_embs(item_ids).squeeze(-1) # (B,) -> b_v\n",
    "        \n",
    "        # --- B∆Ø·ªöC 2: T√≠nh Personalized Interaction Relation (Eq 12) ---\n",
    "        # r_vu^T = W_hu * V_U\n",
    "        \n",
    "        # a. T√≠nh weights (B, L)\n",
    "        weights = self.calculate_weights(history_relation_ids)\n",
    "        \n",
    "        # b. L·∫•y embedding c·ªßa c√°c cluster V_U^1...L\n",
    "        # cluster_embs shape: (L, Dim)\n",
    "        cluster_embs = self.relation_embs(self.cluster_ids)\n",
    "        \n",
    "        # c. T√≠nh t·ªïng c√≥ tr·ªçng s·ªë\n",
    "        # (B, L) x (L, Dim) -> (B, Dim)\n",
    "        r_interaction = torch.matmul(weights, cluster_embs)\n",
    "        \n",
    "        # --- B∆Ø·ªöC 3: T√≠nh Score (Eq 11 & Final Eq) ---\n",
    "        # g = (e_u + r_interaction) . e_v + b_v\n",
    "        \n",
    "        # Dot product: (e_u + r) * e_v\n",
    "        query_vector = u_e + r_interaction # (B, Dim)\n",
    "        dot_product = torch.sum(query_vector * v_e, dim=1) # (B,)\n",
    "        \n",
    "        scores = dot_product + v_b # C·ªông bias\n",
    "        \n",
    "        # 1. Scale Score (Chia cho nhi·ªát ƒë·ªô)\n",
    "        scaled_score = scores / self.temperature\n",
    "        \n",
    "        # 2. √Åp d·ª•ng Sigmoid\n",
    "        rewards = torch.sigmoid(scaled_score)\n",
    "        \n",
    "        return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "911afabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPRecEnvironment(nn.Module):\n",
    "    def __init__(self, kg, entity_embeddings, relation_embeddings, reward_function, max_path_len=3, history_len=3):\n",
    "        \"\"\"\n",
    "        Th√™m tham s·ªë: reward_function\n",
    "        \"\"\"\n",
    "        super(TPRecEnvironment, self).__init__()\n",
    "        self.kg = kg\n",
    "        self.entity_embs = entity_embeddings\n",
    "        self.relation_embs = relation_embeddings\n",
    "        self.max_path_len = max_path_len\n",
    "        self.history_len = history_len\n",
    "        \n",
    "        # L∆ØU REWARD FUNCTION ƒê∆Ø·ª¢C TRUY·ªÄN V√ÄO\n",
    "        self.reward_function = reward_function \n",
    "\n",
    "        # State tracking\n",
    "        self.current_entities = None\n",
    "        self.current_users = None\n",
    "        self.path_history = None\n",
    "        self.step_counter = 0\n",
    "\n",
    "    def reset(self, user_ids):\n",
    "        \"\"\"\n",
    "        Initiate status s_0 = (u, u, ‚àÖ)\n",
    "        \"\"\"\n",
    "        batch_size = user_ids.size(0)\n",
    "\n",
    "        self.current_users = user_ids       # Multiple values because of batch_size\n",
    "        self.current_entities = user_ids    # Initiate status s_0 = (u, u, ‚àÖ)\n",
    "        \n",
    "        # History h_k: store (relation, entity)\n",
    "        # B·ªï sung device=user_ids.device ƒë·ªÉ tr√°nh l·ªói khi d√πng GPU\n",
    "        self.path_history = torch.zeros((batch_size, self.history_len * 2), \n",
    "                                        dtype=torch.long, \n",
    "                                        device=user_ids.device)\n",
    "\n",
    "        self.step_counter = 0        \n",
    "        return self._get_state_embedding()\n",
    "\n",
    "    def _get_state_embedding(self):\n",
    "        \"\"\"\n",
    "        State = [User_Emb, Flattened_History_Emb, Current_Entity_Emb]\n",
    "        Input cho Policy Network s·∫Ω l√† vector n·ªëi d√†i c·ªßa 3 th√†nh ph·∫ßn n√†y.\n",
    "        \"\"\"\n",
    "        u_emb = self.entity_embs(self.current_users)\n",
    "        e_emb = self.entity_embs(self.current_entities)\n",
    "        \n",
    "        # 2a: T√°ch index c·ªßa Relation v√† Entity b·∫±ng k·ªπ thu·∫≠t Slicing\n",
    "        r_indices = self.path_history[:, 0::2] # (Batch, history_len)\n",
    "        e_indices = self.path_history[:, 1::2] # (Batch, history_len)\n",
    "        \n",
    "        # 2b: Lookup Embedding\n",
    "        r_vecs = self.relation_embs(r_indices) \n",
    "        e_vecs = self.entity_embs(e_indices)\n",
    "        \n",
    "        # 2c: K·∫øt h·ª£p Relation v√† Entity t·∫°i m·ªói b∆∞·ªõc\n",
    "        step_vecs = torch.cat([r_vecs, e_vecs], dim=2)\n",
    "        \n",
    "        # 2d: L√†m ph·∫≥ng (Flatten) to√†n b·ªô l·ªãch s·ª≠\n",
    "        batch_size = step_vecs.size(0)\n",
    "        h_emb_flat = step_vecs.view(batch_size, -1)\n",
    "\n",
    "        # 3. K·∫æT H·ª¢P T·∫§T C·∫¢ (Concatenate)\n",
    "        # K·∫øt qu·∫£ c√≥ c·∫•u tr√∫c logic t∆∞∆°ng ƒë∆∞∆°ng: [User_Emb, r1, e1, r2, e2, r3, e3, Current_Entity_Emb]\n",
    "        state_vector = torch.cat([u_emb, h_emb_flat, e_emb], dim=1) \n",
    "        \n",
    "        return state_vector\n",
    "\n",
    "    def get_pruned_actions(self, epsilon=10):\n",
    "        \"\"\"\n",
    "        Th·ª±c hi·ªán Eq (8): Pruning function g_k((r, e_{k+1}) | u)\n",
    "        C·∫Øt t·ªâa kh√¥ng gian h√†nh ƒë·ªông, ch·ªâ gi·ªØ l·∫°i top-epsilon neighbors.\n",
    "        \"\"\"\n",
    "        batch_size = self.current_users.size(0)\n",
    "        device = self.current_users.device # L·∫•y device hi·ªán t·∫°i\n",
    "        valid_actions = []\n",
    "        \n",
    "        u_emb = self.entity_embs(self.current_users) # (B, dim)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            u_id = self.current_users[i].item()\n",
    "            curr_node = self.current_entities[i].item()\n",
    "            \n",
    "            neighbors = self.kg.get_neighbors(curr_node) \n",
    "            \n",
    "            if not neighbors:\n",
    "                valid_actions.append([]) # Dead end\n",
    "                continue\n",
    "                \n",
    "            # ƒê√£ b·ªï sung `device=device` ƒë·ªÉ tensor t·∫°o ra n·∫±m c√πng thi·∫øt b·ªã v·ªõi model\n",
    "            rels = torch.tensor([n[0] for n in neighbors], device=device)\n",
    "            next_nodes = torch.tensor([n[1] for n in neighbors], device=device)\n",
    "            \n",
    "            r_emb = self.relation_embs(rels)\n",
    "            next_node_emb = self.entity_embs(next_nodes)\n",
    "            \n",
    "            query = u_emb[i] + r_emb \n",
    "            scores = torch.sum(query * next_node_emb, dim=1) \n",
    "            \n",
    "            k = min(epsilon, len(scores))\n",
    "            top_scores, top_indices = torch.topk(scores, k)\n",
    "            \n",
    "            actions_i = []\n",
    "            for idx in top_indices:\n",
    "                actions_i.append((rels[idx].item(), next_nodes[idx].item())) # L·∫•y .item() l∆∞u v√†o list\n",
    "            valid_actions.append(actions_i)\n",
    "            \n",
    "        return valid_actions\n",
    "\n",
    "    def get_action_space_batch(self):\n",
    "        \"\"\"\n",
    "        [H√ÄM M·ªöI] L·∫•y kh√¥ng gian h√†nh ƒë·ªông cho c·∫£ Batch v√† Padding th√†nh Tensor.\n",
    "        \"\"\"\n",
    "        raw_actions_list = self.get_pruned_actions() \n",
    "        batch_size = len(raw_actions_list)\n",
    "        \n",
    "        lengths = [len(acts) for acts in raw_actions_list]\n",
    "        max_len = max(lengths) if lengths else 0\n",
    "        if max_len == 0:\n",
    "            max_len = 1\n",
    "            \n",
    "        device = self.current_entities.device\n",
    "        \n",
    "        r_indices = torch.zeros((batch_size, max_len), dtype=torch.long, device=device)\n",
    "        e_indices = torch.zeros((batch_size, max_len), dtype=torch.long, device=device)\n",
    "        action_mask = torch.zeros((batch_size, max_len), dtype=torch.float, device=device)\n",
    "        \n",
    "        for i, actions in enumerate(raw_actions_list):\n",
    "            num_acts = len(actions)\n",
    "            if num_acts > 0:\n",
    "                rs = [a[0] for a in actions]\n",
    "                es = [a[1] for a in actions]\n",
    "                \n",
    "                r_indices[i, :num_acts] = torch.tensor(rs, device=device)\n",
    "                e_indices[i, :num_acts] = torch.tensor(es, device=device)\n",
    "                action_mask[i, :num_acts] = 1.0\n",
    "                \n",
    "        r_emb = self.relation_embs(r_indices)\n",
    "        e_emb = self.entity_embs(e_indices)\n",
    "        \n",
    "        action_embs = torch.cat([r_emb, e_emb], dim=-1)\n",
    "        \n",
    "        return action_embs, action_mask, raw_actions_list\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Transition function (Eq 9): Chuy·ªÉn tr·∫°ng th√°i sang b∆∞·ªõc k+1\n",
    "        \"\"\"\n",
    "        device = self.current_entities.device\n",
    "        batch_size = len(actions)\n",
    "        \n",
    "        rels_list = [a[0] for a in actions]\n",
    "        ents_list = [a[1] for a in actions]\n",
    "        \n",
    "        next_relations = torch.tensor(rels_list, dtype=torch.long, device=device)\n",
    "        next_entities = torch.tensor(ents_list, dtype=torch.long, device=device)\n",
    "        \n",
    "        new_r = next_relations.unsqueeze(1)\n",
    "        new_e = next_entities.unsqueeze(1)\n",
    "        new_entry = torch.cat([new_r, new_e], dim=1)\n",
    "        \n",
    "        history_shifted = self.path_history[:, 2:]\n",
    "        self.path_history = torch.cat([history_shifted, new_entry], dim=1)\n",
    "        \n",
    "        self.current_entities = next_entities\n",
    "        self.step_counter += 1\n",
    "        done = (self.step_counter >= self.max_path_len)\n",
    "        \n",
    "        return self._get_state_embedding(), done\n",
    "\n",
    "    def step_with_indices(self, action_indices, raw_actions_list):\n",
    "        \"\"\"\n",
    "        [H√ÄM M·ªöI] Th·ª±c hi·ªán b∆∞·ªõc ƒëi d·ª±a tr√™n Index (0, 1, 2...) m√† Agent ch·ªçn.\n",
    "        \"\"\"\n",
    "        selected_real_actions = []\n",
    "        batch_size = len(action_indices)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            idx = action_indices[i].item()\n",
    "            user_acts = raw_actions_list[i]\n",
    "            \n",
    "            if len(user_acts) > 0:\n",
    "                idx = min(idx, len(user_acts) - 1) # B·∫£o v·ªá index out of bounds\n",
    "                real_action = user_acts[idx] \n",
    "            else:\n",
    "                curr_node = self.current_entities[i].item()\n",
    "                real_action = (0, curr_node) # Dead-end: T·ª± tr·ªè v·ªÅ ch√≠nh n√≥\n",
    "                \n",
    "            selected_real_actions.append(real_action)\n",
    "            \n",
    "        return self.step(selected_real_actions)\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"\n",
    "        G·ªçi khi done=True. T√≠nh Time-aware Reward g_R(v|u).\n",
    "        \"\"\"\n",
    "        user_ids = self.current_users\n",
    "        item_ids = self.current_entities\n",
    "        \n",
    "        # Ch·ªâ l·∫•y c·ªôt relation t·ª´ l·ªãch s·ª≠\n",
    "        history_relation_ids = self.path_history[:, 0::2] \n",
    "        \n",
    "        rewards = self.reward_function(user_ids, item_ids, history_relation_ids)\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7faeaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPRecPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, dropout=0.1):\n",
    "        \"\"\"\n",
    "        M·∫°ng Policy (Actor) s·ª≠ d·ª•ng MLP ƒë·ªÉ ch·∫•m ƒëi·ªÉm h√†nh ƒë·ªông.\n",
    "        \n",
    "        Args:\n",
    "            state_dim (int): K√≠ch th∆∞·ªõc vector tr·∫°ng th√°i s_k.\n",
    "                             = dim(u) + dim(flattened_history) + dim(e_k)\n",
    "            action_dim (int): K√≠ch th∆∞·ªõc vector h√†nh ƒë·ªông a_k.\n",
    "                              = dim(relation) + dim(next_node)\n",
    "            hidden_dim (int): S·ªë neuron l·ªõp ·∫©n.\n",
    "            dropout (float): T·ª∑ l·ªá dropout ƒë·ªÉ tr√°nh overfitting.\n",
    "        \"\"\"\n",
    "        super(TPRecPolicy, self).__init__()\n",
    "        \n",
    "        # Layer 1: K·∫øt h·ª£p State v√† Action\n",
    "        # Input size = State + Action\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        \n",
    "        # Activation function (ELU ho·∫∑c ReLU th∆∞·ªùng d√πng trong Graph RL)\n",
    "        self.act = nn.ELU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer 2: Output ra 1 ƒëi·ªÉm s·ªë (Scalar Score) cho c·∫∑p (State, Action)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state_emb, action_embs, action_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass ƒë·ªÉ t√≠nh x√°c su·∫•t h√†nh ƒë·ªông.\n",
    "        \n",
    "        Args:\n",
    "            state_emb: (Batch, State_Dim) - Vector tr·∫°ng th√°i s_k\n",
    "            action_embs: (Batch, Max_Actions, Action_Dim) - C√°c h√†nh ƒë·ªông ·ª©ng vi√™n\n",
    "            action_mask: (Batch, Max_Actions) - Mask (1=H·ª£p l·ªá, 0=Padding)\n",
    "            \n",
    "        Returns:\n",
    "            probs: (Batch, Max_Actions) - X√°c su·∫•t (ƒë√£ qua Softmax)\n",
    "            log_probs: (Batch, Max_Actions) - Log x√°c su·∫•t (ƒë·ªÉ t√≠nh Loss)\n",
    "        \"\"\"\n",
    "        batch_size, num_actions, _ = action_embs.size()\n",
    "        \n",
    "        # 1. M·ªü r·ªông State ƒë·ªÉ kh·ªõp v·ªõi s·ªë l∆∞·ª£ng Action (Broadcasting)\n",
    "        # state: (B, S_Dim) -> (B, 1, S_Dim) -> (B, K, S_Dim)\n",
    "        state_expanded = state_emb.unsqueeze(1).expand(-1, num_actions, -1)\n",
    "        \n",
    "        # 2. Gh√©p (Concatenate) State v√† Action\n",
    "        # input: (B, K, S_Dim + A_Dim)\n",
    "        inputs = torch.cat([state_expanded, action_embs], dim=2)\n",
    "        \n",
    "        # 3. Qua m·∫°ng MLP\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        scores = self.fc2(x).squeeze(-1) # (B, K, 1) -> (B, K)\n",
    "        \n",
    "        # 4. Masking (C·ª±c k·ª≥ quan tr·ªçng)\n",
    "        # G√°n ƒëi·ªÉm s·ªë r·∫•t th·∫•p (-1e9) cho c√°c h√†nh ƒë·ªông padding ƒë·ªÉ Softmax = 0\n",
    "        if action_mask is not None:\n",
    "            scores = scores.masked_fill(action_mask == 0, -1e9)\n",
    "        \n",
    "        # 5. T√≠nh x√°c su·∫•t (Softmax)\n",
    "        probs = F.softmax(scores, dim=1)\n",
    "        \n",
    "        # Tr·∫£ v·ªÅ c·∫£ log_probs ƒë·ªÉ ti·ªán t√≠nh REINFORCE Loss sau n√†y\n",
    "        log_probs = F.log_softmax(scores, dim=1)\n",
    "        \n",
    "        return probs, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f005c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86e4d82a",
   "metadata": {},
   "source": [
    "### Class Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bd0c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu\n",
    "    TCKG_PATH = f\"./data/{name}/{name}_TCKG.csv\"  # File CSV ch·ª©a ƒë·ªì th·ªã (nh∆∞ b·∫°n ƒë√£ g·ª≠i)\n",
    "    # TRAIN_USERS_PATH = \"data/train_users.csv\" # File ch·ª©a danh s√°ch user ID d√πng ƒë·ªÉ train\n",
    "    SAVE_DIR = \"./checkpoints\"\n",
    "    \n",
    "    # Si√™u tham s·ªë Model\n",
    "    EMBED_DIM = 64\n",
    "    HIDDEN_DIM = 256\n",
    "    HISTORY_LEN = 3   # k' (ƒë·ªô d√†i l·ªãch s·ª≠)\n",
    "    MAX_PATH_LEN = 3  # K (s·ªë b∆∞·ªõc ƒëi)\n",
    "    \n",
    "    # Si√™u tham s·ªë Training\n",
    "    BATCH_SIZE = 512 # Batch l·ªõn gi√∫p RL ·ªïn ƒë·ªãnh h∆°n\n",
    "    NUM_EPOCHS = 500\n",
    "    LEARNING_RATE = 5e-4\n",
    "    BETA_ENTROPY = 0.01\n",
    "    \n",
    "    # Thi·∫øt b·ªã\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # IDs c·ªßa c√°c Relation Interaction (Quan tr·ªçng cho Reward)\n",
    "    # V√≠ d·ª•: 20=interacted_0, 21=interacted_1, 22=interacted_2\n",
    "    INTERACTION_CLUSTER_IDS = [20, 21, 22, 23] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad71580",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c04d198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. T·∫°o class Dataset l√†m c·∫ßu n·ªëi\n",
    "class InteractionDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        \"\"\"Nh·∫≠n v√†o m·ªôt DataFrame v√† chuy·ªÉn ƒë·ªïi c√°c c·ªôt c·∫ßn thi·∫øt th√†nh m·∫£ng NumPy\"\"\"\n",
    "        self.users = df['user_id'].values\n",
    "        self.entities = df['entity_id'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Tr·∫£ v·ªÅ t·ªïng s·ªë d√≤ng d·ªØ li·ªáu\"\"\"\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"L·∫•y d·ªØ li·ªáu t·∫°i v·ªã tr√≠ idx v√† bi·∫øn th√†nh PyTorch Tensor\"\"\"\n",
    "        user_tensor = torch.tensor(self.users[idx], dtype=torch.long)\n",
    "        entity_tensor = torch.tensor(self.entities[idx], dtype=torch.long)\n",
    "        \n",
    "        return user_tensor\n",
    "\n",
    "# 2. ƒê·ªçc file CSV th√†nh Pandas DataFrame\n",
    "# (L∆∞u √Ω nh·ªè: M√¨nh gi·ªØ nguy√™n t√™n file c·ªßa b·∫°n, nh∆∞ng ch·ªØ 'interacions' h√¨nh nh∆∞ ƒëang thi·∫øu ch·ªØ 't', b·∫°n nh·ªõ ki·ªÉm tra l·∫°i t√™n file th·∫≠t nh√©)\n",
    "train_df = pd.read_csv(f'./data/{name}/{name}_train_interactions.csv')\n",
    "val_df = pd.read_csv(f'./data/{name}/{name}_val_interactions.csv')\n",
    "test_df = pd.read_csv(f'./data/{name}/{name}_test_interactions.csv')\n",
    "\n",
    "# 3. B·ªçc DataFrame v√†o class Dataset v·ª´a t·∫°o\n",
    "train_dataset = InteractionDataset(train_df)\n",
    "val_dataset = InteractionDataset(val_df)\n",
    "test_dataset = InteractionDataset(test_df)\n",
    "\n",
    "# 4. ƒê∆∞a Dataset v√†o DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12f9c9",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c700f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Starting TPRec Training on cpu...\n",
      "Loading Knowledge Graph...\n",
      "Loading TCKG from ./data/book/book_TCKG.csv...\n",
      "Inverse Relation Offset: 24. Total Relation Space: 49\n",
      "TCKG Loaded successfully. Graph construction complete.\n",
      "Loading Pre-trained TransE Embeddings...\n",
      "Setting up Reward Function...\n",
      "Setting up Environment...\n",
      "Building Policy Network...\n",
      "--> State Dim: 512 | Action Dim: 128\n",
      "üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán 500 epochs...\n",
      "T·ªïng s·ªë m·∫´u trong Dataset: 14683\n",
      "T·ªïng s·ªë Batch s·∫Ω ch·∫°y: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/500:   0%|          | 0/15 [00:10<?, ?it/s, Loss=3.3544, R=0.5030]\n",
      "Epoch 1/500:   0%|          | 0/15 [00:10<?, ?it/s, Loss=3.3544, R=0.5030]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 2/500:   0%|          | 0/15 [00:09<?, ?it/s, Loss=3.3780, R=0.5029]\n",
      "Epoch 3/500:   0%|          | 0/15 [00:08<?, ?it/s, Loss=3.3806, R=0.5032]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 257\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(\u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    255\u001b[39m     os.makedirs(\u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 241\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    229\u001b[39m policy_net = TPRecPolicy(\n\u001b[32m    230\u001b[39m     state_dim=state_dim,\n\u001b[32m    231\u001b[39m     action_dim=action_dim,\n\u001b[32m    232\u001b[39m     hidden_dim=cfg.HIDDEN_DIM\n\u001b[32m    233\u001b[39m )\n\u001b[32m    236\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# B∆Ø·ªöC 7: B·∫ÆT ƒê·∫¶U TRAINING\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# G·ªçi h√†m train monolithic (t·∫•t c·∫£ trong m·ªôt) b·∫°n ƒë√£ ch·ªçn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta_entropy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBETA_ENTROPY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env, policy_net, train_loader, num_epochs, learning_rate, beta_entropy, save_path, device)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# --- 4. V√íNG L·∫∂P STEP (Agent ƒëi t√¨m ƒë∆∞·ªùng) ---\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# ƒêi max_path_len b∆∞·ªõc (v√≠ d·ª•: 3 b∆∞·ªõc)\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env.max_path_len):\n\u001b[32m     62\u001b[39m \n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# B. L·∫•y h√†nh ƒë·ªông kh·∫£ thi (Pruned Action Space)\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# action_embs: (Batch, K, Dim)\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# mask: (Batch, K) - ƒê·ªÉ che padding\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;66;03m# raw_actions: List ƒë·ªÉ map l·∫°i ID th·ª±c t·∫ø\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     action_embs, action_mask, raw_actions = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_action_space_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# C. Policy Forward (Ra quy·∫øt ƒë·ªãnh)\u001b[39;00m\n\u001b[32m     70\u001b[39m     probs, log_probs = policy_net(state_emb, action_embs, action_mask)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36mTPRecEnvironment.get_action_space_batch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_action_space_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    111\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    [H√ÄM M·ªöI] L·∫•y kh√¥ng gian h√†nh ƒë·ªông cho c·∫£ Batch v√† Padding th√†nh Tensor.\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     raw_actions_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_pruned_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    115\u001b[39m     batch_size = \u001b[38;5;28mlen\u001b[39m(raw_actions_list)\n\u001b[32m    117\u001b[39m     lengths = [\u001b[38;5;28mlen\u001b[39m(acts) \u001b[38;5;28;01mfor\u001b[39;00m acts \u001b[38;5;129;01min\u001b[39;00m raw_actions_list]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mTPRecEnvironment.get_pruned_actions\u001b[39m\u001b[34m(self, epsilon)\u001b[39m\n\u001b[32m    103\u001b[39m     actions_i = []\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m top_indices:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m         actions_i.append((\u001b[43mrels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, next_nodes[idx].item())) \u001b[38;5;66;03m# L·∫•y .item() l∆∞u v√†o list\u001b[39;00m\n\u001b[32m    106\u001b[39m     valid_actions.append(actions_i)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m valid_actions\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train(env, policy_net, train_loader, num_epochs=50, \n",
    "          learning_rate=1e-3, beta_entropy=0.01, \n",
    "          save_path=\"./checkpoints\", device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    H√†m hu·∫•n luy·ªán g·ªôp (kh√¥ng d√πng h√†m ph·ª• tr·ª£ train_one_epoch).\n",
    "    \n",
    "    C·∫•u tr√∫c l·ªìng nhau:\n",
    "    1. Loop Epochs (H√†ng trƒÉm l·∫ßn)\n",
    "       2. Loop Batches (Duy·ªát qua to√†n b·ªô dataset)\n",
    "          3. Loop Steps (Agent ƒëi k b∆∞·ªõc t√¨m ƒë∆∞·ªùng)\n",
    "             -> T√≠nh Reward & Loss\n",
    "             -> Backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. SETUP ---\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    # ƒê∆∞a model v√† env v√†o thi·∫øt b·ªã (GPU/CPU)\n",
    "    policy_net.to(device)\n",
    "    # env.to(device) # N·∫øu class Env c·ªßa b·∫°n c√≥ h·ªó tr·ª£ .to()\n",
    "    \n",
    "    # Optimizer (Adam)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning Rate Scheduler (Gi·∫£m LR sau m·ªói 50 epoch)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
    "    \n",
    "    print(f\"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán {num_epochs} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"T·ªïng s·ªë m·∫´u trong Dataset: {len(train_loader.dataset)}\")\n",
    "    print(f\"T·ªïng s·ªë Batch s·∫Ω ch·∫°y: {len(train_loader)}\")\n",
    "    \n",
    "    # --- 2. V√íNG L·∫∂P EPOCH (V√≤ng ngo√†i c√πng) ---\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        \n",
    "        policy_net.train() # B·∫≠t ch·∫ø ƒë·ªô train (Dropout, Batchnorm...)\n",
    "        \n",
    "        total_epoch_loss = 0\n",
    "        total_epoch_reward = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Thanh ti·∫øn tr√¨nh cho t·ª´ng Epoch\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
    "        \n",
    "        # --- 3. V√íNG L·∫∂P BATCH (Duy·ªát Dataset) ---\n",
    "        for batch_users in train_loader:\n",
    "            batch_users = batch_users.to(device)\n",
    "            \n",
    "            # A. Reset m√¥i tr∆∞·ªùng cho batch m·ªõi\n",
    "            # state_emb: (Batch, State_Dim)\n",
    "            state_emb = env.reset(batch_users)\n",
    "            \n",
    "            # Danh s√°ch l∆∞u l·∫°i th√¥ng tin ƒë·ªÉ t√≠nh Loss sau khi ƒëi xong\n",
    "            saved_log_probs = [] # Log œÄ(a|s)\n",
    "            saved_entropies = [] # H(œÄ)\n",
    "            \n",
    "            # --- 4. V√íNG L·∫∂P STEP (Agent ƒëi t√¨m ƒë∆∞·ªùng) ---\n",
    "            # ƒêi max_path_len b∆∞·ªõc (v√≠ d·ª•: 3 b∆∞·ªõc)\n",
    "            for t in range(env.max_path_len):\n",
    "                \n",
    "                # B. L·∫•y h√†nh ƒë·ªông kh·∫£ thi (Pruned Action Space)\n",
    "                # action_embs: (Batch, K, Dim)\n",
    "                # mask: (Batch, K) - ƒê·ªÉ che padding\n",
    "                # raw_actions: List ƒë·ªÉ map l·∫°i ID th·ª±c t·∫ø\n",
    "                action_embs, action_mask, raw_actions = env.get_action_space_batch()\n",
    "                \n",
    "                # C. Policy Forward (Ra quy·∫øt ƒë·ªãnh)\n",
    "                probs, log_probs = policy_net(state_emb, action_embs, action_mask)\n",
    "                \n",
    "                # D. Ch·ªçn h√†nh ƒë·ªông (Sampling d·ª±a tr√™n x√°c su·∫•t)\n",
    "                m = torch.distributions.Categorical(probs)\n",
    "                action_indices = m.sample() # (Batch,)\n",
    "                \n",
    "                # E. L∆∞u Log Prob v√† Entropy\n",
    "                saved_log_probs.append(m.log_prob(action_indices))\n",
    "                saved_entropies.append(m.entropy())\n",
    "                \n",
    "                # F. Th·ª±c hi·ªán b∆∞·ªõc ƒëi (Transition)\n",
    "                # H√†m n√†y c·∫≠p nh·∫≠t v·ªã tr√≠ agent v√† l·ªãch s·ª≠\n",
    "                state_emb, done = env.step_with_indices(action_indices, raw_actions)\n",
    "            \n",
    "            # --- 5. T√çNH TO√ÅN LOSS & UPDATE (K·∫øt th√∫c 1 trajectory) ---\n",
    "            \n",
    "            # G. T√≠nh Reward cu·ªëi c√πng (Terminal Reward)\n",
    "            # rewards shape: (Batch,)\n",
    "            rewards = env.get_reward() \n",
    "            \n",
    "            # H. T√≠nh REINFORCE Loss\n",
    "            # Loss = - sum(log_prob * reward) - beta * entropy\n",
    "            batch_loss = 0\n",
    "            \n",
    "            # C·ªông d·ªìn loss qua c√°c b∆∞·ªõc th·ªùi gian (t=1..K)\n",
    "            # Gi·∫£ s·ª≠ Reward ·ªü b∆∞·ªõc cu·ªëi √°p d·ª•ng cho to√†n b·ªô chu·ªói h√†nh ƒë·ªông\n",
    "            for log_prob in saved_log_probs:\n",
    "                # D·∫•u tr·ª´ (-) ƒë·ªÉ chuy·ªÉn b√†i to√°n Maximize Reward th√†nh Minimize Loss\n",
    "                batch_loss += -log_prob * rewards\n",
    "                \n",
    "            # Tr·ª´ ƒëi Entropy (Khuy·∫øn kh√≠ch kh√°m ph√°)\n",
    "            for entropy in saved_entropies:\n",
    "                batch_loss -= beta_entropy * entropy\n",
    "                \n",
    "            # L·∫•y trung b√¨nh loss tr√™n to√†n b·ªô Batch users\n",
    "            loss = batch_loss.mean()\n",
    "            \n",
    "            # I. Backpropagation (Lan truy·ªÅn ng∆∞·ª£c)\n",
    "            optimizer.zero_grad() # X√≥a gradient c≈©\n",
    "            loss.backward()       # T√≠nh gradient m·ªõi\n",
    "            \n",
    "            # K·ªπ thu·∫≠t Gradient Clipping (Tr√°nh b√πng n·ªï gradient - R·∫•t quan tr·ªçng trong RL)\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()      # C·∫≠p nh·∫≠t tr·ªçng s·ªë\n",
    "            \n",
    "            # --- 6. LOGGING ---\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_reward += rewards.mean().item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh (Hi·ªÉn th·ªã loss/reward hi·ªán t·∫°i)\n",
    "            pbar.set_postfix({'Loss': f\"{loss.item():.4f}\", 'R': f\"{rewards.mean().item():.4f}\"})\n",
    "            \n",
    "        # --- K·∫æT TH√öC 1 EPOCH ---\n",
    "        scheduler.step() # C·∫≠p nh·∫≠t Learning Rate\n",
    "        \n",
    "        avg_loss = total_epoch_loss / num_batches\n",
    "        avg_reward = total_epoch_reward / num_batches\n",
    "        \n",
    "        # In k·∫øt qu·∫£ Epoch\n",
    "        # print(f\"\\n[K·∫øt th√∫c Epoch {epoch}] Avg Loss: {avg_loss:.4f} | Avg Reward: {avg_reward:.4f}\")\n",
    "        \n",
    "        # L∆∞u Checkpoint m·ªói 10 Epoch\n",
    "        if epoch % 10 == 0:\n",
    "            ckpt_path = os.path.join(save_path, f\"model_epoch_{epoch}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': policy_net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, ckpt_path)\n",
    "            # print(f\"üíæ ƒê√£ l∆∞u model t·∫°i {ckpt_path}\")\n",
    "\n",
    "    print(f\"‚úÖ Ho√†n th√†nh training sau {(time.time() - start_time):.0f} gi√¢y.\")\n",
    "\n",
    "def main():\n",
    "    cfg = Config()\n",
    "    print(f\"üî• Starting TPRec Training on {cfg.DEVICE}...\")\n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # B∆Ø·ªöC 1: X√¢y d·ª±ng ƒê·ªì th·ªã Tri th·ª©c (TCKG)\n",
    "    # -------------------------------------------------\n",
    "    # L∆∞u √Ω: C·∫ßn ƒë·∫£m b·∫£o file CSV ƒë√£ ƒë∆∞·ª£c map ID v·ªÅ d·∫°ng s·ªë nguy√™n li√™n t·ª•c (0, 1, 2...)\n",
    "    print(\"Loading Knowledge Graph...\")\n",
    "    # T√≠nh offset t·ª± ƒë·ªông (nh∆∞ class TCKG t·ªëi ∆∞u t√¥i ƒë√£ vi·∫øt)\n",
    "    tckg = TCKG(cfg.TCKG_PATH) \n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # B∆Ø·ªöC 2: Kh·ªüi t·∫°o Embeddings t·ª´ file Pickle\n",
    "    # -------------------------------------------------\n",
    "    print(\"Loading Pre-trained TransE Embeddings...\")\n",
    "    pickle_file_path = './pickle/book_transE_embeddings_2026-02-19_14-30-57.pkl'\n",
    "\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "        \n",
    "    pretrained_ent = saved_data['entity_embeddings']\n",
    "    pretrained_rel = saved_data['relation_embeddings']\n",
    "    \n",
    "    # 2. Chuy·ªÉn ƒë·ªïi sang PyTorch Tensor (√©p ki·ªÉu Float32 ƒë·ªÉ t√≠nh to√°n neural network)\n",
    "    # N·∫øu data ƒëang l√† Numpy array:\n",
    "    if isinstance(pretrained_ent, np.ndarray):\n",
    "        ent_tensor = torch.tensor(pretrained_ent, dtype=torch.float32)\n",
    "        rel_tensor = torch.tensor(pretrained_rel, dtype=torch.float32)\n",
    "    else:\n",
    "        # N·∫øu data ƒë√£ l√† Tensor s·∫µn:\n",
    "        ent_tensor = pretrained_ent.clone().detach().float()\n",
    "        rel_tensor = pretrained_rel.clone().detach().float()\n",
    "        \n",
    "    # 3. N·∫°p v√†o nn.Embedding\n",
    "    # freeze=False: Cho ph√©p RL Agent ti·∫øp t·ª•c c·∫≠p nh·∫≠t (fine-tune) vector trong l√∫c t√¨m ƒë∆∞·ªùng\n",
    "    # freeze=True: Kh√≥a c·ª©ng vector, RL Agent ch·ªâ h·ªçc Policy Network (H·ªçc nhanh h∆°n, ch·ªëng overfit)\n",
    "    entity_embs = nn.Embedding.from_pretrained(ent_tensor, freeze=False)\n",
    "    relation_embs = nn.Embedding.from_pretrained(rel_tensor, freeze=False)\n",
    "\n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # B∆Ø·ªöC 3: Kh·ªüi t·∫°o Reward Function\n",
    "    # -------------------------------------------------\n",
    "    print(\"Setting up Reward Function...\")\n",
    "    reward_func = TimeAwareRewardFunction(\n",
    "        user_embs=entity_embs,    # Chia s·∫ª tr·ªçng s·ªë v·ªõi Env\n",
    "        entity_embs=entity_embs,\n",
    "        relation_embs=relation_embs,\n",
    "        interaction_cluster_ids=cfg.INTERACTION_CLUSTER_IDS,\n",
    "        bias_embs=None, # T·ª± t·∫°o bias m·ªõi\n",
    "        temperature= 2.0 \n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # B∆Ø·ªöC 4: Kh·ªüi t·∫°o M√¥i tr∆∞·ªùng (Environment)\n",
    "    # -------------------------------------------------\n",
    "    print(\"Setting up Environment...\")\n",
    "    env = TPRecEnvironment(\n",
    "        kg=tckg,\n",
    "        entity_embeddings=entity_embs,\n",
    "        relation_embeddings=relation_embs,\n",
    "        reward_function=reward_func, # Inject reward v√†o env\n",
    "        max_path_len=cfg.MAX_PATH_LEN,\n",
    "        history_len=cfg.HISTORY_LEN\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # B∆Ø·ªöC 5: Kh·ªüi t·∫°o Policy Network (Agent)\n",
    "    # -------------------------------------------------\n",
    "    print(\"Building Policy Network...\")\n",
    "    \n",
    "    # T√≠nh to√°n k√≠ch th∆∞·ªõc State v√† Action theo c√¥ng th·ª©c chu·∫©n\n",
    "    # Action = Relation + Entity\n",
    "    action_dim = cfg.EMBED_DIM + cfg.EMBED_DIM \n",
    "    \n",
    "    # State = User + Flattened_History + Current_Entity\n",
    "    # Flattened_History = k' * (Relation + Entity)\n",
    "    history_flat_dim = cfg.HISTORY_LEN * (cfg.EMBED_DIM + cfg.EMBED_DIM)\n",
    "    state_dim = cfg.EMBED_DIM + history_flat_dim + cfg.EMBED_DIM\n",
    "    \n",
    "    print(f\"--> State Dim: {state_dim} | Action Dim: {action_dim}\")\n",
    "    \n",
    "    policy_net = TPRecPolicy(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=cfg.HIDDEN_DIM\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # B∆Ø·ªöC 7: B·∫ÆT ƒê·∫¶U TRAINING\n",
    "    # -------------------------------------------------\n",
    "    # G·ªçi h√†m train monolithic (t·∫•t c·∫£ trong m·ªôt) b·∫°n ƒë√£ ch·ªçn\n",
    "\n",
    "    train(\n",
    "        env=env,\n",
    "        policy_net=policy_net,\n",
    "        train_loader=train_loader,\n",
    "        num_epochs=cfg.NUM_EPOCHS,\n",
    "        learning_rate=cfg.LEARNING_RATE,\n",
    "        beta_entropy=cfg.BETA_ENTROPY,\n",
    "        save_path=cfg.SAVE_DIR,\n",
    "        device=cfg.DEVICE\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # T·∫°o th∆∞ m·ª•c data gi·∫£ l·∫≠p n·∫øu ch∆∞a c√≥ ƒë·ªÉ test logic (T√πy ch·ªçn)\n",
    "    if not os.path.exists(\"checkpoints\"):\n",
    "        os.makedirs(\"checkpoints\")\n",
    "        \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
