{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36101a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, os\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Set environment variables for reproducibility and safety\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 1. Configuration & Seeding\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'book' \n",
    "INTERACTION_PATH = f'./data/{NAME}_processed_interactions.csv'\n",
    "GRAPH_PATH = f'./data/{NAME}_processed_graph.csv'\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "LR = 0.001\n",
    "MAX_STEPS = 3\n",
    "GAMMA = 0.99 # Discount factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b86610",
   "metadata": {},
   "source": [
    "### 1.1 Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f091523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data():\n",
    "    print(\">>> [Step 1] Loading and Splitting Data...\")\n",
    "    \n",
    "    # 1. Load Static KG (Tri thức nền)\n",
    "    static_kg = pd.read_csv(GRAPH_PATH)\n",
    "    print(f\"Loaded Static KG: {len(static_kg)} edges\")\n",
    "    \n",
    "    # 2. Load Interactions (Hành vi người dùng)\n",
    "    interactions = pd.read_csv(INTERACTION_PATH)\n",
    "    print(f\"Loaded Interactions: {len(interactions)} rows\")\n",
    "    # Xử lý thời gian\n",
    "    interactions['timestamp'] = pd.to_datetime(interactions['timestamp'])\n",
    "    interactions = interactions.sort_values(by=['user_id', 'timestamp'])\n",
    "    # Tạo relation_id giả định cho tương tác (nếu chưa có)\n",
    "    # Ví dụ: gán toàn bộ tương tác là loại quan hệ có ID = -1 hoặc một ID đặc biệt\n",
    "    # Ở đây ta chỉ cần nó khác với relation_id trong Static KG\n",
    "    max_kg_relation = static_kg['relation_id'].max()\n",
    "    INTERACT_REL_ID = max_kg_relation + 1\n",
    "    interactions['relation_id'] = INTERACT_REL_ID \n",
    "    \n",
    "    # Đổi tên cột cho khớp logic đồ thị: user -> head, item -> tail\n",
    "    interactions = interactions.rename(columns={'user_id': 'head_id', 'entity_id': 'tail_id'})\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    # 3. Chia tập theo User (Chronological)\n",
    "    grouped = interactions.groupby('head_id')\n",
    "    \n",
    "    for user, group in tqdm(grouped, desc=\"Splitting per user\"):\n",
    "        n = len(group)\n",
    "        if n < 3: \n",
    "            train_data.append(group)\n",
    "            continue\n",
    "            \n",
    "        train_end = int(n * 0.8)\n",
    "        val_end = int(n * 0.9)\n",
    "        \n",
    "        train_data.append(group.iloc[:train_end])\n",
    "        val_data.append(group.iloc[train_end:val_end])\n",
    "        test_data.append(group.iloc[val_end:])\n",
    "    train_df = pd.concat(train_data)\n",
    "    val_df = pd.concat(val_data)\n",
    "    test_df = pd.concat(test_data)\n",
    "    \n",
    "    print(f\"Train/Val/Test sizes: {len(train_df)} / {len(val_df)} / {len(test_df)}\")\n",
    "    return static_kg, train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8bd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGEnvironment:\n",
    "    def __init__(self, static_kg, train_interactions, embedding_dim):\n",
    "        with open('../04. RL/pickle/book_transE_embeddings_2026-02-14_16-43-39.pkl', 'rb') as f:\n",
    "            emb_data = pickle.load(f)\n",
    "        # Load pretrained weights\n",
    "        # Lưu ý: Cần map đúng index từ emb_data sang index của graph hiện tại\n",
    "        # (Ở đây giả sử index khớp hoặc bạn cần viết hàm map lại)\n",
    "        pretrained_emb = torch.tensor(emb_data['entity_embeddings'])\n",
    "        self.node_embeds = nn.Embedding.from_pretrained(pretrained_emb, freeze=False) \n",
    "\n",
    "        self.graph = nx.MultiDiGraph()\n",
    "        \n",
    "        # Build Graph\n",
    "        all_edges = pd.concat([\n",
    "            static_kg[['head_id', 'relation_id', 'tail_id']], \n",
    "            train_interactions[['head_id', 'relation_id', 'tail_id']]\n",
    "        ])\n",
    "        \n",
    "        for _, row in tqdm(all_edges.iterrows(), total=len(all_edges), desc=\"Building Graph\"):\n",
    "            self.graph.add_edge(row['head_id'], row['tail_id'], relation=row['relation_id'])\n",
    "            \n",
    "        self.nodes = list(self.graph.nodes())\n",
    "        \n",
    "        # --- Khởi tạo Embedding ngẫu nhiên cho Node & Relation ---\n",
    "        # (Trong thực tế nên dùng Pre-trained TransE/Bert để tốt hơn)\n",
    "        self.max_node_id = max(self.nodes)\n",
    "        self.max_rel_id = all_edges['relation_id'].max()\n",
    "        \n",
    "        # print(\"Initializing Embeddings...\")\n",
    "        # self.node_embeds = nn.Embedding(self.max_node_id + 1000, embedding_dim) # +1000 buffer\n",
    "        self.rel_embeds = nn.Embedding(self.max_rel_id + 1000, embedding_dim)\n",
    "        \n",
    "    def get_state(self, node_id, hop_k):\n",
    "        # Trạng thái hiện tại đơn giản là Embedding của Node đó (có thể concat thêm hop)\n",
    "        # Convert node_id to tensor\n",
    "        node_idx = torch.tensor([node_id], dtype=torch.long)\n",
    "        return self.node_embeds(node_idx) \n",
    "        \n",
    "    def get_valid_actions(self, curr_node):\n",
    "        if not self.graph.has_node(curr_node):\n",
    "            return [], [], []\n",
    "        \n",
    "        neighbors = []\n",
    "        relations = []\n",
    "        next_node_ids = []\n",
    "        \n",
    "        for u, v, attr in self.graph.out_edges(curr_node, data=True):\n",
    "            neighbors.append(v)\n",
    "            relations.append(attr['relation'])\n",
    "            next_node_ids.append(v)\n",
    "            \n",
    "        return neighbors, relations, next_node_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef37e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. POLICY NETWORK (The Brain)\n",
    "# ==========================================\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # Input: [State_Emb; Relation_Emb; Next_Node_Emb]\n",
    "        # Output: Score scalar\n",
    "        self.fc1 = nn.Linear(state_dim * 3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1) # Điểm số cho hành động\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, state_emb, action_embs):\n",
    "        # state_emb: (1, dim)\n",
    "        # action_embs (Neighbors): (num_neighbors, dim * 2) gồm [Rel; Next_Node]\n",
    "        \n",
    "        # Expand state to match neighbors count\n",
    "        num_actions = action_embs.shape[0]\n",
    "        curr_state = state_emb.repeat(num_actions, 1) # (num_neighbors, dim)\n",
    "        \n",
    "        # Concat: [Current State, Relation, Next Node]\n",
    "        x = torch.cat([curr_state, action_embs], dim=1) # (num_neighbors, dim*3)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        scores = self.fc2(x) # (num_neighbors, 1)\n",
    "        \n",
    "        # Softmax để ra xác suất chọn từng neighbor\n",
    "        probs = F.softmax(scores.view(-1), dim=0)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. TRAINING LOOP (REINFORCE)\n",
    "# ==========================================\n",
    "def train(env, policy_net, train_df, episodes=50):\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "    \n",
    "    print(\"\\n>>> Start Training...\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Sample batch user để train cho nhanh (Stochastic)\n",
    "        batch_samples = train_df.sample(n=256) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = 0\n",
    "        \n",
    "        for _, row in batch_samples.iterrows():\n",
    "            curr_node = row['head_id']\n",
    "            target_node = row['tail_id']\n",
    "            \n",
    "            episode_log_probs = []\n",
    "            rewards = []\n",
    "            \n",
    "            # --- Walking ---\n",
    "            done = False\n",
    "            for step in range(MAX_STEPS):\n",
    "                # 1. Get State\n",
    "                state = env.get_state(curr_node, step) # (1, dim)\n",
    "                \n",
    "                # 2. Get Valid Actions (Neighbors)\n",
    "                neighbors, rels, neighbor_ids = env.get_valid_actions(curr_node)\n",
    "                \n",
    "                if len(neighbors) == 0:\n",
    "                    break # Dead end\n",
    "                \n",
    "                # 3. Tạo Embedding cho Actions để đưa vào mạng\n",
    "                neighbor_tensor = torch.tensor(neighbor_ids, dtype=torch.long)\n",
    "                rel_tensor = torch.tensor(rels, dtype=torch.long)\n",
    "                \n",
    "                neigh_embeds = env.node_embeds(neighbor_tensor)\n",
    "                rel_embeds = env.rel_embeds(rel_tensor)\n",
    "                action_features = torch.cat([rel_embeds, neigh_embeds], dim=1) # (num_neighbors, dim*2)\n",
    "                \n",
    "                # 4. Agent chọn hành động\n",
    "                probs = policy_net(state, action_features)\n",
    "                \n",
    "                # Sampling action dựa trên xác suất (Exploration)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                action_idx = dist.sample()\n",
    "                log_prob = dist.log_prob(action_idx)\n",
    "                \n",
    "                episode_log_probs.append(log_prob)\n",
    "                \n",
    "                # 5. Execute Action\n",
    "                next_node = neighbors[action_idx.item()]\n",
    "                curr_node = next_node\n",
    "                \n",
    "                # 6. Check Reward\n",
    "                if curr_node == target_node:\n",
    "                    rewards.append(1.0) # Tìm thấy!\n",
    "                    done = True\n",
    "                    break\n",
    "                else:\n",
    "                    rewards.append(0.0) # Chưa thấy\n",
    "            \n",
    "            # --- Tính Loss (Policy Gradient) ---\n",
    "            # Nếu tìm thấy đích ở bước cuối, phần thưởng lan truyền ngược lại\n",
    "            # Discounted Return\n",
    "            R = 0\n",
    "            returns = []\n",
    "            for r in reversed(rewards):\n",
    "                R = r + GAMMA * R\n",
    "                returns.insert(0, R)\n",
    "            \n",
    "            if done: # Chỉ học nếu tìm thấy đích (hoặc có thể phạt nhẹ nếu không tìm thấy)\n",
    "                total_reward += 1\n",
    "                for log_prob, R in zip(episode_log_probs, returns):\n",
    "                    batch_loss -= log_prob * R # Gradient Ascent -> Minimize Negative Reward\n",
    "        \n",
    "        # Update Weights\n",
    "        if batch_loss != 0:\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (episode+1) % 5 == 0:\n",
    "            print(f\"Episode {episode+1}/{episodes} | Hit Success: {total_reward}/256 | Batch Loss: {batch_loss:.4f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ee08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy_net, test_df, top_k=10):\n",
    "    print(\"\\n>>> Start Evaluation...\")\n",
    "    policy_net.eval() # Chuyển sang chế độ đánh giá (tắt dropout nếu có)\n",
    "    \n",
    "    hits = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Lấy mẫu ngẫu nhiên từ Test set để chạy cho nhanh (hoặc chạy hết nếu muốn chính xác)\n",
    "    test_samples = test_df.sample(n=100) if len(test_df) > 100 else test_df\n",
    "    \n",
    "    with torch.no_grad(): # Tắt tính toán gradient để tiết kiệm bộ nhớ\n",
    "        for _, row in tqdm(test_samples.iterrows(), total=len(test_samples)):\n",
    "            curr_node = row['head_id']\n",
    "            target_true = row['tail_id']\n",
    "            \n",
    "            # --- Beam Search (Giản lược: Greedy Best-First) ---\n",
    "            # Để đơn giản, ta cho Agent chọn top-1 đường đi tốt nhất\n",
    "            \n",
    "            for step in range(MAX_STEPS):\n",
    "                state = env.get_state(curr_node, step)\n",
    "                neighbors, rels, neighbor_ids = env.get_valid_actions(curr_node)\n",
    "                \n",
    "                if not neighbors: break \n",
    "                \n",
    "                # Chuẩn bị input cho mạng\n",
    "                neighbor_tensor = torch.tensor(neighbor_ids, dtype=torch.long)\n",
    "                rel_tensor = torch.tensor(rels, dtype=torch.long)\n",
    "                neigh_embeds = env.node_embeds(neighbor_tensor)\n",
    "                rel_embeds = env.rel_embeds(rel_tensor)\n",
    "                action_features = torch.cat([rel_embeds, neigh_embeds], dim=1)\n",
    "                \n",
    "                # Dự đoán xác suất\n",
    "                probs = policy_net(state, action_features)\n",
    "                \n",
    "                # --- KHÁC BIỆT: Chọn bước đi tốt nhất (Argmax) ---\n",
    "                # Thay vì random sample, ta chọn nước đi có xác suất cao nhất\n",
    "                best_action_idx = torch.argmax(probs).item()\n",
    "                \n",
    "                next_node = neighbors[best_action_idx]\n",
    "                curr_node = next_node\n",
    "                \n",
    "                # Kiểm tra xem node hiện tại có phải là đích không\n",
    "                if curr_node == target_true:\n",
    "                    hits += 1\n",
    "                    break\n",
    "            \n",
    "            total_samples += 1\n",
    "            \n",
    "    # Tính HR@1 (Hit Rate) - Tỉ lệ tìm thấy chính xác Item\n",
    "    acc = hits / total_samples if total_samples > 0 else 0\n",
    "    print(f\"Evaluation Rank 1 (Exact Match): {acc:.4f}\")\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e703784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# MAIN\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Data\n",
    "    static_kg, train_df, val_df, test_df = load_and_split_data()\n",
    "    \n",
    "    # 2. Init Env (Cần Embedding cho Node/Rel)\n",
    "    env = KGEnvironment(static_kg, train_df, embedding_dim=EMBEDDING_DIM)\n",
    "    \n",
    "    # 3. Init Agent\n",
    "    policy_net = PolicyNetwork(state_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM)\n",
    "    \n",
    "    # 4. Train\n",
    "    train(env, policy_net, train_df, episodes=50000) # Tăng episodes để thấy reward tăng\n",
    "\n",
    "    evaluate(env, policy_net, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
