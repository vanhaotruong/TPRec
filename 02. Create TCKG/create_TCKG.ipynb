{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d8b26601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math, os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "00d1c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'book'\n",
    "N_CLUSTERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b1dbe",
   "metadata": {},
   "source": [
    "### 1. CREATE TEMPORAL STAT & STRUCTURE FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "420f9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalFeatureCal:\n",
    "    def __init__(self, interaction_df, n_clusters):\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "        self.interaction_df = interaction_df\n",
    "        self.interaction_df['timestamp'] = pd.to_datetime(self.interaction_df['timestamp'])\n",
    "        self.interaction_df = self.interaction_df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "        # train_interaction_df['year'] = train_interaction_df['timestamp'].dt.year\n",
    "        self.interaction_df['month'] = self.interaction_df['timestamp'].dt.month\n",
    "        self.interaction_df['week'] = self.interaction_df['timestamp'].dt.isocalendar().week\n",
    "\n",
    "        # Simple season mapping (1:Winter, 2:Spring, 3:Summer, 4:Fall)\n",
    "        self.interaction_df['season'] = self.interaction_df['month'].apply(lambda x: (x%12 + 3)//3)\n",
    "    \n",
    "\n",
    "    ################ Creating Temporal Statistical Features ################\n",
    "    def _encode_cyclic(self, data, max_val):\n",
    "        data_norm = 2 * np.pi * data / max_val\n",
    "        return np.sin(data_norm), np.cos(data_norm)\n",
    "    \n",
    "    def create_temporal_stat_features(self):\n",
    "        self.interaction_df['month_sin'], self.interaction_df['month_cos'] = self._encode_cyclic(self.interaction_df['month'], 12)\n",
    "        self.interaction_df['week_sin'], self.interaction_df['week_cos'] = self._encode_cyclic(self.interaction_df['week'], 52)\n",
    "        self.interaction_df['season_sin'], self.interaction_df['season_cos'] = self._encode_cyclic(self.interaction_df['season'], 4)\n",
    "\n",
    "        feature_cols = [\n",
    "            'season_sin', 'season_cos',\n",
    "            'month_sin', 'month_cos', \n",
    "            'week_sin', 'week_cos',\n",
    "        ]\n",
    "\n",
    "        f_stat = self.interaction_df[['timestamp'] + feature_cols] # dataframe\n",
    "        f_stat = f_stat.drop_duplicates()\n",
    "\n",
    "        return f_stat\n",
    "    ################ Creating Temporal Statistical Features ################\n",
    "    \n",
    "\n",
    "    ################ Creating Temporal Structure Features ################\n",
    "    def _calculate_structural_features(self, df, interaction_count_col='interaction_count', \n",
    "                                gaps=[90, 30, 7, 1]):\n",
    "        \"\"\"\n",
    "        Calculates 1st and 2nd order structural features (z' and z'') for specified gaps.\n",
    "        print(f_stat.tail(1))\n",
    "        Parameters:\n",
    "        - df: DataFrame containing time-series data.\n",
    "        - interaction_count_col: Name of the column containing interaction counts (z(i)).\n",
    "        - gaps: List of window sizes (default: [90, 30, 7, 1] for season, month, week, day).\n",
    "        \n",
    "        Returns:\n",
    "        - DataFrame with new columns for each gap (z_prime and z_double_prime).\n",
    "        \"\"\"\n",
    "        \n",
    "        # We work on a copy to avoid Modifying the original dataframe\n",
    "        result_df = df.copy()\n",
    "        result_df = result_df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # Extract the base interaction series z(i)\n",
    "        z = result_df[interaction_count_col]\n",
    "        \n",
    "        feature_list = []\n",
    "        \n",
    "        for gap in gaps:\n",
    "            # --- 1. First-Order Structural Feature: z'_{gap}(t) ---\n",
    "            # Formula: (Sum(current_window) - Sum(previous_window)) / gap\n",
    "            \n",
    "            # Calculate rolling sum for the current window [t-gap, t]\n",
    "            # This corresponds to Sum_{i=t-gap}^{t} z(i)\n",
    "            current_sum = z.rolling(window=gap, min_periods=gap).sum()\n",
    "            \n",
    "            # The previous window sum is just the current sum shifted by 'gap'\n",
    "            # This corresponds to Sum_{i=t-2gap}^{t-gap} z(i)\n",
    "            prev_sum = current_sum.shift(gap)\n",
    "            \n",
    "            # Calculate z'\n",
    "            z_prime = (current_sum - prev_sum) / gap\n",
    "            \n",
    "            # Rename for storage\n",
    "            z_prime_col_name = f'z_prime_{gap}'\n",
    "            result_df[z_prime_col_name] = z_prime\n",
    "            \n",
    "            # --- 2. Second-Order Structural Feature: z''_{gap}(t) ---\n",
    "            # Formula: (Sum(current_window_of_z') - Sum(previous_window_of_z')) / gap\n",
    "            \n",
    "            # Now we apply the same rolling logic to the z_prime series we just created\n",
    "            current_sum_prime = z_prime.rolling(window=gap, min_periods=gap).sum()\n",
    "            prev_sum_prime = current_sum_prime.shift(gap)\n",
    "\n",
    "            # Calculate z''\n",
    "            z_double_prime = (current_sum_prime - prev_sum_prime) / gap\n",
    "            \n",
    "            # Rename for storage\n",
    "            z_double_prime_col_name = f'z_double_prime_{gap}'\n",
    "            result_df[z_double_prime_col_name] = z_double_prime\n",
    "            \n",
    "            # --- 3. Padding (Handling Initial NaNs) ---\n",
    "            # The paper states: \"padding... with the nearest timestamp's temporal structural feature\"\n",
    "            # Because we used rolling windows, the beginning of the series will have NaNs.\n",
    "            # We use backfill (bfill) to propagate the first valid observation backwards.\n",
    "            result_df[z_prime_col_name] = result_df[z_prime_col_name].bfill()\n",
    "            result_df[z_double_prime_col_name] = result_df[z_double_prime_col_name].bfill()\n",
    "            \n",
    "            # Keep track of feature names for the final concatenation\n",
    "            feature_list.extend([z_prime_col_name, z_double_prime_col_name])\n",
    "\n",
    "        # Return only the extracted features (concatenated as per Eq 2)\n",
    "        return result_df[['timestamp'] + feature_list]\n",
    "    \n",
    "    def create_temporal_structure_features(self):\n",
    "        daily_interaction_count_df = self.interaction_df.groupby('timestamp').size().reset_index(name='interaction_count')\n",
    "        daily_interaction_count_df = daily_interaction_count_df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "        # Extract Features\n",
    "        f_stru = self._calculate_structural_features(daily_interaction_count_df, \n",
    "                                                gaps=[90, 30, 7, 1])    #dataframe\n",
    "        return f_stru\n",
    "    ################ Creating Temporal Structure Features ################\n",
    "\n",
    "    ################ Concat Temporal Stat & Structure Features ################\n",
    "    def create_temporal_features(self):\n",
    "        f_stat = self.create_temporal_stat_features()\n",
    "        f_stru = self.create_temporal_structure_features()\n",
    "\n",
    "        f_all = pd.merge(f_stat, f_stru, on='timestamp', how='inner')\n",
    "\n",
    "        return f_all\n",
    "    ################ Concat Temporal Stat & Structure Features ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e5cc2464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'season_sin', 'season_cos', 'month_sin', 'month_cos',\n",
      "       'week_sin', 'week_cos', 'z_prime_90', 'z_double_prime_90', 'z_prime_30',\n",
      "       'z_double_prime_30', 'z_prime_7', 'z_double_prime_7', 'z_prime_1',\n",
      "       'z_double_prime_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "TRAIN_INTERACTION_CSV= f'./data/{NAME}/{NAME}_train_interactions.csv'\n",
    "train_interaction_df = pd.read_csv(TRAIN_INTERACTION_CSV )\n",
    "train_temporal = TemporalFeatureCal(train_interaction_df, n_clusters= N_CLUSTERS)\n",
    "\n",
    "train_f_all = train_temporal.create_temporal_features()\n",
    "print(train_f_all.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61533421",
   "metadata": {},
   "source": [
    "### 2. APPLY GMM CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7eeac3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_f_all.shape: (5413, 20)\n",
      "train_f_all.columns: Index(['timestamp', 'season_sin', 'season_cos', 'month_sin', 'month_cos',\n",
      "       'week_sin', 'week_cos', 'z_prime_90', 'z_double_prime_90', 'z_prime_30',\n",
      "       'z_double_prime_30', 'z_prime_7', 'z_double_prime_7', 'z_prime_1',\n",
      "       'z_double_prime_1', 'cluster_label', 'prob_cluster_0', 'prob_cluster_1',\n",
      "       'prob_cluster_2', 'prob_cluster_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "############## Apply GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=N_CLUSTERS, random_state=42, n_init=10)\n",
    "\n",
    "train_f_all_feature_only = train_f_all.drop('timestamp', axis=1)\n",
    "train_f_all['cluster_label'] = gmm.fit_predict(train_f_all_feature_only)    # Return that cluster_label having highest probaililty\n",
    "\n",
    "W_matrix = gmm.predict_proba(train_f_all_feature_only)      # Returen probability of each clusters,\n",
    "for i in range(N_CLUSTERS):\n",
    "    train_f_all[f'prob_cluster_{i}'] = W_matrix[:, i]\n",
    "\n",
    "print(f'train_f_all.shape: {train_f_all.shape}')\n",
    "print(f'train_f_all.columns: {train_f_all.columns}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08be9fd",
   "metadata": {},
   "source": [
    "## 2. Create TCKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6b651475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCKG_COLUMN_NAMES: Index(['head_id', 'relation_id', 'tail_id', 'head_id:token',\n",
      "       'relation_id:token', 'tail_id:token'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "graph_df = pd.read_csv(f'./data/{NAME}/{NAME}_processed_static_graph.csv')\n",
    "\n",
    "temporal_train_interaction_df = pd.merge(train_interaction_df, \n",
    "                                        train_f_all[['timestamp', 'cluster_label']], \n",
    "                                        on='timestamp', how='inner')\n",
    "\n",
    "temporal_train_interaction_df = temporal_train_interaction_df.rename(columns={'user_id': 'head_id', \n",
    "                                            'entity_id': 'tail_id',\n",
    "                                            'user_id:token': 'head_id:token',\n",
    "                                            'entity_id:token': 'tail_id:token'})\n",
    "\n",
    "max_relation_id_in_graph = graph_df['relation_id'].max()\n",
    "\n",
    "temporal_train_interaction_df['relation_id'] = temporal_train_interaction_df['cluster_label'] \\\n",
    "                                                + max_relation_id_in_graph + 1  # new relation_id. +1 because relation_id starts at 1, not 0\n",
    "temporal_train_interaction_df['relation_id:token'] = 'interacted_' + temporal_train_interaction_df['cluster_label'].astype(str)  # new relation_id:token\n",
    "\n",
    "TCKG_COLUMN_NAMES = graph_df.columns\n",
    "print(f'TCKG_COLUMN_NAMES: {TCKG_COLUMN_NAMES}')\n",
    "temporal_train_interaction_df = temporal_train_interaction_df[TCKG_COLUMN_NAMES]\n",
    "                            \n",
    "TCKG_df = pd.concat([graph_df, temporal_train_interaction_df], ignore_index=True)\n",
    "TCKG_df = TCKG_df.sort_values(by=['relation_id', 'head_id', 'tail_id'])\n",
    "TCKG_df.to_csv(f'./data/{NAME}/{NAME}_TCKG.csv', index= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6b4a234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMN_NAMES = ['head_id', 'relation_id', 'tail_id',\n",
    "#                                 'head_id:token', 'relation_id:token', 'tail_id:token']\n",
    "# COLUMN_NAMES.append('cluster_label')\n",
    "# for i in range(N_CLUSTERS):\n",
    "#     COLUMN_NAMES.append(f'prob_cluster_{i}')\n",
    "\n",
    "# print(COLUMN_NAMES)\n",
    "\n",
    "# temporal_train_interaction_df = temporal_train_interaction_df[COLUMN_NAMES]\n",
    "# temporal_train_interaction_df.to_csv(f'./data/{NAME}/{NAME}_train_set.csv', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
