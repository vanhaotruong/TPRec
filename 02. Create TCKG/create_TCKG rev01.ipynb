{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d8b26601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math, os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "00d1c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'book'\n",
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class TemporalClusteringPipeline:\n",
    "    def __init__(self, interaction_csv_path, n_clusters, feature_cols):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.feature_cols = feature_cols\n",
    "        self.interaction_csv_path = interaction_csv_path\n",
    "\n",
    "        self.interaction_df = pd.read_csv(self.interaction_csv_path )\n",
    "        self.interaction_df['timestamp'] = pd.to_datetime(self.interaction_df['timestamp'])\n",
    "        self.interaction_df = self.interaction_df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "        # train_interaction_df['year'] = train_interaction_df['timestamp'].dt.year\n",
    "        self.interaction_df['month'] = self.interaction_df['timestamp'].dt.month\n",
    "        self.interaction_df['week'] = self.interaction_df['timestamp'].dt.isocalendar().week\n",
    "\n",
    "        # Simple season mapping (1:Winter, 2:Spring, 3:Summer, 4:Fall)\n",
    "        self.interaction_df['season'] = self.interaction_df['month'].apply(lambda x: (x%12 + 3)//3)\n",
    "    \n",
    "\n",
    "    ################ Creating Temporal Statistical Features ################\n",
    "    def _encode_cyclic(data, max_val):\n",
    "        data_norm = 2 * np.pi * data / max_val\n",
    "        return np.sin(data_norm), np.cos(data_norm)\n",
    "    \n",
    "    def create_temporal_stat_features(self):\n",
    "        self.interaction_df['month_sin'], self.interaction_df['month_cos'] = self._encode_cyclic(self.interaction_df['month'], 12)\n",
    "        self.interaction_df['week_sin'], self.interaction_df['week_cos'] = self._encode_cyclic(self.interaction_df['week'], 52)\n",
    "        self.interaction_df['season_sin'], self.interaction_df['season_cos'] = self._encode_cyclic(self.interaction_df['season'], 4)\n",
    "\n",
    "        feature_cols = [\n",
    "            'season_sin', 'season_cos',\n",
    "            'month_sin', 'month_cos', \n",
    "            'week_sin', 'week_cos',\n",
    "        ]\n",
    "\n",
    "        f_stat = self.interaction_df[['timestamp'] + feature_cols] # dataframe\n",
    "        f_stat = f_stat.drop_duplicates()\n",
    "\n",
    "        return f_stat\n",
    "    ################ Creating Temporal Statistical Features ################\n",
    "    \n",
    "\n",
    "    ################ Creating Temporal Structure Features ################\n",
    "    def _calculate_structural_features(df, interaction_count_col='interaction_count', \n",
    "                                gaps=[90, 30, 7, 1]):\n",
    "        \"\"\"\n",
    "        Calculates 1st and 2nd order structural features (z' and z'') for specified gaps.\n",
    "        print(f_stat.tail(1))\n",
    "        Parameters:\n",
    "        - df: DataFrame containing time-series data.\n",
    "        - interaction_count_col: Name of the column containing interaction counts (z(i)).\n",
    "        - gaps: List of window sizes (default: [90, 30, 7, 1] for season, month, week, day).\n",
    "        \n",
    "        Returns:\n",
    "        - DataFrame with new columns for each gap (z_prime and z_double_prime).\n",
    "        \"\"\"\n",
    "        \n",
    "        # We work on a copy to avoid Modifying the original dataframe\n",
    "        result_df = df.copy()\n",
    "        result_df = result_df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # Extract the base interaction series z(i)\n",
    "        z = result_df[interaction_count_col]\n",
    "        \n",
    "        feature_list = []\n",
    "        \n",
    "        for gap in gaps:\n",
    "            # --- 1. First-Order Structural Feature: z'_{gap}(t) ---\n",
    "            # Formula: (Sum(current_window) - Sum(previous_window)) / gap\n",
    "            \n",
    "            # Calculate rolling sum for the current window [t-gap, t]\n",
    "            # This corresponds to Sum_{i=t-gap}^{t} z(i)\n",
    "            current_sum = z.rolling(window=gap, min_periods=gap).sum()\n",
    "            \n",
    "            # The previous window sum is just the current sum shifted by 'gap'\n",
    "            # This corresponds to Sum_{i=t-2gap}^{t-gap} z(i)\n",
    "            prev_sum = current_sum.shift(gap)\n",
    "            \n",
    "            # Calculate z'\n",
    "            z_prime = (current_sum - prev_sum) / gap\n",
    "            \n",
    "            # Rename for storage\n",
    "            z_prime_col_name = f'z_prime_{gap}'\n",
    "            result_df[z_prime_col_name] = z_prime\n",
    "            \n",
    "            # --- 2. Second-Order Structural Feature: z''_{gap}(t) ---\n",
    "            # Formula: (Sum(current_window_of_z') - Sum(previous_window_of_z')) / gap\n",
    "            \n",
    "            # Now we apply the same rolling logic to the z_prime series we just created\n",
    "            current_sum_prime = z_prime.rolling(window=gap, min_periods=gap).sum()\n",
    "            prev_sum_prime = current_sum_prime.shift(gap)\n",
    "\n",
    "            # Calculate z''\n",
    "            z_double_prime = (current_sum_prime - prev_sum_prime) / gap\n",
    "            \n",
    "            # Rename for storage\n",
    "            z_double_prime_col_name = f'z_double_prime_{gap}'\n",
    "            result_df[z_double_prime_col_name] = z_double_prime\n",
    "            \n",
    "            # --- 3. Padding (Handling Initial NaNs) ---\n",
    "            # The paper states: \"padding... with the nearest timestamp's temporal structural feature\"\n",
    "            # Because we used rolling windows, the beginning of the series will have NaNs.\n",
    "            # We use backfill (bfill) to propagate the first valid observation backwards.\n",
    "            result_df[z_prime_col_name] = result_df[z_prime_col_name].bfill()\n",
    "            result_df[z_double_prime_col_name] = result_df[z_double_prime_col_name].bfill()\n",
    "            \n",
    "            # Keep track of feature names for the final concatenation\n",
    "            feature_list.extend([z_prime_col_name, z_double_prime_col_name])\n",
    "\n",
    "        # Return only the extracted features (concatenated as per Eq 2)\n",
    "        return result_df[['timestamp'] + feature_list]\n",
    "    \n",
    "    def create_temporal_structure_features(self):\n",
    "        daily_interaction_count_df = self.interaction_df.groupby('timestamp').size().reset_index(name='interaction_count')\n",
    "        daily_interaction_count_df = daily_interaction_count_df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "        # Extract Features\n",
    "        f_stru = self._calculate_structural_features(daily_interaction_count_df, \n",
    "                                                gaps=[90, 30, 7, 1])    #dataframe\n",
    "        return f_stru\n",
    "    ################ Creating Temporal Structure Features ################\n",
    "\n",
    "    ################ Concat Temporal Stat & Structure Features ################\n",
    "    def create_temporal_features(self):\n",
    "        f_stat = self.create_temporal_stat_features()\n",
    "        f_stru = self.create_temporal_structure_features()\n",
    "\n",
    "        f_all = pd.merge(f_stat, f_stru, on='timestamp', how='inner')\n",
    "\n",
    "        return f_all\n",
    "    ################ Concat Temporal Stat & Structure Features ################\n",
    "\n",
    "    def process_train(self, train_csv_path, output_csv_path):\n",
    "        \"\"\"\n",
    "        Xử lý tập TRAIN: Fit Scaler, Fit GMM, trích xuất xác suất và lưu Model.\n",
    "        \"\"\"\n",
    "        print(f\"⏳ Đang xử lý tập TRAIN: {train_csv_path}...\")\n",
    "        df_train = pd.read_csv(train_csv_path)\n",
    "        \n",
    "        # 1. Trích xuất đúng các cột thời gian\n",
    "        X_train = df_train[self.feature_cols]\n",
    "        \n",
    "        # 2. Fit và Transform Scaler\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        \n",
    "        # 3. Khởi tạo và Fit GMM\n",
    "        gmm = GaussianMixture(n_components=self.n_clusters, random_state=42, n_init=10)\n",
    "        gmm.fit(X_train_scaled)\n",
    "        \n",
    "        # 4. Lấy Label và Ma trận Xác suất\n",
    "        df_train['cluster_label'] = gmm.predict(X_train_scaled)\n",
    "        W_matrix = gmm.predict_proba(X_train_scaled)\n",
    "        \n",
    "        # 5. Đẩy xác suất vào từng cột\n",
    "        for i in range(self.n_clusters):\n",
    "            df_train[f'prob_cluster_{i}'] = W_matrix[:, i]\n",
    "            \n",
    "        # 6. LƯU LẠI SCALER VÀ GMM ĐỂ DÙNG CHO VALIDATION\n",
    "        joblib.dump(scaler, self.scaler_path)\n",
    "        joblib.dump(gmm, self.gmm_path)\n",
    "        \n",
    "        # 7. Xuất ra file CSV mới\n",
    "        df_train.to_csv(output_csv_path, index=False)\n",
    "        print(f\"✅ Đã hoàn tất Train! Model được lưu tại: {self.model_save_dir}\")\n",
    "        print(f\"✅ Đã xuất dữ liệu Train mới ra: {output_csv_path}\\n\")\n",
    "        \n",
    "        return df_train\n",
    "\n",
    "    def process_validation(self, val_csv_path, output_csv_path):\n",
    "        \"\"\"\n",
    "        Xử lý tập VALIDATION/TEST: Chỉ Load Model, Transform và trích xuất xác suất.\n",
    "        \"\"\"\n",
    "        print(f\"⏳ Đang xử lý tập VALIDATION: {val_csv_path}...\")\n",
    "        \n",
    "        # 1. Kiểm tra xem model đã được train chưa\n",
    "        if not os.path.exists(self.scaler_path) or not os.path.exists(self.gmm_path):\n",
    "            raise FileNotFoundError(\"Chưa tìm thấy Scaler hoặc GMM model! Hãy chạy process_train() trước.\")\n",
    "            \n",
    "        df_val = pd.read_csv(val_csv_path)\n",
    "        X_val = df_val[self.feature_cols]\n",
    "        \n",
    "        # 2. LOAD LẠI SCALER VÀ GMM\n",
    "        scaler = joblib.load(self.scaler_path)\n",
    "        gmm = joblib.load(self.gmm_path)\n",
    "        \n",
    "        # 3. CHỈ TRANSFORM (Tuyệt đối không dùng fit)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        # 4. Dự đoán Label và Ma trận Xác suất\n",
    "        df_val['cluster_label'] = gmm.predict(X_val_scaled)\n",
    "        W_matrix = gmm.predict_proba(X_val_scaled)\n",
    "        \n",
    "        # 5. Đẩy xác suất vào từng cột\n",
    "        for i in range(self.n_clusters):\n",
    "            df_val[f'prob_cluster_{i}'] = W_matrix[:, i]\n",
    "            \n",
    "        # 6. Xuất ra file CSV mới\n",
    "        df_val.to_csv(output_csv_path, index=False)\n",
    "        print(f\"✅ Đã hoàn tất Validation! Dữ liệu xuất ra: {output_csv_path}\\n\")\n",
    "        \n",
    "        return df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427345c",
   "metadata": {},
   "source": [
    "## 1. Creating Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "28e020ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interaction_df = pd.read_csv(f'./data/{name}/{name}_train_interactions.csv')\n",
    "train_interaction_df['timestamp'] = pd.to_datetime(train_interaction_df['timestamp'])\n",
    "train_interaction_df = train_interaction_df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "# train_interaction_df['year'] = train_interaction_df['timestamp'].dt.year\n",
    "train_interaction_df['month'] = train_interaction_df['timestamp'].dt.month\n",
    "train_interaction_df['week'] = train_interaction_df['timestamp'].dt.isocalendar().week\n",
    "\n",
    "# Simple season mapping (1:Winter, 2:Spring, 3:Summer, 4:Fall)\n",
    "train_interaction_df['season'] = train_interaction_df['month'].apply(lambda x: (x%12 + 3)//3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ed843",
   "metadata": {},
   "source": [
    "### 1.1 Creating Temporal Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c6356ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Handle Cyclical Features (Month, Week, Season)\n",
    "# We transform \"Month\" into two dimensions: sin_month and cos_month.\n",
    "# This places months on a unit circle.\n",
    "def encode_cyclic(data, max_val):\n",
    "    data_norm = 2 * np.pi * data / max_val\n",
    "    return np.sin(data_norm), np.cos(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bee9eae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_stat.shape: (5413, 7)\n",
      "f_stat.columns: Index(['timestamp', 'season_sin', 'season_cos', 'month_sin', 'month_cos',\n",
      "       'week_sin', 'week_cos'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_interaction_df['month_sin'], train_interaction_df['month_cos'] = encode_cyclic(train_interaction_df['month'], 12)\n",
    "train_interaction_df['week_sin'], train_interaction_df['week_cos'] = encode_cyclic(train_interaction_df['week'], 52)\n",
    "train_interaction_df['season_sin'], train_interaction_df['season_cos'] = encode_cyclic(train_interaction_df['season'], 4)\n",
    "\n",
    "feature_cols = [\n",
    "    'season_sin', 'season_cos',\n",
    "    'month_sin', 'month_cos', \n",
    "    'week_sin', 'week_cos',\n",
    "]\n",
    "\n",
    "f_stat = train_interaction_df[['timestamp'] + feature_cols] # dataframe\n",
    "f_stat = f_stat.drop_duplicates()\n",
    "\n",
    "print(f\"f_stat.shape: {f_stat.shape}\")    # Feature Matrix Shape: (5919, 7)\n",
    "print(f'f_stat.columns: {f_stat.columns}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd4872",
   "metadata": {},
   "source": [
    "### 1.2 Creating Temporal Structure Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "795e8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_structural_features(df, interaction_count_col='interaction_count', \n",
    "                                gaps=[90, 30, 7, 1]):\n",
    "    \"\"\"\n",
    "    Calculates 1st and 2nd order structural features (z' and z'') for specified gaps.\n",
    "    print(f_stat.tail(1))\n",
    "    Parameters:\n",
    "    - df: DataFrame containing time-series data.\n",
    "    - interaction_count_col: Name of the column containing interaction counts (z(i)).\n",
    "    - gaps: List of window sizes (default: [90, 30, 7, 1] for season, month, week, day).\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with new columns for each gap (z_prime and z_double_prime).\n",
    "    \"\"\"\n",
    "    \n",
    "    # We work on a copy to avoid Modifying the original dataframe\n",
    "    result_df = df.copy()\n",
    "    result_df = result_df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Extract the base interaction series z(i)\n",
    "    z = result_df[interaction_count_col]\n",
    "    \n",
    "    feature_list = []\n",
    "    \n",
    "    for gap in gaps:\n",
    "        # --- 1. First-Order Structural Feature: z'_{gap}(t) ---\n",
    "        # Formula: (Sum(current_window) - Sum(previous_window)) / gap\n",
    "        \n",
    "        # Calculate rolling sum for the current window [t-gap, t]\n",
    "        # This corresponds to Sum_{i=t-gap}^{t} z(i)\n",
    "        current_sum = z.rolling(window=gap, min_periods=gap).sum()\n",
    "        \n",
    "        # The previous window sum is just the current sum shifted by 'gap'\n",
    "        # This corresponds to Sum_{i=t-2gap}^{t-gap} z(i)\n",
    "        prev_sum = current_sum.shift(gap)\n",
    "        \n",
    "        # Calculate z'\n",
    "        z_prime = (current_sum - prev_sum) / gap\n",
    "        \n",
    "        # Rename for storage\n",
    "        z_prime_col_name = f'z_prime_{gap}'\n",
    "        result_df[z_prime_col_name] = z_prime\n",
    "        \n",
    "        # --- 2. Second-Order Structural Feature: z''_{gap}(t) ---\n",
    "        # Formula: (Sum(current_window_of_z') - Sum(previous_window_of_z')) / gap\n",
    "        \n",
    "        # Now we apply the same rolling logic to the z_prime series we just created\n",
    "        current_sum_prime = z_prime.rolling(window=gap, min_periods=gap).sum()\n",
    "        prev_sum_prime = current_sum_prime.shift(gap)\n",
    "\n",
    "        # Calculate z''\n",
    "        z_double_prime = (current_sum_prime - prev_sum_prime) / gap\n",
    "        \n",
    "        # Rename for storage\n",
    "        z_double_prime_col_name = f'z_double_prime_{gap}'\n",
    "        result_df[z_double_prime_col_name] = z_double_prime\n",
    "        \n",
    "        # --- 3. Padding (Handling Initial NaNs) ---\n",
    "        # The paper states: \"padding... with the nearest timestamp's temporal structural feature\"\n",
    "        # Because we used rolling windows, the beginning of the series will have NaNs.\n",
    "        # We use backfill (bfill) to propagate the first valid observation backwards.\n",
    "        result_df[z_prime_col_name] = result_df[z_prime_col_name].bfill()\n",
    "        result_df[z_double_prime_col_name] = result_df[z_double_prime_col_name].bfill()\n",
    "        \n",
    "        # Keep track of feature names for the final concatenation\n",
    "        feature_list.extend([z_prime_col_name, z_double_prime_col_name])\n",
    "\n",
    "    # Return only the extracted features (concatenated as per Eq 2)\n",
    "    return result_df[['timestamp'] + feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dceab660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_stru: (5413, 9)\n",
      "f_stru: Index(['timestamp', 'z_prime_90', 'z_double_prime_90', 'z_prime_30',\n",
      "       'z_double_prime_30', 'z_prime_7', 'z_double_prime_7', 'z_prime_1',\n",
      "       'z_double_prime_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "daily_interaction_count_df = train_interaction_df.groupby('timestamp').size().reset_index(name='interaction_count')\n",
    "daily_interaction_count_df = daily_interaction_count_df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "# Extract Features\n",
    "f_stru = calculate_structural_features(daily_interaction_count_df, \n",
    "                                        gaps=[90, 30, 7, 1])    #dataframe\n",
    "\n",
    "# Check shape (Should have 9 columns: 1 for timestamp and 2 for each of the 4 gaps)\n",
    "print(f\"f_stru: {f_stru.shape}\")    #Feature Matrix Shape: (5919, 9)\n",
    "print(f\"f_stru: {f_stru.columns}\")    #Feature Matrix Shape: (5919, 9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe00118",
   "metadata": {},
   "source": [
    "### 1.3 Concat Temporal Statistical Features and Temporal Structure Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1a18a3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_stat.shape: (5413, 7)\n",
      "f_stru.shape: (5413, 9)\n",
      "f_all.shape: (5413, 15)\n"
     ]
    }
   ],
   "source": [
    "f_all = pd.merge(f_stat, f_stru, on='timestamp', how='inner')\n",
    "\n",
    "print(f'f_stat.shape: {f_stat.shape}')\n",
    "print(f'f_stru.shape: {f_stru.shape}')\n",
    "print(f'f_all.shape: {f_all.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61533421",
   "metadata": {},
   "source": [
    "### 1.4 Clustering time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeac3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_all.shape: (5413, 20)\n",
      "f_all.columns: Index(['timestamp', 'season_sin', 'season_cos', 'month_sin', 'month_cos',\n",
      "       'week_sin', 'week_cos', 'z_prime_90', 'z_double_prime_90', 'z_prime_30',\n",
      "       'z_double_prime_30', 'z_prime_7', 'z_double_prime_7', 'z_prime_1',\n",
      "       'z_double_prime_1', 'cluster_label', 'prob_cluster_0', 'prob_cluster_1',\n",
      "       'prob_cluster_2', 'prob_cluster_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# # # Apply K-Means\n",
    "# kmeans = KMeans(n_clusters= n_clusters, random_state=42, n_init=10)\n",
    "\n",
    "# f_all_feature_only = f_all.drop('timestamp', axis=1)\n",
    "# f_all['cluster_label'] = kmeans.fit_predict(f_all_feature_only)\n",
    "\n",
    "# print(f'f_all.shape: {f_all.shape}')\n",
    "# print(f'f_all.columns: {f_all.columns}')\n",
    "\n",
    "\n",
    "############## Apply GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=n_clusters, random_state=42, n_init=10)\n",
    "\n",
    "f_all_feature_only = f_all.drop('timestamp', axis=1)\n",
    "f_all['cluster_label'] = gmm.fit_predict(f_all_feature_only)    # Return that cluster_label having highest probaililty\n",
    "\n",
    "W_matrix = gmm.predict_proba(f_all_feature_only)      # Returen probability of each clusters,\n",
    "for i in range(n_clusters):\n",
    "    f_all[f'prob_cluster_{i}'] = W_matrix[:, i]\n",
    "\n",
    "print(f'f_all.shape: {f_all.shape}')\n",
    "print(f'f_all.columns: {f_all.columns}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "24ce035b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporal_train_interaction_df.shape: (37513, 34)\n",
      "temporal_train_interaction_df.columns: Index(['user_id', 'entity_id', 'timestamp', 'user_id:token', 'entity_id:token',\n",
      "       'item_id:token', 'month', 'week', 'season', 'month_sin_x',\n",
      "       'month_cos_x', 'week_sin_x', 'week_cos_x', 'season_sin_x',\n",
      "       'season_cos_x', 'season_sin_y', 'season_cos_y', 'month_sin_y',\n",
      "       'month_cos_y', 'week_sin_y', 'week_cos_y', 'z_prime_90',\n",
      "       'z_double_prime_90', 'z_prime_30', 'z_double_prime_30', 'z_prime_7',\n",
      "       'z_double_prime_7', 'z_prime_1', 'z_double_prime_1', 'cluster_label',\n",
      "       'prob_cluster_0', 'prob_cluster_1', 'prob_cluster_2', 'prob_cluster_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "temporal_train_interaction_df = pd.merge(train_interaction_df, f_all, on='timestamp', how='inner')\n",
    "print(f'temporal_train_interaction_df.shape: {temporal_train_interaction_df.shape}')\n",
    "print(f'temporal_train_interaction_df.columns: {temporal_train_interaction_df.columns}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08be9fd",
   "metadata": {},
   "source": [
    "## 2. Create TCKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b651475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['head_id', 'relation_id', 'tail_id', 'head_id:token', 'relation_id:token', 'tail_id:token', 'cluster_label', 'prob_cluster_0', 'prob_cluster_1', 'prob_cluster_2', 'prob_cluster_3']\n"
     ]
    }
   ],
   "source": [
    "graph_df = pd.read_csv(f'./data/{name}/{name}_processed_static_graph.csv')\n",
    "\n",
    "temporal_train_interaction_df = temporal_train_interaction_df.rename(columns={'user_id': 'head_id', \n",
    "                                            'entity_id': 'tail_id',\n",
    "                                            'user_id:token': 'head_id:token',\n",
    "                                            'entity_id:token': 'tail_id:token'})\n",
    "\n",
    "max_relation_id_in_graph = graph_df['relation_id'].max()\n",
    "\n",
    "temporal_train_interaction_df['relation_id'] = temporal_train_interaction_df['cluster_label'] \\\n",
    "                                                + max_relation_id_in_graph + 1  # new relation_id. +1 because relation_id starts at 1, not 0\n",
    "temporal_train_interaction_df['relation_id:token'] = 'interacted_' + temporal_train_interaction_df['cluster_label'].astype(str)  # new relation_id:token\n",
    "\n",
    "\n",
    "COLUMN_NAMES = ['head_id', 'relation_id', 'tail_id',\n",
    "                                'head_id:token', 'relation_id:token', 'tail_id:token']\n",
    "\n",
    "df1 = temporal_train_interaction_df[COLUMN_NAMES]\n",
    "\n",
    "                            \n",
    "TCKG_df = pd.concat([graph_df, df1], ignore_index=True)\n",
    "TCKG_df = TCKG_df.sort_values(by=['relation_id', 'head_id', 'tail_id'])\n",
    "TCKG_df.to_csv(f'./data/{name}/{name}_TCKG.csv', index= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274536f",
   "metadata": {},
   "source": [
    "###### TRAIN CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAMES = ['head_id', 'relation_id', 'tail_id',\n",
    "                                'head_id:token', 'relation_id:token', 'tail_id:token']\n",
    "COLUMN_NAMES.append('cluster_label')\n",
    "for i in range(n_clusters):\n",
    "    COLUMN_NAMES.append(f'prob_cluster_{i}')\n",
    "\n",
    "print(COLUMN_NAMES)\n",
    "\n",
    "temporal_train_interaction_df = temporal_train_interaction_df[COLUMN_NAMES]\n",
    "temporal_train_interaction_df.to_csv(f'./data/{name}/{name}_train_set.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27db0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd1695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
