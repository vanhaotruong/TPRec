{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8b26601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Set environment variables for reproducibility and safety\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 1. Configuration & Seeding\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00d1c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'book'\n",
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427345c",
   "metadata": {},
   "source": [
    "## 1. Learn Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16a21a",
   "metadata": {},
   "source": [
    "### 1.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4bf5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCKGDataset(Dataset):\n",
    "    def __init__(self, triplets, num_entities):\n",
    "        self.triplets = triplets\n",
    "        self.num_entities = num_entities\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    def __getitem__(self, idx):\n",
    "        # Trảmovie về bộ ba (head, relation, tail)\n",
    "        return self.triplets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae3ed5",
   "metadata": {},
   "source": [
    "### 1.2 TransE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28e020ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(pl.LightningModule):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim=64, lr=1e-3, weight_decay=1e-4, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Khởi tạo Embeddings\n",
    "        self.entity_emb = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.relation_emb = nn.Embedding(num_relations, embedding_dim)\n",
    "        \n",
    "        # Xavier initialization giúp hội tụ tốt hơn\n",
    "        nn.init.xavier_uniform_(self.entity_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.relation_emb.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, h, r, t):\n",
    "        h_e = self.entity_emb(h)\n",
    "        r_e = self.relation_emb(r)\n",
    "        t_e = self.entity_emb(t)\n",
    "\n",
    "        # 2. Embedding Normalization (Rất quan trọng cho TransE)\n",
    "        # Ép độ dài các vector về 1 (Unit Norm constraint)\n",
    "        h_e = F.normalize(h_e, p=2, dim=1)\n",
    "        r_e = F.normalize(r_e, p=2, dim=1)\n",
    "        t_e = F.normalize(t_e, p=2, dim=1)\n",
    "        \n",
    "        # 3. Áp dụng Dropout\n",
    "        h_e = self.dropout(h_e)\n",
    "        r_e = self.dropout(r_e)\n",
    "        t_e = self.dropout(t_e)\n",
    "        \n",
    "        # Công thức (6): Khoảng cách bình phương L2\n",
    "        # g_r(h, t) = ||h + r - t||^2\n",
    "        score = torch.sum((h_e + r_e - t_e)**2, dim=1)\n",
    "        return score\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        h, r, t = batch[:, 0], batch[:, 1], batch[:, 2]\n",
    "        \n",
    "        # Tính score cho bộ ba đúng (Positive) -> Cần giảm thiểu khoảng cách này\n",
    "        pos_scores = self(h, r, t)\n",
    "        \n",
    "        # Negative Sampling: Thay thế tail t bằng t' ngẫu nhiên\n",
    "        # t' không nhất thiết phải là không đúng thực tế (simplified), nhưng xác suất cao là không đúng.\n",
    "        rand_t = torch.randint(0, self.hparams.num_entities, t.shape, device=self.device)\n",
    "        \n",
    "        # Tính score cho bộ ba sai (Negative) -> Cần tối đa hóa khoảng cách này\n",
    "        neg_scores = self(h, r, rand_t)\n",
    "        \n",
    "        # Công thức (7) Loss: -ln(sigmoid(g_neg - g_pos))\n",
    "        # Chúng ta muốn g_neg > g_pos (khoảng cách sai lớn hơn đúng)\n",
    "        # => (g_neg - g_pos) càng lớn càng tốt\n",
    "        loss = -F.logsigmoid(neg_scores - pos_scores).mean()\n",
    "        \n",
    "        # Log loss\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        h, r, t = batch[:, 0], batch[:, 1], batch[:, 2]\n",
    "        \n",
    "        # 1. Tính loss trên valid set\n",
    "        pos_scores = self(h, r, t)\n",
    "        \n",
    "        # Negative sampling (đơn giản hoá để tính loss theo dõi)\n",
    "        rand_t = torch.randint(0, self.hparams.num_entities, t.shape, device=self.device)\n",
    "        neg_scores = self(h, r, rand_t)\n",
    "        \n",
    "        val_loss = -F.logsigmoid(neg_scores - pos_scores).mean()\n",
    "        self.log('val_loss', val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 4. Thêm weight_decay (L2 regularization) vào Adam\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a4929",
   "metadata": {},
   "source": [
    "### 1.3 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "576d8ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./data/book_TCKG.csv...\n",
      "Total Entities: 36884\n",
      "Total Relations: 24\n",
      "triplets_tensor.shape: torch.Size([97181, 3])\n"
     ]
    }
   ],
   "source": [
    "file_path = f'./data/{name}_TCKG.csv' \n",
    "print(f\"Loading data from {file_path}...\")\n",
    "\n",
    "TCKG_df = pd.read_csv(file_path)\n",
    "\n",
    "# Tạo Mapping ID an toàn (dựa trên token để tránh trùng lặp giữa UserID và EntityID)\n",
    "# Gom tất cả head và tail tokens để tạo không gian Entity\n",
    "all_entities = pd.concat([TCKG_df['head_id:token'], TCKG_df['tail_id:token']]).unique()\n",
    "entity_to_idx = {token: i for i, token in enumerate(all_entities)}\n",
    "\n",
    "all_relations = TCKG_df['relation_id:token'].unique()\n",
    "relation_to_idx = {token: i for i, token in enumerate(all_relations)}\n",
    "\n",
    "print(f\"Total Entities: {len(all_entities)}\")\n",
    "print(f\"Total Relations: {len(all_relations)}\")\n",
    "\n",
    "# Chuyển đổi dữ liệu sang index\n",
    "triplets_np = np.stack([\n",
    "    TCKG_df['head_id:token'].map(entity_to_idx).values,\n",
    "    TCKG_df['relation_id:token'].map(relation_to_idx).values,\n",
    "    TCKG_df['tail_id:token'].map(entity_to_idx).values\n",
    "], axis=1)\n",
    "\n",
    "# Chuyển sang Tensor\n",
    "triplets_tensor = torch.tensor(triplets_np, dtype=torch.long)\n",
    "print(f'triplets_tensor.shape: {triplets_tensor.shape}')\n",
    "\n",
    "# Tạo DataLoader\n",
    "full_dataset = TCKGDataset(triplets_tensor, num_entities=len(all_entities))\n",
    "\n",
    "# Chia 90% Train - 10% Val\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Tạo 2 Loaders\n",
    "train_loader = DataLoader(train_set, batch_size=1024, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=1024, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a424b4",
   "metadata": {},
   "source": [
    "### 1.3 Init and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fb6af53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name         | Type      | Params | Mode  | FLOPs\n",
      "-----------------------------------------------------------\n",
      "0 | entity_emb   | Embedding | 4.7 M  | train | 0    \n",
      "1 | relation_emb | Embedding | 3.1 K  | train | 0    \n",
      "2 | dropout      | Dropout   | 0      | train | 0    \n",
      "-----------------------------------------------------------\n",
      "4.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 M     Total params\n",
      "18.897    Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 86/86 [00:07<00:00, 11.76it/s, v_num=7, train_loss=0.504, val_loss=0.472]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 86/86 [00:07<00:00, 11.84it/s, v_num=7, train_loss=0.347, val_loss=0.433]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.039 >= min_delta = 0.001. New best score: 0.433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 86/86 [00:07<00:00, 11.67it/s, v_num=7, train_loss=0.303, val_loss=0.411]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.022 >= min_delta = 0.001. New best score: 0.411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 86/86 [00:07<00:00, 11.25it/s, v_num=7, train_loss=0.268, val_loss=0.404]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.001. New best score: 0.404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 86/86 [00:07<00:00, 11.66it/s, v_num=7, train_loss=0.253, val_loss=0.397]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.001. New best score: 0.397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 86/86 [00:07<00:00, 11.40it/s, v_num=7, train_loss=0.251, val_loss=0.393]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.001. New best score: 0.393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 86/86 [00:07<00:00, 11.40it/s, v_num=7, train_loss=0.248, val_loss=0.390]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 0.390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 86/86 [00:07<00:00, 11.55it/s, v_num=7, train_loss=0.244, val_loss=0.386]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.001. New best score: 0.386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 86/86 [00:07<00:00, 11.55it/s, v_num=7, train_loss=0.237, val_loss=0.384]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 0.384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 86/86 [00:07<00:00, 11.41it/s, v_num=7, train_loss=0.253, val_loss=0.380]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.001. New best score: 0.380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 86/86 [00:07<00:00, 11.75it/s, v_num=7, train_loss=0.256, val_loss=0.384]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.380. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 86/86 [00:07<00:00, 11.74it/s, v_num=7, train_loss=0.256, val_loss=0.384]\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',       # Theo dõi val_loss\n",
    "    dirpath=f'./checkpoints/', # Thư mục lưu\n",
    "    filename=f'{name}-transE-{timestamp}-{{epoch:02d}}-{{val_loss:.4f}}', \n",
    "    save_top_k=1,             # Chỉ giữ lại 1 model tốt nhất\n",
    "    mode='min',               # Lưu khi val_loss nhỏ nhất\n",
    ")\n",
    "import pickle\n",
    "import numpy as np\n",
    "# 5. Early Stopping Callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss', # Theo dõi val_loss\n",
    "    min_delta=0.001,    # Cải thiện tối thiểu cần thiết\n",
    "    patience=10,         # Chờ 5 epochs nếu không cải thiện thì dừng\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "model = TransE(\n",
    "    num_entities=len(all_entities), \n",
    "    num_relations=len(all_relations), \n",
    "    embedding_dim=128, # Có thể chỉnh d-dimension tại đây\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-3,  # Tăng lên nếu vẫn overfit (ví dụ: 1e-3)\n",
    "    dropout_rate=0.3    # Tăng lên nếu vẫn overfit (tối đa 0.5)\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50, \n",
    "    accelerator=\"auto\", # Tự động dùng GPU nếu có\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "# Bắt đầu huấn luyện\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "# Sau khi train, bạn có thể lấy embedding bằng:\n",
    "# entity_embeddings = model.entity_emb.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188aed9",
   "metadata": {},
   "source": [
    "### 1.5 Save trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9129a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings and mappings saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract Embeddings from Model (move to CPU and convert to numpy)\n",
    "entity_embeddings = model.entity_emb.weight.detach().cpu().numpy()\n",
    "relation_embeddings = model.relation_emb.weight.detach().cpu().numpy()\n",
    "\n",
    "# 2. Package everything into a dictionary\n",
    "saved_data = {\n",
    "    'entity_embeddings': entity_embeddings,      # (Num_Entities, dim)\n",
    "    'relation_embeddings': relation_embeddings,  # (Num_Relations, dim)\n",
    "    'entity_to_idx': entity_to_idx,              # Dict: logs\"item_123\" -> 0\n",
    "    'relation_to_idx': relation_to_idx           # Dict: \"interacted_0\" -> 0\n",
    "}\n",
    "# 3. Save to a single file\n",
    "with open(f'./pickle/{name}_transE_embeddings_{timestamp}.pkl', 'wb') as f:\n",
    "    pickle.dump(saved_data, f)\n",
    "print(\"Embeddings and mappings saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
