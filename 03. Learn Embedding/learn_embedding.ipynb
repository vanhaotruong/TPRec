{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b26601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Set environment variables for reproducibility and safety\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 1. Configuration & Seeding\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d1c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'book'\n",
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427345c",
   "metadata": {},
   "source": [
    "## 1. Learn Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16a21a",
   "metadata": {},
   "source": [
    "### 1.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4bf5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCKGDataset(Dataset):\n",
    "    def __init__(self, triplets):\n",
    "        self.triplets = triplets\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    def __getitem__(self, idx):\n",
    "        # Trảmovie về bộ ba (head, relation, tail)\n",
    "        return self.triplets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae3ed5",
   "metadata": {},
   "source": [
    "### 1.2 TransE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e020ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(pl.LightningModule):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim=64, lr=1e-3, weight_decay=1e-4, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Khởi tạo Embeddings\n",
    "        self.entity_emb = nn.Embedding(num_entities + 1, embedding_dim, padding_idx=0)     # +1 because starting at 1 instead of 0\n",
    "        self.relation_emb = nn.Embedding(num_relations + 1, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # # Xavier initialization giúp hội tụ tốt hơn\n",
    "        # nn.init.xavier_uniform_(self.entity_emb.weight)\n",
    "        # nn.init.xavier_uniform_(self.relation_emb.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, h, r, t):\n",
    "        h_e = self.entity_emb(h)\n",
    "        r_e = self.relation_emb(r)\n",
    "        t_e = self.entity_emb(t)\n",
    "\n",
    "        # 2. Embedding Normalization (Rất quan trọng cho TransE)\n",
    "        # Ép độ dài các vector về 1 (Unit Norm constraint)\n",
    "        h_e = F.normalize(h_e, p=2, dim=1)\n",
    "        r_e = F.normalize(r_e, p=2, dim=1)\n",
    "        t_e = F.normalize(t_e, p=2, dim=1)\n",
    "        \n",
    "        # 3. Áp dụng Dropout\n",
    "        h_e = self.dropout(h_e)\n",
    "        r_e = self.dropout(r_e)\n",
    "        t_e = self.dropout(t_e)\n",
    "        \n",
    "        # Công thức (6): Khoảng cách bình phương L2\n",
    "        # g_r(h, t) = ||h + r - t||^2\n",
    "        score = torch.sum((h_e + r_e - t_e)**2, dim=1)\n",
    "        return score\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        h, r, t = batch[:, 0], batch[:, 1], batch[:, 2]\n",
    "        \n",
    "        # Tính score cho bộ ba đúng (Positive) -> Cần giảm thiểu khoảng cách này\n",
    "        pos_scores = self(h, r, t)\n",
    "        \n",
    "        # Negative Sampling: Thay thế tail t bằng t' ngẫu nhiên\n",
    "        # t' không nhất thiết phải là không đúng thực tế (simplified), nhưng xác suất cao là không đúng.\n",
    "        rand_t = torch.randint(1, self.hparams.num_entities + 1, t.shape, device=self.device)\n",
    "        \n",
    "        # Tính score cho bộ ba sai (Negative) -> Cần tối đa hóa khoảng cách này\n",
    "        neg_scores = self(h, r, rand_t)\n",
    "        \n",
    "        # Công thức (7) Loss: -ln(sigmoid(g_neg - g_pos))\n",
    "        # Chúng ta muốn g_neg > g_pos (khoảng cách sai lớn hơn đúng)\n",
    "        # => (g_neg - g_pos) càng lớn càng tốt\n",
    "        loss = -F.logsigmoid(neg_scores - pos_scores).mean()\n",
    "        \n",
    "        # Log loss\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        h, r, t = batch[:, 0], batch[:, 1], batch[:, 2] #h,r,t shape = batch_size\n",
    "        \n",
    "        # 1. Tính loss trên valid set\n",
    "        pos_scores = self(h, r, t)\n",
    "        \n",
    "        # Negative sampling (đơn giản hoá để tính loss theo dõi)\n",
    "        rand_t = torch.randint(1, self.hparams.num_entities + 1, t.shape, device=self.device)\n",
    "\n",
    "        neg_scores = self(h, r, rand_t)\n",
    "        \n",
    "        val_loss = -F.logsigmoid(neg_scores - pos_scores).mean()\n",
    "        self.log('val_loss', val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 4. Thêm weight_decay (L2 regularization) vào Adam\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a4929",
   "metadata": {},
   "source": [
    "### 1.3 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "576d8ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./data/book_TCKG.csv...\n",
      "triplets_tensor.shape: torch.Size([190734, 3])\n"
     ]
    }
   ],
   "source": [
    "file_path = f'./data/{name}_TCKG.csv' \n",
    "print(f\"Loading data from {file_path}...\")\n",
    "\n",
    "TCKG_df = pd.read_csv(file_path)\n",
    "\n",
    "# Chuyển đổi dữ liệu sang index\n",
    "triplets_np = np.stack([\n",
    "    TCKG_df['head_id'],\n",
    "    TCKG_df['relation_id'],\n",
    "    TCKG_df['tail_id']\n",
    "], axis=1)\n",
    "\n",
    "# 2. Tìm Offset (Lấy ID lớn nhất của relation hiện tại)\n",
    "# Ví dụ: nếu relation_id chạy từ 1 đến 10, offset sẽ là 10.\n",
    "offset = TCKG_df['relation_id'].max()\n",
    "\n",
    "# 3. Tạo Inverse Connections (Cạnh ngược)\n",
    "# Đảo vị trí Tail -> Head, Head -> Tail, và cộng offset vào Relation\n",
    "inverse_triplets_np = np.stack([\n",
    "    TCKG_df['tail_id'],                 # Tail thành Head\n",
    "    TCKG_df['relation_id'] + offset,    # Relation mới = Relation cũ + offset\n",
    "    TCKG_df['head_id']                  # Head thành Tail\n",
    "], axis=1)\n",
    "\n",
    "# 4. Gộp cả 2 mảng lại với nhau\n",
    "# axis=0 nghĩa là nối tiếp theo chiều dọc (thêm dòng)\n",
    "all_triplets_np = np.concatenate([triplets_np, inverse_triplets_np], axis=0)\n",
    "\n",
    "# # Lưu all_triplets_np ra file CSV\n",
    "# df1 = pd.DataFrame(all_triplets_np, columns=['head_id', 'relation_id', 'tail_id'])\n",
    "# df1 = df1.sort_values(by=['relation_id'])\n",
    "# df1.to_csv(f'./data/{name}_TCKG_all.csv', index=False)\n",
    "\n",
    "\n",
    "# Chuyển sang Tensor\n",
    "triplets_tensor = torch.tensor(all_triplets_np, dtype=torch.long)\n",
    "print(f'triplets_tensor.shape: {triplets_tensor.shape}')\n",
    "\n",
    "# Tạo DataLoader\n",
    "full_dataset = TCKGDataset(triplets_tensor)\n",
    "\n",
    "# Chia 90% Train - 10% Val\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Tạo 2 Loaders\n",
    "train_loader = DataLoader(train_set, batch_size=1024, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_set, batch_size=1024, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a424b4",
   "metadata": {},
   "source": [
    "### 1.3 Init and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fb6af53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Entities: 46186\n",
      "Total Relations: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name         | Type      | Params | Mode  | FLOPs\n",
      "-----------------------------------------------------------\n",
      "0 | entity_emb   | Embedding | 3.0 M  | train | 0    \n",
      "1 | relation_emb | Embedding | 3.1 K  | train | 0    \n",
      "2 | dropout      | Dropout   | 0      | train | 0    \n",
      "-----------------------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "11.836    Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "num_entites = pd.concat([TCKG_df['head_id'], TCKG_df['tail_id']]).max()\n",
    "\n",
    "num_relations = TCKG_df['relation_id'].max() * 2    #*2 to double relation for inverse connection\n",
    "\n",
    "print(f\"Total Entities: {num_entites}\")\n",
    "print(f\"Total Relations: {num_relations}\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',       # Theo dõi val_loss\n",
    "    dirpath=f'./checkpoints/', # Thư mục lưu\n",
    "    filename=f'{name}-transE-{timestamp}-{{epoch:02d}}-{{val_loss:.4f}}', \n",
    "    save_top_k=1,             # Chỉ giữ lại 1 model tốt nhất\n",
    "    mode='min',               # Lưu khi val_loss nhỏ nhất\n",
    ")\n",
    "\n",
    "# 5. Early Stopping Callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss', # Theo dõi val_loss\n",
    "    min_delta=0.001,    # Cải thiện tối thiểu cần thiết\n",
    "    patience=20,         # Chờ 5 epochs nếu không cải thiện thì dừng\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "model = TransE(\n",
    "    num_entities=num_entites, \n",
    "    num_relations=num_relations, \n",
    "    embedding_dim=64, # Có thể chỉnh d-dimension tại đây\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-3,  # Tăng lên nếu vẫn overfit (ví dụ: 1e-3)\n",
    "    dropout_rate=0.3    # Tăng lên nếu vẫn overfit (tối đa 0.5)\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=500, \n",
    "    accelerator=\"auto\", # Tự động dùng GPU nếu có\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "# Bắt đầu huấn luyện\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "# Sau khi train, bạn có thể lấy embedding bằng:\n",
    "# entity_embeddings = model.entity_emb.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188aed9",
   "metadata": {},
   "source": [
    "### 1.5 Save trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9129a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract Embeddings from Model (move to CPU and convert to numpy)\n",
    "entity_embeddings = model.entity_emb.weight.detach().cpu().numpy()\n",
    "relation_embeddings = model.relation_emb.weight.detach().cpu().numpy()\n",
    "\n",
    "# 2. Package everything into a dictionary\n",
    "saved_data = {\n",
    "    'entity_embeddings': entity_embeddings,      # (Num_Entities, dim)\n",
    "    'relation_embeddings': relation_embeddings,  # (Num_Relations, dim)\n",
    "}\n",
    "# 3. Save to a single file\n",
    "with open(f'./pickle/{name}_transE_embeddings_{timestamp}.pkl', 'wb') as f:\n",
    "    pickle.dump(saved_data, f)\n",
    "print(\"Embeddings and mappings saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
