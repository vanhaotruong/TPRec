{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8b26601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Set environment variables for reproducibility and safety\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 1. Configuration & Seeding\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d1c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'book'\n",
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427345c",
   "metadata": {},
   "source": [
    "## 1. Learn Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16a21a",
   "metadata": {},
   "source": [
    "### 1.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4bf5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCKGDataset(Dataset):\n",
    "    def __init__(self, triplets):\n",
    "        self.triplets = triplets\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    def __getitem__(self, idx):\n",
    "        # Trảmovie về bộ ba (head, relation, tail)\n",
    "        return self.triplets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae3ed5",
   "metadata": {},
   "source": [
    "### 1.2 TransE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28e020ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(pl.LightningModule):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim=64, lr=1e-3, weight_decay=1e-4, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Khởi tạo Embeddings\n",
    "        self.entity_emb = nn.Embedding(num_entities + 1, embedding_dim, padding_idx=0)     # +1 because starting at 1 instead of 0\n",
    "        self.relation_emb = nn.Embedding(num_relations + 1, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # # Xavier initialization giúp hội tụ tốt hơn\n",
    "        # nn.init.xavier_uniform_(self.entity_emb.weight)\n",
    "        # nn.init.xavier_uniform_(self.relation_emb.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, h, r, t):\n",
    "        h_e = self.entity_emb(h)\n",
    "        r_e = self.relation_emb(r)\n",
    "        t_e = self.entity_emb(t)\n",
    "\n",
    "        # 2. Embedding Normalization (Rất quan trọng cho TransE)\n",
    "        # Ép độ dài các vector về 1 (Unit Norm constraint)\n",
    "        h_e = F.normalize(h_e, p=2, dim=1)\n",
    "        r_e = F.normalize(r_e, p=2, dim=1)\n",
    "        t_e = F.normalize(t_e, p=2, dim=1)\n",
    "        \n",
    "        # 3. Áp dụng Dropout\n",
    "        h_e = self.dropout(h_e)\n",
    "        r_e = self.dropout(r_e)\n",
    "        t_e = self.dropout(t_e)\n",
    "        \n",
    "        # Công thức (6): Khoảng cách bình phương L2\n",
    "        # g_r(h, t) = ||h + r - t||^2\n",
    "        score = torch.sum((h_e + r_e - t_e)**2, dim=1)\n",
    "        return score\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        h, r, t = batch[:, 0], batch[:, 1], batch[:, 2]\n",
    "        \n",
    "        # Tính score cho bộ ba đúng (Positive) -> Cần giảm thiểu khoảng cách này\n",
    "        pos_scores = self(h, r, t)\n",
    "        \n",
    "        # Negative Sampling: Thay thế tail t bằng t' ngẫu nhiên\n",
    "        # t' không nhất thiết phải là không đúng thực tế (simplified), nhưng xác suất cao là không đúng.\n",
    "        rand_t = torch.randint(1, self.hparams.num_entities + 1, t.shape, device=self.device)\n",
    "        \n",
    "        # Tính score cho bộ ba sai (Negative) -> Cần tối đa hóa khoảng cách này\n",
    "        neg_scores = self(h, r, rand_t)\n",
    "        \n",
    "        # Công thức (7) Loss: -ln(sigmoid(g_neg - g_pos))\n",
    "        # Chúng ta muốn g_neg > g_pos (khoảng cách sai lớn hơn đúng)\n",
    "        # => (g_neg - g_pos) càng lớn càng tốt\n",
    "        loss = -F.logsigmoid(neg_scores - pos_scores).mean()\n",
    "        \n",
    "        # Log loss\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        h, r, t = batch[:, 0], batch[:, 1], batch[:, 2] #h,r,t shape = batch_size\n",
    "        \n",
    "        # 1. Tính loss trên valid set\n",
    "        pos_scores = self(h, r, t)\n",
    "        \n",
    "        # Negative sampling (đơn giản hoá để tính loss theo dõi)\n",
    "        rand_t = torch.randint(1, self.hparams.num_entities + 1, t.shape, device=self.device)\n",
    "\n",
    "        neg_scores = self(h, r, rand_t)\n",
    "        \n",
    "        val_loss = -F.logsigmoid(neg_scores - pos_scores).mean()\n",
    "        self.log('val_loss', val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 4. Thêm weight_decay (L2 regularization) vào Adam\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a4929",
   "metadata": {},
   "source": [
    "### 1.3 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "576d8ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./data/book_TCKG.csv...\n",
      "triplets_tensor.shape: torch.Size([236394, 3])\n"
     ]
    }
   ],
   "source": [
    "file_path = f'./data/{name}_TCKG.csv' \n",
    "print(f\"Loading data from {file_path}...\")\n",
    "\n",
    "TCKG_df = pd.read_csv(file_path)\n",
    "\n",
    "# Chuyển đổi dữ liệu sang index\n",
    "triplets_np = np.stack([\n",
    "    TCKG_df['head_id'],\n",
    "    TCKG_df['relation_id'],\n",
    "    TCKG_df['tail_id']\n",
    "], axis=1)\n",
    "\n",
    "# 2. Tìm Offset (Lấy ID lớn nhất của relation hiện tại)\n",
    "# Ví dụ: nếu relation_id chạy từ 1 đến 10, offset sẽ là 10.\n",
    "offset = TCKG_df['relation_id'].max()\n",
    "\n",
    "# 3. Tạo Inverse Connections (Cạnh ngược)\n",
    "# Đảo vị trí Tail -> Head, Head -> Tail, và cộng offset vào Relation\n",
    "inverse_triplets_np = np.stack([\n",
    "    TCKG_df['tail_id'],                 # Tail thành Head\n",
    "    TCKG_df['relation_id'] + offset,    # Relation mới = Relation cũ + offset\n",
    "    TCKG_df['head_id']                  # Head thành Tail\n",
    "], axis=1)\n",
    "\n",
    "# 4. Gộp cả 2 mảng lại với nhau\n",
    "# axis=0 nghĩa là nối tiếp theo chiều dọc (thêm dòng)\n",
    "all_triplets_np = np.concatenate([triplets_np, inverse_triplets_np], axis=0)\n",
    "\n",
    "# # Lưu all_triplets_np ra file CSV\n",
    "# df1 = pd.DataFrame(all_triplets_np, columns=['head_id', 'relation_id', 'tail_id'])\n",
    "# df1 = df1.sort_values(by=['relation_id'])\n",
    "# df1.to_csv(f'./data/{name}_TCKG_all.csv', index=False)\n",
    "\n",
    "\n",
    "# Chuyển sang Tensor\n",
    "triplets_tensor = torch.tensor(all_triplets_np, dtype=torch.long)\n",
    "print(f'triplets_tensor.shape: {triplets_tensor.shape}')\n",
    "\n",
    "# Tạo DataLoader\n",
    "full_dataset = TCKGDataset(triplets_tensor)\n",
    "\n",
    "# Chia 90% Train - 10% Val\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Tạo 2 Loaders\n",
    "train_loader = DataLoader(train_set, batch_size=1024, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_set, batch_size=1024, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a424b4",
   "metadata": {},
   "source": [
    "### 1.3 Init and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fb6af53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name         | Type      | Params | Mode  | FLOPs\n",
      "-----------------------------------------------------------\n",
      "0 | entity_emb   | Embedding | 2.5 M  | train | 0    \n",
      "1 | relation_emb | Embedding | 4.5 K  | train | 0    \n",
      "2 | dropout      | Dropout   | 0      | train | 0    \n",
      "-----------------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 M     Total params\n",
      "9.973     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Entities: 38885\n",
      "Total Relations: 70\n",
      "Epoch 0: 100%|██████████| 208/208 [00:15<00:00, 13.55it/s, v_num=0, train_loss=0.787, val_loss=0.719]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 208/208 [00:09<00:00, 21.80it/s, v_num=0, train_loss=0.820, val_loss=0.716]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.001. New best score: 0.716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 208/208 [00:10<00:00, 20.78it/s, v_num=0, train_loss=0.782, val_loss=0.715]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 0.715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 208/208 [00:10<00:00, 19.03it/s, v_num=0, train_loss=0.821, val_loss=0.709]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.001. New best score: 0.709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 208/208 [00:11<00:00, 17.99it/s, v_num=0, train_loss=0.828, val_loss=0.706]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.001. New best score: 0.706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 208/208 [00:10<00:00, 19.12it/s, v_num=0, train_loss=0.793, val_loss=0.698]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.001. New best score: 0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 208/208 [00:10<00:00, 19.30it/s, v_num=0, train_loss=0.833, val_loss=0.692]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.001. New best score: 0.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 208/208 [00:10<00:00, 18.93it/s, v_num=0, train_loss=0.782, val_loss=0.676]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.016 >= min_delta = 0.001. New best score: 0.676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 208/208 [00:11<00:00, 17.71it/s, v_num=0, train_loss=0.799, val_loss=0.658]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.018 >= min_delta = 0.001. New best score: 0.658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 208/208 [00:09<00:00, 21.32it/s, v_num=0, train_loss=0.799, val_loss=0.628]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.031 >= min_delta = 0.001. New best score: 0.628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 208/208 [00:10<00:00, 20.49it/s, v_num=0, train_loss=0.716, val_loss=0.587]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.040 >= min_delta = 0.001. New best score: 0.587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 208/208 [00:10<00:00, 19.65it/s, v_num=0, train_loss=0.695, val_loss=0.531]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.056 >= min_delta = 0.001. New best score: 0.531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 208/208 [00:11<00:00, 18.40it/s, v_num=0, train_loss=0.626, val_loss=0.478]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.053 >= min_delta = 0.001. New best score: 0.478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 208/208 [00:12<00:00, 16.91it/s, v_num=0, train_loss=0.548, val_loss=0.426]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.052 >= min_delta = 0.001. New best score: 0.426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 208/208 [00:11<00:00, 17.63it/s, v_num=0, train_loss=0.467, val_loss=0.383]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.043 >= min_delta = 0.001. New best score: 0.383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 208/208 [00:10<00:00, 20.11it/s, v_num=0, train_loss=0.462, val_loss=0.350]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.033 >= min_delta = 0.001. New best score: 0.350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 208/208 [00:11<00:00, 18.84it/s, v_num=0, train_loss=0.340, val_loss=0.328]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.021 >= min_delta = 0.001. New best score: 0.328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 208/208 [00:10<00:00, 19.11it/s, v_num=0, train_loss=0.348, val_loss=0.311]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.018 >= min_delta = 0.001. New best score: 0.311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 208/208 [00:09<00:00, 22.53it/s, v_num=0, train_loss=0.308, val_loss=0.301]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.010 >= min_delta = 0.001. New best score: 0.301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 208/208 [00:11<00:00, 17.64it/s, v_num=0, train_loss=0.306, val_loss=0.294]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.001. New best score: 0.294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 208/208 [00:09<00:00, 22.28it/s, v_num=0, train_loss=0.313, val_loss=0.288]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.001. New best score: 0.288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 208/208 [00:09<00:00, 23.06it/s, v_num=0, train_loss=0.313, val_loss=0.283]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.001. New best score: 0.283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 208/208 [00:08<00:00, 23.84it/s, v_num=0, train_loss=0.281, val_loss=0.277]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.001. New best score: 0.277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 208/208 [00:09<00:00, 22.90it/s, v_num=0, train_loss=0.287, val_loss=0.274]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.001. New best score: 0.274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 208/208 [00:08<00:00, 23.18it/s, v_num=0, train_loss=0.279, val_loss=0.273]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 0.273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 208/208 [00:08<00:00, 23.58it/s, v_num=0, train_loss=0.284, val_loss=0.268]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.001. New best score: 0.268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 208/208 [00:09<00:00, 22.72it/s, v_num=0, train_loss=0.288, val_loss=0.266]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 0.266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 208/208 [00:08<00:00, 23.44it/s, v_num=0, train_loss=0.256, val_loss=0.263]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.001. New best score: 0.263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 208/208 [00:08<00:00, 23.32it/s, v_num=0, train_loss=0.282, val_loss=0.261]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 0.261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 208/208 [00:09<00:00, 22.34it/s, v_num=0, train_loss=0.251, val_loss=0.260]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 0.260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 208/208 [00:11<00:00, 18.86it/s, v_num=0, train_loss=0.268, val_loss=0.259]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 0.259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 208/208 [00:11<00:00, 18.50it/s, v_num=0, train_loss=0.265, val_loss=0.260]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.259. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 208/208 [00:11<00:00, 18.50it/s, v_num=0, train_loss=0.265, val_loss=0.260]\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "num_entites = pd.concat([TCKG_df['head_id'], TCKG_df['tail_id']]).max()\n",
    "\n",
    "num_relations = TCKG_df['relation_id'].max() * 2    #*2 to double relation for inverse connection\n",
    "\n",
    "print(f\"Total Entities: {num_entites}\")\n",
    "print(f\"Total Relations: {num_relations}\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',       # Theo dõi val_loss\n",
    "    dirpath=f'./checkpoints/', # Thư mục lưu\n",
    "    filename=f'{name}-transE-{timestamp}-{{epoch:02d}}-{{val_loss:.4f}}', \n",
    "    save_top_k=1,             # Chỉ giữ lại 1 model tốt nhất\n",
    "    mode='min',               # Lưu khi val_loss nhỏ nhất\n",
    ")\n",
    "\n",
    "# 5. Early Stopping Callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss', # Theo dõi val_loss\n",
    "    min_delta=0.001,    # Cải thiện tối thiểu cần thiết\n",
    "    patience=20,         # Chờ 5 epochs nếu không cải thiện thì dừng\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "model = TransE(\n",
    "    num_entities=num_entites, \n",
    "    num_relations=num_relations, \n",
    "    embedding_dim=64, # Có thể chỉnh d-dimension tại đây\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-3,  # Tăng lên nếu vẫn overfit (ví dụ: 1e-3)\n",
    "    dropout_rate=0.3    # Tăng lên nếu vẫn overfit (tối đa 0.5)\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=500, \n",
    "    accelerator=\"auto\", # Tự động dùng GPU nếu có\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "# Bắt đầu huấn luyện\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "# Sau khi train, bạn có thể lấy embedding bằng:\n",
    "# entity_embeddings = model.entity_emb.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188aed9",
   "metadata": {},
   "source": [
    "### 1.5 Save trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9129a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings and mappings saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract Embeddings from Model (move to CPU and convert to numpy)\n",
    "entity_embeddings = model.entity_emb.weight.detach().cpu().numpy()\n",
    "relation_embeddings = model.relation_emb.weight.detach().cpu().numpy()\n",
    "\n",
    "# 2. Package everything into a dictionary\n",
    "saved_data = {\n",
    "    'entity_embeddings': entity_embeddings,      # (Num_Entities, dim)\n",
    "    'relation_embeddings': relation_embeddings,  # (Num_Relations, dim)\n",
    "}\n",
    "# 3. Save to a single file\n",
    "with open(f'./pickle/{name}_transE_embeddings_{timestamp}.pkl', 'wb') as f:\n",
    "    pickle.dump(saved_data, f)\n",
    "print(\"Embeddings and mappings saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
